{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimiw0821/DS-GA_1011_NLP/blob/master/HW2/lm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XLYAo3OBfcVk"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KKioHyAAfcVn",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfhwSMNPfcVt"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpEk23hBfcVv"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l3FyElwEfcVx",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySlA8mEVfcV1"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5WHZwSwkfcV2",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wm36MLfmTh0P",
        "outputId": "57b9b1a4-f2ab-424c-8cda-87752d9069fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PCNXYS4lfcWF",
        "outputId": "a66b3ee8-f238-4511-a0ca-c0f22e58d3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e5fb4656f6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramAdditive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# +1 is for <eos>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTM0XO8IfcWK"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W5xTnBuAfcWK",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, alpha, gamma, vsize):\n",
        "        self.n = n\n",
        "#         self.lam = lam\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.vsize = vsize\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for n in range(1, self.n+1):\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        \n",
        "        if self.total[prefix] > 0:\n",
        "            prob = (self.count[prefix][word] / self.total[prefix]) * self.alpha\n",
        "        else:\n",
        "            prob = self.gamma * self.ngram_prob(ngram[1:])\n",
        "        \n",
        "#         if len(ngram) >= 2:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*self.ngram_prob(ngram[1:])\n",
        "#         elif len(ngram) == 1:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*1./self.vsize\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SuS45FMOfcWM"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oBAj7JKyfcWN",
        "outputId": "ad595933-c598-48da-8486-54a898c9b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# n = 2\n",
        "# for lambda_ in np.linspace(0.1,1,10):\n",
        "#     lm2 = NGramInterpolation(n=n, lam=lambda_, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm2.estimate(datasets['train'])\n",
        "#     print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['train'])))\n",
        "#     print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Valid Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['valid'])))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-da048ee97292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramInterpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# +1 is for <eos>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lam'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqUJUo4mfcWR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ox4t2hMXfcWU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yPLub2RsfcWY"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbG5lE3CfcWZ",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZM9ArzbEfcWc"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zR28L4vPfcWd",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43KrHPW1fcWh",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DlckZW9zfcWk",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otoIoQF0fcWo",
        "outputId": "3f7ece0d-8ed2-4296-a764-9be7f9e11070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:14<00:00, 581.63it/s]\n",
            "100%|██████████| 8464/8464 [00:10<00:00, 783.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_x7xOMnbfcWr",
        "colab": {}
      },
      "source": [
        "# Construct Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aw1YalFTfcWu",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = wikitext_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W4qo745ffcWx",
        "outputId": "c94cfca1-af24-4931-862d-43a1e3505a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 102520.91it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 135100.86it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 134708.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OiahWa0lfcW1",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)#, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ak-71_1fcW5"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXh2hKp7fcW7",
        "colab": {}
      },
      "source": [
        "# making a FFNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRxfJc-EfcXC",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RnnLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.rnn = nn.RNN(self.embed_dim, self.hidden_dim, self.num_layers, dropout=self.p, batch_first=self.batch_first)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        batch_size = len(encoded_input_sequence)\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        states = self.zero_state(batch_size)\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        rnn_outputs, states = self.rnn(x, states)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits, states\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        '''\n",
        "        Return the tuple of hidden state and cell state\n",
        "        '''\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SMmYZceKfcXI",
        "colab": {}
      },
      "source": [
        "class LstmLM(torch.nn.Module):\n",
        "    def __init__(self, options):\n",
        "        '''\n",
        "        params:\n",
        "            @options: dictionary of model parameters\n",
        "        '''\n",
        "        super(LstmLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim, self.num_layers, batch_first=self.batch_first, dropout=self.p) # lstm takes word embeddings as inputs and outputs hidden states (dim=hidden_dinm)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size) # linear layer maps from hidden states to word space\n",
        "\n",
        "    def forward(self, encoded_input_sequence):\n",
        "        '''\n",
        "        Forwrad method process the input from token ids to logits\n",
        "        params:\n",
        "            @inp: input sentence\n",
        "        '''\n",
        "        embedded = self.lookup(encoded_input_sequence)\n",
        "        batch_size = len(encoded_input_sequence)\n",
        "        states = self.zero_state(batch_size) # initialize states\n",
        "        lstm_out, states = self.lstm(embedded, states)\n",
        "        logits = self.projection(lstm_out)\n",
        "        return logits, states\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        '''\n",
        "        Return the tuple of hidden state and cell state\n",
        "        '''\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY90VuhCfcXL",
        "colab": {}
      },
      "source": [
        "def compute_predictions(logits):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags(words).\"\"\"\n",
        "    # Create softmax (F.softmax) function\n",
        "    softmax_output = F.softmax(logits, axis=-1)\n",
        "    predictions = torch.multinomial(softmax_output, num_samples=1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jEbfMa_ffcXN",
        "colab": {}
      },
      "source": [
        "# defining what device to use\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGd7jN0LfcXQ",
        "colab": {}
      },
      "source": [
        "# define model parameters -- options\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "options = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "thrCgjyqfcXc"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umLMmIAY2Zwm",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "  '''\n",
        "  function that computes perplexity\n",
        "  '''\n",
        "  return 2**(loss/np.log(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uHG18vvMfcXd",
        "colab": {}
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "def train_model(model, model_name, hyperparams, loaders, save=True):\n",
        "    '''\n",
        "    function to train neural  LM\n",
        "    params:\n",
        "        @model: LM object\n",
        "        @model_name: str\n",
        "        @hyperparams: dictionary of hyperparameters set for the model\n",
        "        @loaders: DataLoader\n",
        "    '''\n",
        "    print(\"Training {}:\".format(model_name))\n",
        "    \n",
        "    # criterion:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "\n",
        "    PATH = model_name + '.pth'\n",
        "    if os.path.exists(PATH): # load pre-trained\n",
        "        print(\"PATH exists!\")\n",
        "        checkpoint = torch.load(PATH, map_location=current_device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    else:\n",
        "      # optimizer:\n",
        "        model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        if hyperparams['optimizer'] == 'SGD':\n",
        "            optimizer = optim.SGD(model_params, lr=hyperparams['lr'], momentum=hyperparams['momentum'])\n",
        "        elif hyperparams['optimizer'] == 'Adam':\n",
        "            optimizer = optim.Adam(model_params, lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "\n",
        "    plot_cache = []\n",
        "    num_epochs = hyperparams['num_epochs']\n",
        "    # # initialize states\n",
        "    # states = model.zero_state()\n",
        "    # if isinstance(states, tuple):\n",
        "    #   h_states, c_states = states\n",
        "    #   h_states.to(current_device)\n",
        "    #   c_states.to(current_device)\n",
        "    # else:\n",
        "    #   h_states = states\n",
        "    #   h_states.to(current_device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits, _ = model(inp)\n",
        "            # if 'LSTM' in model_name:\n",
        "            #   logits, (h_states, c_states) = model(inp, (h_states, c_states))\n",
        "            #   # h_states.detach()\n",
        "            #   # c_states.detach()\n",
        "            # else:\n",
        "            #   logits, h_states = model(inp, h_states)\n",
        "            #   # h_states.detach()\n",
        "            # # compute loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            # back-propogation\n",
        "            loss.backward()\n",
        "            # gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "            optimizer.step()\n",
        "            train_log_cache.append(loss.item()) # store training loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                avg_train_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                avg_train_perplexity = perplexity(avg_train_loss)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_train_loss, prec=4))\n",
        "                print('Step {} avg train perplexity = {:.{prec}f}'.format(i, avg_train_perplexity, prec=4))\n",
        "                train_log_cache = []\n",
        "\n",
        "        #do validation\n",
        "        valid_losses = []\n",
        "        # states = model.zero_state()\n",
        "        # if isinstance(states, tuple):\n",
        "        #   h_states, c_states = states\n",
        "        #   h_states.to(current_device)\n",
        "        #   c_states.to(current_device)\n",
        "        # else:\n",
        "        #   h_states = states\n",
        "        #   h_states.to(current_device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(loaders['valid']):\n",
        "                # current_batch_size = len(inp)\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                device = torch.device(\"cuda\")\n",
        "                logits, _ = model(inp)\n",
        "                # if 'LSTM' in model_name:\n",
        "                #   logits, (h_states, c_states) = model(inp, (h_states, c_states))\n",
        "                #   # h_states.detach()\n",
        "                #   # c_states.detach()\n",
        "                # else:\n",
        "                #   logits, h_states = model(inp, h_states)\n",
        "                #   # h_states.detach()\n",
        "                # compute loss\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item()) # store validation loss\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            avg_val_perplexity = perplexity(avg_val_loss)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch, avg_val_loss, prec=4))\n",
        "            print('Validation perplexity after {} epoch = {:.{prec}f}'.format(epoch, avg_val_perplexity, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_train_loss, avg_val_loss, avg_train_perplexity, avg_val_perplexity))\n",
        "        # # early stopping\n",
        "        # if len(plot_cache)>1:\n",
        "        #   np.abs((plot_cache[epoch][1] - plot_cache[epoch-1][1])/plot_cache[epoch-1][1]) <= 0.0005\n",
        "        #   print(\"Meets early stopping criteria: Finish training\")\n",
        "        #   return plot_cache\n",
        "    \n",
        "    if save:\n",
        "        torch.save({\n",
        "            'epoch':  hyperparams['num_epochs'],\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': plot_cache}, PATH)\n",
        "\n",
        "    print('Finished training')\n",
        "    return plot_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PEdZfQfuiIix",
        "colab": {}
      },
      "source": [
        "### TESTING\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = 64\n",
        "hidden_dim = 200\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "option = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}\n",
        "\n",
        "hyperparam = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 5,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "# train\n",
        "model_name = 'LSTM_temp'\n",
        "model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bYTI_8vfcXg",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss(losses):\n",
        "    epochs = np.array(list(range(len(losses))))\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    axes = fig.subplots(nrows=1, ncols=2)\n",
        "    # plot losses\n",
        "    axes[0].plot(epochs, [i[0] for i in losses], label='Train loss')\n",
        "    axes[0].plot(epochs, [i[1] for i in losses], label='Val loss')\n",
        "    axes[0].set_title(\"Training and Validation losses over time\")\n",
        "    axes[0].set_xlabel(\"Steps\")\n",
        "    axes[0].set_ylabel(\"Losses\")\n",
        "    axes[0].legend(loc='best')\n",
        "    # plot training & validation accuracy\n",
        "    axes[1].plot(epochs, [i[2] for i in losses], label='Train Perplexity')\n",
        "    axes[1].plot(epochs, [i[3] for i in losses], label='Val Perplexity')\n",
        "    axes[1].set_title(\"Training and Validation perplexity over time\")\n",
        "    axes[1].set_xlabel(\"Steps\")\n",
        "    axes[1].set_ylabel(\"Perplexity\")\n",
        "    axes[1].legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJvEVnWnfcXj",
        "outputId": "7c78816e-0ede-4256-e67d-79d591e9dae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# RNN with baseline hyperparameters\n",
        "baseline_hyperparams = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 10,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "model_rnn = RnnLM(options).to(current_device)\n",
        "print(model_rnn)\n",
        "base_rnn_losses = train_model(model_rnn, \"RNN_LM\", baseline_hyperparams, wikitext_loaders)\n",
        "plot_loss(base_rnn_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RnnLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training RNN_LM:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "rnn.weight_ih_l0 \t torch.Size([128, 64])\n",
            "rnn.weight_hh_l0 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l0 \t torch.Size([128])\n",
            "rnn.bias_hh_l0 \t torch.Size([128])\n",
            "rnn.weight_ih_l1 \t torch.Size([128, 128])\n",
            "rnn.weight_hh_l1 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l1 \t torch.Size([128])\n",
            "rnn.bias_hh_l1 \t torch.Size([128])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140704218383344, 140704218030152, 140704218033320, 140704218032960, 140704218032600, 140704218032888, 140704218030800, 140704218030512, 140704218031088, 140704218030656, 140704218031592]}]\n",
            "Step 0 avg train loss = 10.4280\n",
            "Step 0 avg train perplexity = 33792.2613\n",
            "Step 1000 avg train loss = 6.6343\n",
            "Step 1000 avg train perplexity = 760.7827\n",
            "Step 2000 avg train loss = 6.0561\n",
            "Step 2000 avg train perplexity = 426.7124\n",
            "Validation loss after 0 epoch = 5.7173\n",
            "Validation perplexity after 0 epoch = 304.0769\n",
            "Step 0 avg train loss = 5.7544\n",
            "Step 0 avg train perplexity = 315.5682\n",
            "Step 1000 avg train loss = 5.7078\n",
            "Step 1000 avg train perplexity = 301.1947\n",
            "Step 2000 avg train loss = 5.6420\n",
            "Step 2000 avg train perplexity = 282.0250\n",
            "Validation loss after 1 epoch = 5.5239\n",
            "Validation perplexity after 1 epoch = 250.6163\n",
            "Step 0 avg train loss = 5.3850\n",
            "Step 0 avg train perplexity = 218.1166\n",
            "Step 1000 avg train loss = 5.4272\n",
            "Step 1000 avg train perplexity = 227.5182\n",
            "Step 2000 avg train loss = 5.4071\n",
            "Step 2000 avg train perplexity = 222.9731\n",
            "Validation loss after 2 epoch = 5.4497\n",
            "Validation perplexity after 2 epoch = 232.6817\n",
            "Step 0 avg train loss = 5.2722\n",
            "Step 0 avg train perplexity = 194.8386\n",
            "Step 1000 avg train loss = 5.2463\n",
            "Step 1000 avg train perplexity = 189.8650\n",
            "Step 2000 avg train loss = 5.2526\n",
            "Step 2000 avg train perplexity = 191.0550\n",
            "Validation loss after 3 epoch = 5.3830\n",
            "Validation perplexity after 3 epoch = 217.6670\n",
            "Step 0 avg train loss = 5.0770\n",
            "Step 0 avg train perplexity = 160.2960\n",
            "Step 1000 avg train loss = 5.1142\n",
            "Step 1000 avg train perplexity = 166.3723\n",
            "Step 2000 avg train loss = 5.1364\n",
            "Step 2000 avg train perplexity = 170.1002\n",
            "Validation loss after 4 epoch = 5.3569\n",
            "Validation perplexity after 4 epoch = 212.0733\n",
            "Step 0 avg train loss = 5.0265\n",
            "Step 0 avg train perplexity = 152.3934\n",
            "Step 1000 avg train loss = 5.0086\n",
            "Step 1000 avg train perplexity = 149.6914\n",
            "Step 2000 avg train loss = 5.0457\n",
            "Step 2000 avg train perplexity = 155.3585\n",
            "Validation loss after 5 epoch = 5.3476\n",
            "Validation perplexity after 5 epoch = 210.1077\n",
            "Step 0 avg train loss = 4.7588\n",
            "Step 0 avg train perplexity = 116.6020\n",
            "Step 1000 avg train loss = 4.9215\n",
            "Step 1000 avg train perplexity = 137.2104\n",
            "Step 2000 avg train loss = 4.9666\n",
            "Step 2000 avg train perplexity = 143.5377\n",
            "Validation loss after 6 epoch = 5.3342\n",
            "Validation perplexity after 6 epoch = 207.3157\n",
            "Step 0 avg train loss = 4.8287\n",
            "Step 0 avg train perplexity = 125.0502\n",
            "Step 1000 avg train loss = 4.8506\n",
            "Step 1000 avg train perplexity = 127.8133\n",
            "Step 2000 avg train loss = 4.9033\n",
            "Step 2000 avg train perplexity = 134.7383\n",
            "Validation loss after 7 epoch = 5.3396\n",
            "Validation perplexity after 7 epoch = 208.4335\n",
            "Step 0 avg train loss = 4.6493\n",
            "Step 0 avg train perplexity = 104.5068\n",
            "Step 1000 avg train loss = 4.7969\n",
            "Step 1000 avg train perplexity = 121.1361\n",
            "Step 2000 avg train loss = 4.8431\n",
            "Step 2000 avg train perplexity = 126.8627\n",
            "Validation loss after 8 epoch = 5.3259\n",
            "Validation perplexity after 8 epoch = 205.6034\n",
            "Step 0 avg train loss = 4.6967\n",
            "Step 0 avg train perplexity = 109.5801\n",
            "Step 1000 avg train loss = 4.7424\n",
            "Step 1000 avg train perplexity = 114.7119\n",
            "Step 2000 avg train loss = 4.7929\n",
            "Step 2000 avg train perplexity = 120.6479\n",
            "Validation loss after 9 epoch = 5.3343\n",
            "Validation perplexity after 9 epoch = 207.3237\n",
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAFNCAYAAAC0ZpNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "veibMk9afcXp"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zC4kjNcnr3w1",
        "outputId": "ab94e98e-592c-46e8-8d13-63062d9bd915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = [64]\n",
        "hidden_dim = [128, 200]\n",
        "num_layers = [2]\n",
        "dropout = [0.1, 0.5]\n",
        "options = {\n",
        "    'vocab_size': [len(wikitext_dict)],\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': [wikitext_dict.get_id('<pad>')],\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': [True],\n",
        "}\n",
        "\n",
        "regularized_hyperparams = {\n",
        "    'optimizer': ['Adam'],\n",
        "    'lr': [0.001, 0.01],\n",
        "    'num_epochs': [5],\n",
        "    'weight_decay': [0]\n",
        "}\n",
        "\n",
        "finetune_res = {}\n",
        "i=0\n",
        "for option in ParameterGrid(options):\n",
        "    for hyperparam in ParameterGrid(regularized_hyperparams):\n",
        "        model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "        print(model_lstm_tuned)\n",
        "        print('option: ', option)\n",
        "        print('hyperparam: ', hyperparam)\n",
        "        # train\n",
        "        model_name = 'LSTM_Finetuned_' + str(i)\n",
        "        PATH = model_name + '.pth'\n",
        "        finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)\n",
        "        finetune_res[model_name]=({**option, **hyperparam}, finetune_lstm_losses)\n",
        "        i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.1, 'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_0:\n",
            "Step 0 avg train loss = 10.4142\n",
            "Step 0 avg train perplexity = 33328.1886\n",
            "Step 1000 avg train loss = 6.8964\n",
            "Step 1000 avg train perplexity = 988.6871\n",
            "Step 2000 avg train loss = 6.3067\n",
            "Step 2000 avg train perplexity = 548.2088\n",
            "Validation loss after 0 epoch = 5.8833\n",
            "Validation perplexity after 0 epoch = 358.9937\n",
            "Step 0 avg train loss = 6.0402\n",
            "Step 0 avg train perplexity = 419.9697\n",
            "Step 1000 avg train loss = 5.9105\n",
            "Step 1000 avg train perplexity = 368.8722\n",
            "Step 2000 avg train loss = 5.7828\n",
            "Step 2000 avg train perplexity = 324.6831\n",
            "Validation loss after 1 epoch = 5.5815\n",
            "Validation perplexity after 1 epoch = 265.4795\n",
            "Step 0 avg train loss = 5.5994\n",
            "Step 0 avg train perplexity = 270.2691\n",
            "Step 1000 avg train loss = 5.5656\n",
            "Step 1000 avg train perplexity = 261.2921\n",
            "Step 2000 avg train loss = 5.5177\n",
            "Step 2000 avg train perplexity = 249.0717\n",
            "Validation loss after 2 epoch = 5.4454\n",
            "Validation perplexity after 2 epoch = 231.7009\n",
            "Step 0 avg train loss = 5.4878\n",
            "Step 0 avg train perplexity = 241.7334\n",
            "Step 1000 avg train loss = 5.3533\n",
            "Step 1000 avg train perplexity = 211.3018\n",
            "Step 2000 avg train loss = 5.3266\n",
            "Step 2000 avg train perplexity = 205.7292\n",
            "Validation loss after 3 epoch = 5.3716\n",
            "Validation perplexity after 3 epoch = 215.2102\n",
            "Step 0 avg train loss = 5.1794\n",
            "Step 0 avg train perplexity = 177.5720\n",
            "Step 1000 avg train loss = 5.1918\n",
            "Step 1000 avg train perplexity = 179.7859\n",
            "Step 2000 avg train loss = 5.1761\n",
            "Step 2000 avg train perplexity = 176.9845\n",
            "Validation loss after 4 epoch = 5.3263\n",
            "Validation perplexity after 4 epoch = 205.6724\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.1, 'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.01, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_1:\n",
            "Step 0 avg train loss = 10.4190\n",
            "Step 0 avg train perplexity = 33490.3956\n",
            "Step 1000 avg train loss = 6.5142\n",
            "Step 1000 avg train perplexity = 674.6798\n",
            "Step 2000 avg train loss = 5.9019\n",
            "Step 2000 avg train perplexity = 365.7151\n",
            "Validation loss after 0 epoch = 5.5729\n",
            "Validation perplexity after 0 epoch = 263.1845\n",
            "Step 0 avg train loss = 5.6089\n",
            "Step 0 avg train perplexity = 272.8523\n",
            "Step 1000 avg train loss = 5.5356\n",
            "Step 1000 avg train perplexity = 253.5640\n",
            "Step 2000 avg train loss = 5.4541\n",
            "Step 2000 avg train perplexity = 233.7232\n",
            "Validation loss after 1 epoch = 5.4217\n",
            "Validation perplexity after 1 epoch = 226.2663\n",
            "Step 0 avg train loss = 5.2421\n",
            "Step 0 avg train perplexity = 189.0702\n",
            "Step 1000 avg train loss = 5.2311\n",
            "Step 1000 avg train perplexity = 186.9936\n",
            "Step 2000 avg train loss = 5.2185\n",
            "Step 2000 avg train perplexity = 184.6615\n",
            "Validation loss after 2 epoch = 5.3776\n",
            "Validation perplexity after 2 epoch = 216.5057\n",
            "Step 0 avg train loss = 5.1371\n",
            "Step 0 avg train perplexity = 170.2201\n",
            "Step 1000 avg train loss = 5.0453\n",
            "Step 1000 avg train perplexity = 155.2851\n",
            "Step 2000 avg train loss = 5.0585\n",
            "Step 2000 avg train perplexity = 157.3492\n",
            "Validation loss after 3 epoch = 5.3960\n",
            "Validation perplexity after 3 epoch = 220.5229\n",
            "Step 0 avg train loss = 4.9620\n",
            "Step 0 avg train perplexity = 142.8826\n",
            "Step 1000 avg train loss = 4.9164\n",
            "Step 1000 avg train perplexity = 136.5043\n",
            "Step 2000 avg train loss = 4.9467\n",
            "Step 2000 avg train perplexity = 140.7044\n",
            "Validation loss after 4 epoch = 5.4215\n",
            "Validation perplexity after 4 epoch = 226.2215\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.1, 'embed_dim': 64, 'hidden_dim': 200, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_2:\n",
            "Step 0 avg train loss = 10.4065\n",
            "Step 0 avg train perplexity = 33073.3997\n",
            "Step 1000 avg train loss = 6.8780\n",
            "Step 1000 avg train perplexity = 970.7133\n",
            "Step 2000 avg train loss = 6.1879\n",
            "Step 2000 avg train perplexity = 486.8021\n",
            "Validation loss after 0 epoch = 5.7618\n",
            "Validation perplexity after 0 epoch = 317.9184\n",
            "Step 0 avg train loss = 6.0280\n",
            "Step 0 avg train perplexity = 414.8875\n",
            "Step 1000 avg train loss = 5.7632\n",
            "Step 1000 avg train perplexity = 318.3791\n",
            "Step 2000 avg train loss = 5.6431\n",
            "Step 2000 avg train perplexity = 282.3432\n",
            "Validation loss after 1 epoch = 5.4779\n",
            "Validation perplexity after 1 epoch = 239.3430\n",
            "Step 0 avg train loss = 5.4331\n",
            "Step 0 avg train perplexity = 228.8538\n",
            "Step 1000 avg train loss = 5.3941\n",
            "Step 1000 avg train perplexity = 220.1022\n",
            "Step 2000 avg train loss = 5.3452\n",
            "Step 2000 avg train perplexity = 209.5996\n",
            "Validation loss after 2 epoch = 5.3476\n",
            "Validation perplexity after 2 epoch = 210.0939\n",
            "Step 0 avg train loss = 5.2071\n",
            "Step 0 avg train perplexity = 182.5665\n",
            "Step 1000 avg train loss = 5.1408\n",
            "Step 1000 avg train perplexity = 170.8452\n",
            "Step 2000 avg train loss = 5.1219\n",
            "Step 2000 avg train perplexity = 167.6499\n",
            "Validation loss after 3 epoch = 5.2828\n",
            "Validation perplexity after 3 epoch = 196.9146\n",
            "Step 0 avg train loss = 4.7377\n",
            "Step 0 avg train perplexity = 114.1667\n",
            "Step 1000 avg train loss = 4.9462\n",
            "Step 1000 avg train perplexity = 140.6434\n",
            "Step 2000 avg train loss = 4.9497\n",
            "Step 2000 avg train perplexity = 141.1290\n",
            "Validation loss after 4 epoch = 5.2600\n",
            "Validation perplexity after 4 epoch = 192.4759\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.1, 'embed_dim': 64, 'hidden_dim': 200, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.01, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_3:\n",
            "Step 0 avg train loss = 10.4085\n",
            "Step 0 avg train perplexity = 33139.6711\n",
            "Step 1000 avg train loss = 6.4537\n",
            "Step 1000 avg train perplexity = 635.0537\n",
            "Step 2000 avg train loss = 5.8331\n",
            "Step 2000 avg train perplexity = 341.4055\n",
            "Validation loss after 0 epoch = 5.5102\n",
            "Validation perplexity after 0 epoch = 247.2042\n",
            "Step 0 avg train loss = 5.6411\n",
            "Step 0 avg train perplexity = 281.7731\n",
            "Step 1000 avg train loss = 5.4332\n",
            "Step 1000 avg train perplexity = 228.8791\n",
            "Step 2000 avg train loss = 5.3589\n",
            "Step 2000 avg train perplexity = 212.4836\n",
            "Validation loss after 1 epoch = 5.3462\n",
            "Validation perplexity after 1 epoch = 209.8124\n",
            "Step 0 avg train loss = 5.1358\n",
            "Step 0 avg train perplexity = 170.0019\n",
            "Step 1000 avg train loss = 5.0923\n",
            "Step 1000 avg train perplexity = 162.7603\n",
            "Step 2000 avg train loss = 5.0912\n",
            "Step 2000 avg train perplexity = 162.5835\n",
            "Validation loss after 2 epoch = 5.3118\n",
            "Validation perplexity after 2 epoch = 202.7214\n",
            "Step 0 avg train loss = 4.9065\n",
            "Step 0 avg train perplexity = 135.1588\n",
            "Step 1000 avg train loss = 4.8759\n",
            "Step 1000 avg train perplexity = 131.0953\n",
            "Step 2000 avg train loss = 4.9009\n",
            "Step 2000 avg train perplexity = 134.4076\n",
            "Validation loss after 3 epoch = 5.3363\n",
            "Validation perplexity after 3 epoch = 207.7390\n",
            "Step 0 avg train loss = 4.7686\n",
            "Step 0 avg train perplexity = 117.7522\n",
            "Step 1000 avg train loss = 4.7289\n",
            "Step 1000 avg train perplexity = 113.1669\n",
            "Step 2000 avg train loss = 4.7605\n",
            "Step 2000 avg train perplexity = 116.8002\n",
            "Validation loss after 4 epoch = 5.4041\n",
            "Validation perplexity after 4 epoch = 222.3141\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.5, 'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_4:\n",
            "Step 0 avg train loss = 10.4166\n",
            "Step 0 avg train perplexity = 33408.5723\n",
            "Step 1000 avg train loss = 6.8953\n",
            "Step 1000 avg train perplexity = 987.5745\n",
            "Step 2000 avg train loss = 6.2917\n",
            "Step 2000 avg train perplexity = 540.0922\n",
            "Validation loss after 0 epoch = 5.8963\n",
            "Validation perplexity after 0 epoch = 363.7034\n",
            "Step 0 avg train loss = 6.0154\n",
            "Step 0 avg train perplexity = 409.6951\n",
            "Step 1000 avg train loss = 5.9593\n",
            "Step 1000 avg train perplexity = 387.3491\n",
            "Step 2000 avg train loss = 5.8473\n",
            "Step 2000 avg train perplexity = 346.3073\n",
            "Validation loss after 1 epoch = 5.6196\n",
            "Validation perplexity after 1 epoch = 275.7862\n",
            "Step 0 avg train loss = 5.8436\n",
            "Step 0 avg train perplexity = 345.0094\n",
            "Step 1000 avg train loss = 5.6601\n",
            "Step 1000 avg train perplexity = 287.1762\n",
            "Step 2000 avg train loss = 5.6145\n",
            "Step 2000 avg train perplexity = 274.3806\n",
            "Validation loss after 2 epoch = 5.5082\n",
            "Validation perplexity after 2 epoch = 246.6955\n",
            "Step 0 avg train loss = 5.2676\n",
            "Step 0 avg train perplexity = 193.9494\n",
            "Step 1000 avg train loss = 5.4770\n",
            "Step 1000 avg train perplexity = 239.1212\n",
            "Step 2000 avg train loss = 5.4627\n",
            "Step 2000 avg train perplexity = 235.7290\n",
            "Validation loss after 3 epoch = 5.4418\n",
            "Validation perplexity after 3 epoch = 230.8568\n",
            "Step 0 avg train loss = 5.3827\n",
            "Step 0 avg train perplexity = 217.6069\n",
            "Step 1000 avg train loss = 5.3449\n",
            "Step 1000 avg train perplexity = 209.5292\n",
            "Step 2000 avg train loss = 5.3352\n",
            "Step 2000 avg train perplexity = 207.5196\n",
            "Validation loss after 4 epoch = 5.3983\n",
            "Validation perplexity after 4 epoch = 221.0230\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.5, 'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.01, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_5:\n",
            "Step 0 avg train loss = 10.4034\n",
            "Step 0 avg train perplexity = 32971.4267\n",
            "Step 1000 avg train loss = 6.5362\n",
            "Step 1000 avg train perplexity = 689.6327\n",
            "Step 2000 avg train loss = 5.9557\n",
            "Step 2000 avg train perplexity = 385.9616\n",
            "Validation loss after 0 epoch = 5.5898\n",
            "Validation perplexity after 0 epoch = 267.6892\n",
            "Step 0 avg train loss = 5.7759\n",
            "Step 0 avg train perplexity = 322.4258\n",
            "Step 1000 avg train loss = 5.6207\n",
            "Step 1000 avg train perplexity = 276.0940\n",
            "Step 2000 avg train loss = 5.5587\n",
            "Step 2000 avg train perplexity = 259.4913\n",
            "Validation loss after 1 epoch = 5.4474\n",
            "Validation perplexity after 1 epoch = 232.1498\n",
            "Step 0 avg train loss = 5.3434\n",
            "Step 0 avg train perplexity = 209.2320\n",
            "Step 1000 avg train loss = 5.3641\n",
            "Step 1000 avg train perplexity = 213.6019\n",
            "Step 2000 avg train loss = 5.3540\n",
            "Step 2000 avg train perplexity = 211.4422\n",
            "Validation loss after 2 epoch = 5.3896\n",
            "Validation perplexity after 2 epoch = 219.1119\n",
            "Step 0 avg train loss = 5.1906\n",
            "Step 0 avg train perplexity = 179.5838\n",
            "Step 1000 avg train loss = 5.1989\n",
            "Step 1000 avg train perplexity = 181.0813\n",
            "Step 2000 avg train loss = 5.2153\n",
            "Step 2000 avg train perplexity = 184.0759\n",
            "Validation loss after 3 epoch = 5.4055\n",
            "Validation perplexity after 3 epoch = 222.6168\n",
            "Step 0 avg train loss = 5.1173\n",
            "Step 0 avg train perplexity = 166.8869\n",
            "Step 1000 avg train loss = 5.0782\n",
            "Step 1000 avg train perplexity = 160.4899\n",
            "Step 2000 avg train loss = 5.1116\n",
            "Step 2000 avg train perplexity = 165.9299\n",
            "Validation loss after 4 epoch = 5.4257\n",
            "Validation perplexity after 4 epoch = 227.1681\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.5, 'embed_dim': 64, 'hidden_dim': 200, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_6:\n",
            "Step 0 avg train loss = 10.4037\n",
            "Step 0 avg train perplexity = 32982.0250\n",
            "Step 1000 avg train loss = 6.8394\n",
            "Step 1000 avg train perplexity = 933.8888\n",
            "Step 2000 avg train loss = 6.2434\n",
            "Step 2000 avg train perplexity = 514.6106\n",
            "Validation loss after 0 epoch = 5.8149\n",
            "Validation perplexity after 0 epoch = 335.2737\n",
            "Step 0 avg train loss = 5.9597\n",
            "Step 0 avg train perplexity = 387.4887\n",
            "Step 1000 avg train loss = 5.8505\n",
            "Step 1000 avg train perplexity = 347.3965\n",
            "Step 2000 avg train loss = 5.7354\n",
            "Step 2000 avg train perplexity = 309.6393\n",
            "Validation loss after 1 epoch = 5.5421\n",
            "Validation perplexity after 1 epoch = 255.2077\n",
            "Step 0 avg train loss = 5.3160\n",
            "Step 0 avg train perplexity = 203.5646\n",
            "Step 1000 avg train loss = 5.5234\n",
            "Step 1000 avg train perplexity = 250.4752\n",
            "Step 2000 avg train loss = 5.4697\n",
            "Step 2000 avg train perplexity = 237.4007\n",
            "Validation loss after 2 epoch = 5.4192\n",
            "Validation perplexity after 2 epoch = 225.7023\n",
            "Step 0 avg train loss = 5.2567\n",
            "Step 0 avg train perplexity = 191.8440\n",
            "Step 1000 avg train loss = 5.3041\n",
            "Step 1000 avg train perplexity = 201.1653\n",
            "Step 2000 avg train loss = 5.2859\n",
            "Step 2000 avg train perplexity = 197.5329\n",
            "Validation loss after 3 epoch = 5.3566\n",
            "Validation perplexity after 3 epoch = 212.0028\n",
            "Step 0 avg train loss = 5.2262\n",
            "Step 0 avg train perplexity = 186.0819\n",
            "Step 1000 avg train loss = 5.1401\n",
            "Step 1000 avg train perplexity = 170.7321\n",
            "Step 2000 avg train loss = 5.1386\n",
            "Step 2000 avg train perplexity = 170.4849\n",
            "Validation loss after 4 epoch = 5.3296\n",
            "Validation perplexity after 4 epoch = 206.3457\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.5, 'embed_dim': 64, 'hidden_dim': 200, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.01, 'num_epochs': 5, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_7:\n",
            "Step 0 avg train loss = 10.4098\n",
            "Step 0 avg train perplexity = 33183.9152\n",
            "Step 1000 avg train loss = 6.4940\n",
            "Step 1000 avg train perplexity = 661.1484\n",
            "Step 2000 avg train loss = 5.9023\n",
            "Step 2000 avg train perplexity = 365.8908\n",
            "Validation loss after 0 epoch = 5.5534\n",
            "Validation perplexity after 0 epoch = 258.1151\n",
            "Step 0 avg train loss = 5.6647\n",
            "Step 0 avg train perplexity = 288.5108\n",
            "Step 1000 avg train loss = 5.5630\n",
            "Step 1000 avg train perplexity = 260.5979\n",
            "Validation loss after 1 epoch = 5.4119\n",
            "Validation perplexity after 1 epoch = 224.0558\n",
            "Step 0 avg train loss = 5.3190\n",
            "Step 0 avg train perplexity = 204.1889\n",
            "Step 1000 avg train loss = 5.2854\n",
            "Step 1000 avg train perplexity = 197.4372\n",
            "Step 2000 avg train loss = 5.2828\n",
            "Step 2000 avg train perplexity = 196.9244\n",
            "Validation loss after 2 epoch = 5.3779\n",
            "Validation perplexity after 2 epoch = 216.5720\n",
            "Step 0 avg train loss = 4.8536\n",
            "Step 0 avg train perplexity = 128.2029\n",
            "Step 1000 avg train loss = 5.0988\n",
            "Step 1000 avg train perplexity = 163.8246\n",
            "Step 2000 avg train loss = 5.1269\n",
            "Step 2000 avg train perplexity = 168.4928\n",
            "Validation loss after 3 epoch = 5.4038\n",
            "Validation perplexity after 3 epoch = 222.2437\n",
            "Step 0 avg train loss = 4.9869\n",
            "Step 0 avg train perplexity = 146.4857\n",
            "Step 1000 avg train loss = 4.9712\n",
            "Step 1000 avg train perplexity = 144.2011\n",
            "Step 2000 avg train loss = 5.0082\n",
            "Step 2000 avg train perplexity = 149.6309\n",
            "Validation loss after 4 epoch = 5.4399\n",
            "Validation perplexity after 4 epoch = 230.4104\n",
            "Finished training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jayNuTfjHYs-",
        "outputId": "d240a4d0-7371-4dde-fc71-5a14e2ce8aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# find best comb (lowest validation loss)\n",
        "sorted(finetune_res.items(), key=lambda x: x[1][1][-1][1])[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('LSTM_Finetuned_2',\n",
              " ({'batch_first': True,\n",
              "   'dropout': 0.1,\n",
              "   'embed_dim': 64,\n",
              "   'hidden_dim': 200,\n",
              "   'lr': 0.001,\n",
              "   'num_epochs': 5,\n",
              "   'num_layers': 2,\n",
              "   'optimizer': 'Adam',\n",
              "   'padding_idx': 2,\n",
              "   'vocab_size': 33181,\n",
              "   'weight_decay': 0},\n",
              "  [(6.187857599258423,\n",
              "    5.7617946318860325,\n",
              "    486.80206313338266,\n",
              "    317.9183636962075),\n",
              "   (5.64312326002121, 5.47789755947185, 282.3431739613465, 239.3429736567724),\n",
              "   (5.345199208259583,\n",
              "    5.347554626104967,\n",
              "    209.59963441865992,\n",
              "    210.09391102340302),\n",
              "   (5.121877596855164,\n",
              "    5.282769904946381,\n",
              "    167.64985312909596,\n",
              "    196.91455524837647),\n",
              "   (4.94967449760437,\n",
              "    5.259970715360821,\n",
              "    141.12901861057756,\n",
              "    192.47585462875875)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-bO3PC2KfcXs"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Jh1-FBYfcXt"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_cq4NDNTTh0",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import umap\n",
        "except:\n",
        "    !pip install umap-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qc2COexlfcXt",
        "outputId": "f8bbeaa4-ff85-4134-e87f-535eced01929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "%pylab inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EsPkFZOkfcXw",
        "outputId": "339f29b6-9359-402a-ae41-8f4b12fa46a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2LJWwkZcfcXy"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ioLIuLUvfcX0",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def compute_cosine_similarity(emb_matrix):\n",
        "    if current_device == 'CUDA':\n",
        "        emb_matrix = emb_matrix.cpu()\n",
        "    product = torch.mm(emb_matrix, emb_matrix.T) \n",
        "\n",
        "    return product "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9u2IQKseLPw9",
        "outputId": "5f8eccd2-012d-443a-b36b-fc4282c3cb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "PATH = 'LSTM_LM_Finetuned_0.pth'\n",
        "best_model = torch.load(PATH, map_location=current_device)\n",
        "distance_matrix=compute_cosine_similarity(best_model['lookup.weight'])\n",
        "words = ['the', 'run', 'dog', 'where', 'quick']\n",
        "words_ids = [wikitext_dict.get_id(i) for i in words]\n",
        "closest_words = []\n",
        "furtherest_words = []\n",
        "for word in words:\n",
        "    row_distance = distance_matrix[wikitext_dict.get_id(word)]\n",
        "    if current_device == 'CUDA':\n",
        "        row_distance=row_distance.cpu()\n",
        "    for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "        closest_words.append(wikitext_dict.get_token(i))\n",
        "    for i in row_distance.numpy().argsort()[:10]:\n",
        "        furtherest_words.append(wikitext_dict.get_token(i))\n",
        "    print(\"For <{}>:\".format(word))\n",
        "    print(\"the most similar words are: \", closest_words)\n",
        "    print(\"the least similar words are: \", furtherest_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For <the>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose']\n",
            "For <run>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv']\n",
            "For <dog>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits']\n",
            "For <where>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help', 'where', 'what', ',', 'injuring', 'hires', 'joint', 'they', 'Whitehead', 'ensure', 'whom']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits', 'Livin', 'co', 'Publishers', '5191', 'Nature', 'pesos', '5', 'au', 'widening', 'Corey']\n",
            "For <quick>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help', 'where', 'what', ',', 'injuring', 'hires', 'joint', 'they', 'Whitehead', 'ensure', 'whom', 'quick', 'bestselling', 'Tawakal', 'reserve', 'peaked', 'Djan', 'Cassino', 'lens', 'NBA', '364']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits', 'Livin', 'co', 'Publishers', '5191', 'Nature', 'pesos', '5', 'au', 'widening', 'Corey', 'Megastore', 'because', 'Carre', 'Model', 'Smithy', 'Indonesia', 'what', 'Beloved', 'Swamp', 'anatomy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B64VdHasfcX4"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "22lYpwzsfcX5",
        "colab": {}
      },
      "source": [
        "words_selected = words + closest_words + furtherest_words\n",
        "words_selected_ids = [wikitext_dict.get_id(i) for i in words_selected]\n",
        "umap_plot(distance_matrix, words_selected_ids, words_selected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6aDwB3jHfcYA"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCzDobrwfcYA",
        "colab": {}
      },
      "source": [
        "distributed_distance_matrix=compute_cosine_similarity(best_model['projection.weight'])\n",
        "closest_words2 = []\n",
        "furtherest_words2 = []\n",
        "for word in words:\n",
        "  row_distance = distributed_distance_matrix[wikitext_dict.get_id(word)]\n",
        "  if current_device = 'CUDA':\n",
        "    row_distance=row_distance.cpu()\n",
        "  for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "    closest_words2.append(wikitext_dict.get_token(i))\n",
        "  for i in row_distance.numpy().argsort()[10:]:\n",
        "    furtherest_words2.append(wikitext_dict.get_token(i))\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words2)\n",
        "  print(\"the least similar words are: \", furtherest_words2)\n",
        "\n",
        "words_selected2 = words + closest_words2 + furtherest_words2\n",
        "words_selected_ids2 = [wikitext_dict.get_id(i) for i in words_selected2]\n",
        "umap_plot(distributed_distance_matrix, words_selected_ids2, words_selected2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-q_6ZeVbfcYD"
      },
      "source": [
        "Discussion of Results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HtgckC9xfcYD"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XsuFyfyS8hg",
        "colab": {}
      },
      "source": [
        "def compute_score(logits):\n",
        "  return F.log_softmax(logits, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7MDN0NX4fcYH"
      },
      "source": [
        "#### II.3.2 \n",
        "Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jpkn6pkMfcYE",
        "colab": {}
      },
      "source": [
        "model=best_model\n",
        "model.eval()\n",
        "scores = {} # key: sequence, value: score\n",
        "with torch.no_grad():\n",
        "    for i, (inp, target) in enumerate(loaders['valid']):\n",
        "        inp = inp.to(current_device)\n",
        "        target = target.to(current_device)\n",
        "        device = torch.device(\"cuda\")\n",
        "        logits = model(inp)\n",
        "        # compute score\n",
        "        seq_score = F.log_softmax(logits, dim=1)\n",
        "        scores[(inp,target)] = seq_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMBtQSr5ShyS",
        "colab": {}
      },
      "source": [
        "top_scores = sorted(scores.items(), key=lambda x: x[1], inverse=True)[:10]\n",
        "top_sequences = [seq+tag for seq_tag, score in topscores for (seq, tag) in seq_tag]\n",
        "lowest_scores = sorted(scores.items(), key=lambda x: x[1])[:10]\n",
        "lowest_sequences = [seq+tag for seq_tag, score in lowest_scores for (seq, tag) in seq_tag]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5U6hOImfcYI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bvfYp0nHfcYS"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "96qrGetifcYS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UDZcyAJYfcYV"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLbk5inDqUsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = model.zero_state(batch_size)\n",
        "word = '<bos>'\n",
        "samples = []\n",
        "sentence = ''\n",
        "for _ in range(1000): # generate 1000 sequences\n",
        "  while word is not '<eos>':\n",
        "    logits, states = model(word)\n",
        "    word = wikitext_dict.get_token(compute_predictions(logits))\n",
        "    sentence += ' '\n",
        "    sentence += word\n",
        "  samples.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYazCjIqtT-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly sample 1000 examples from the validation set\n",
        "from random import sample \n",
        "valid_samples = sample(datasets['valid'], 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8HBA1Z5kfcYZ"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sr2n-ec9fcYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f5c729b3-d44b-40b0-9718-3b883c737032"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# validation set\n",
        "all_words_valid = [word for lst in valid_samples for word in lst]\n",
        "num_unique_valid = len(set(all_words_valid))\n",
        "print(num_unique_valid)\n",
        "seq_len_valid = [len(lst) for lst in valid_samples]\n",
        "print(max(seq_len_valid))\n",
        "print(min(seq_len_valid))\n",
        "plt.hist(seq_len_valid)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4915\n",
            "88\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([286., 317., 221., 104.,  34.,  23.,   8.,   5.,   1.,   1.]),\n",
              " array([ 8., 16., 24., 32., 40., 48., 56., 64., 72., 80., 88.]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mdM+glqXMGvSR1zqCXpM79Hw6n/Id3sWyTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqtVEaR0xyq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7i24wZxfcYi"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fyes6PHWfcYi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}