{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimiw0821/DS-GA_1011_NLP/blob/master/HW2/lm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLYAo3OBfcVk",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKioHyAAfcVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "cf8984f7-f605-4874-8839-422d44949b15"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing the package, RESTART THIS CELL\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfhwSMNPfcVt",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpEk23hBfcVv",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3FyElwEfcVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlA8mEVfcV1",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WHZwSwkfcV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCNXYS4lfcWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "6914ba18-1048-461a-a5aa-c0c821b2a667"
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-30 20:22:21--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 20:22:21--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 20:22:21--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!C4meo90Q14coIzlnGBjtbVJsKhPTg52zL_GX4flZ6LSzALhTy-fgObnN-3g6qboQDO16LGm2-UtmB1i5GF6-p9sdRqiyq6yh_bcF2iJjBbuFfgRwVK5AKGcp97oIvdlC4JkgGmF1Y34aDPESGnwyGJQTMXbOhxuXqJwi--wdVOqW01WUp1RIm9we8pUOF7gsodi-TCaWCVHu1LMb5DkSuomN0h1r_U0es6wvq1EYgtHKzaSb-PUF0gWcoE7aQ8EfCh6eX4oCXCGGScNCc17Wz-fcAJeil6GyufP2Dq9s5e86GSSSaCQ5Y-sH2cFitc-Af0P_mntY1kdLSRrJsYbZG9ca9fZrKTNsN6Y3PMok52JoLNXnKOdxV3NIlhXtB37xh1zo4lJ0WfwU5Uoghd4SIvScRIufXyuMr_kAsEBMoe2SfQ51aY_K3eAyw3Hn1rrN4bsCFf2eGZH4yA8Kl0QS3k4bTzzEbuUGfEpktag65Qp2HfVTFHA3nZV8gB-EDjnj2iEX5oP7TlxF_LsPhZyPLAWYsGFVZM9F6v4qpaSHu70o0GCwdLy6hgqIcWLp-7SIkS0pxYvGx4XOJGl4RMHRKEvd7lb0lVAzJ6h847C04FquvFR1enlVuw0tGVSK7ZDpVrAkMb9va-NoX9JjkVcabskK-Y54GSCuMpnW8xLINbGSS6ocU6etOIJxuMCmjOW1FzK9v2ArNleVtEbQZA7ekJoEht0D0Tu87VT5npgNFoECL8v1q5k9di474eny9OgTYTKVSGsTmeA6PLzEv32wtHEA2ov6VaB4a0c6Yx5tQV2AqU0Jx45_5TfL9ZCVMavgPOWkUsWJoigG96Z-nZVVbhZz3zP_K3AbOz9NlmO8_TWa_h4Mgo3y3tSi9ZoqCnimy9G9jAqlof0MCg2-G2_X06x7FJ7cZJPw2GAVOmQ5rcnfrf3nnXYOHG5_SgKTGackaVNvmCjQj_i_ubIbijXVbVZ2qVyCgg3H2I4pinPlOsAwyXIUMp7U3-VkSLjh_oBp6ArBjXapgLJanJaz_StjRMGu9ZO59HopP51U4JLfEYy8eFPe-ekCwhAaqJDtjISSphA0f6b-EiJ73ejwa2uJYV2Q4GsuSy8ziZ_MhvJxe8bthJzmc9Ggp778riOaLLZ6uc4vIuKjxjYhOvKscwqHKSJf1rfHaWpADF4llpAXMGUKa7EcGQyF2-Y20h29LKoyyZJnrpX5bVrL2MMobkhQHbUIzMGEU5sRcM2wVYwjtvqJJBVUSmKIS1ebubPx9SCYbE1lvqrofTPwJfk9dgcfQr9ovdKkaZ3xAVz9-6crDdSbOoaRPNZ79yXUNxNfhrJguRmPvP3kiBFzQgF8IYxhwhA2Hsd_JsLs0It_LN8hSna0J79wcAthaH4oALWjTho6CTvibMiB_HJuCg4./download [following]\n",
            "--2019-09-30 20:22:22--  https://public.boxcloud.com/d/1/b1!C4meo90Q14coIzlnGBjtbVJsKhPTg52zL_GX4flZ6LSzALhTy-fgObnN-3g6qboQDO16LGm2-UtmB1i5GF6-p9sdRqiyq6yh_bcF2iJjBbuFfgRwVK5AKGcp97oIvdlC4JkgGmF1Y34aDPESGnwyGJQTMXbOhxuXqJwi--wdVOqW01WUp1RIm9we8pUOF7gsodi-TCaWCVHu1LMb5DkSuomN0h1r_U0es6wvq1EYgtHKzaSb-PUF0gWcoE7aQ8EfCh6eX4oCXCGGScNCc17Wz-fcAJeil6GyufP2Dq9s5e86GSSSaCQ5Y-sH2cFitc-Af0P_mntY1kdLSRrJsYbZG9ca9fZrKTNsN6Y3PMok52JoLNXnKOdxV3NIlhXtB37xh1zo4lJ0WfwU5Uoghd4SIvScRIufXyuMr_kAsEBMoe2SfQ51aY_K3eAyw3Hn1rrN4bsCFf2eGZH4yA8Kl0QS3k4bTzzEbuUGfEpktag65Qp2HfVTFHA3nZV8gB-EDjnj2iEX5oP7TlxF_LsPhZyPLAWYsGFVZM9F6v4qpaSHu70o0GCwdLy6hgqIcWLp-7SIkS0pxYvGx4XOJGl4RMHRKEvd7lb0lVAzJ6h847C04FquvFR1enlVuw0tGVSK7ZDpVrAkMb9va-NoX9JjkVcabskK-Y54GSCuMpnW8xLINbGSS6ocU6etOIJxuMCmjOW1FzK9v2ArNleVtEbQZA7ekJoEht0D0Tu87VT5npgNFoECL8v1q5k9di474eny9OgTYTKVSGsTmeA6PLzEv32wtHEA2ov6VaB4a0c6Yx5tQV2AqU0Jx45_5TfL9ZCVMavgPOWkUsWJoigG96Z-nZVVbhZz3zP_K3AbOz9NlmO8_TWa_h4Mgo3y3tSi9ZoqCnimy9G9jAqlof0MCg2-G2_X06x7FJ7cZJPw2GAVOmQ5rcnfrf3nnXYOHG5_SgKTGackaVNvmCjQj_i_ubIbijXVbVZ2qVyCgg3H2I4pinPlOsAwyXIUMp7U3-VkSLjh_oBp6ArBjXapgLJanJaz_StjRMGu9ZO59HopP51U4JLfEYy8eFPe-ekCwhAaqJDtjISSphA0f6b-EiJ73ejwa2uJYV2Q4GsuSy8ziZ_MhvJxe8bthJzmc9Ggp778riOaLLZ6uc4vIuKjxjYhOvKscwqHKSJf1rfHaWpADF4llpAXMGUKa7EcGQyF2-Y20h29LKoyyZJnrpX5bVrL2MMobkhQHbUIzMGEU5sRcM2wVYwjtvqJJBVUSmKIS1ebubPx9SCYbE1lvqrofTPwJfk9dgcfQr9ovdKkaZ3xAVz9-6crDdSbOoaRPNZ79yXUNxNfhrJguRmPvP3kiBFzQgF8IYxhwhA2Hsd_JsLs0It_LN8hSna0J79wcAthaH4oALWjTho6CTvibMiB_HJuCg4./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  11.2MB/s    in 1.1s    \n",
            "\n",
            "2019-09-30 20:22:24 (11.2 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTM0XO8IfcWK",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xTnBuAfcWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, alpha, gamma, vsize):\n",
        "        self.n = n\n",
        "#         self.lam = lam\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.vsize = vsize\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for n in range(1, self.n+1):\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        \n",
        "        if self.total[prefix] > 0:\n",
        "            prob = (self.count[prefix][word] / self.total[prefix]) * self.alpha\n",
        "        else:\n",
        "            prob = self.gamma * self.ngram_prob(ngram[1:])\n",
        "        \n",
        "#         if len(ngram) >= 2:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*self.ngram_prob(ngram[1:])\n",
        "#         elif len(ngram) == 1:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*1./self.vsize\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuS45FMOfcWM",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAj7JKyfcWN",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c5972dd-be14-48f3-8f3c-f89f245f9ba6"
      },
      "source": [
        "n = 2\n",
        "for lambda_ in np.linspace(0.1,1,10):\n",
        "    lm2 = NGramInterpolation(n=n, lam=lambda_, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm2.estimate(datasets['train'])\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['train'])))\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Valid Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When lambda = 0.1\n",
            "Baseline (Interpolation, n=2, delta=0.1000)) Train Perplexity: 517.683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3a96ce3a6820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Valid Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-98f3e2a94251>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogp_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogp_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_logp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mn_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogp_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36msequence_logp\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtotal_logp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_logp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36mngram_prob\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqUJUo4mfcWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox4t2hMXfcWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPLub2RsfcWY",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbG5lE3CfcWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM9ArzbEfcWc",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR28L4vPfcWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43KrHPW1fcWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlckZW9zfcWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otoIoQF0fcWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "55c803f2-2d0b-4cc7-9d30-3043ef9d9941"
      },
      "source": [
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [01:50<00:00, 706.97it/s]\n",
            "100%|██████████| 8464/8464 [00:08<00:00, 970.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x7xOMnbfcWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw1YalFTfcWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = wikitext_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4qo745ffcWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "4909970b-1e66-497e-8513-aab1af3034a3"
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:01<00:00, 68301.46it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 166258.86it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 158842.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiahWa0lfcW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ak-71_1fcW5",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXh2hKp7fcW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making a FFNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRxfJc-EfcXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RnnLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.rnn = nn.RNN(self.embed_dim, self.hidden_dim, self.num_layers, dropout=self.p, batch_first=self.batch_first)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmYZceKfcXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmLM(torch.nn.Module):\n",
        "    def __init__(self, options):\n",
        "        '''\n",
        "        params:\n",
        "            @options: dictionary of model parameters\n",
        "        '''\n",
        "        super(LstmLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim, self.num_layers, batch_first=self.batch_first, dropout=self.p) # lstm takes word embeddings as inputs and outputs hidden states (dim=hidden_dinm)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size) # linear layer maps from hidden states to word space\n",
        "\n",
        "    def forward(self, encoded_input_sequence):\n",
        "        '''\n",
        "        Forwrad method process the input from token ids to logits\n",
        "        params:\n",
        "            @inp: input sentence\n",
        "        '''\n",
        "        embedded = self.lookup(encoded_input_sequence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        logits = self.projection(lstm_out)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY90VuhCfcXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(logits):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags(words).\"\"\"\n",
        "    # Create softmax (F.softmax) function\n",
        "    softmax_output = F.softmax(logits, axis=-1)\n",
        "    predictions = torch.argmax(softmax_output, axis=-1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEbfMa_ffcXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a model, criterion and optimizer\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGd7jN0LfcXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model parameters -- options\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "options = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrCgjyqfcXc",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLMmIAY2Zwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "  '''\n",
        "  function that computes perplexity\n",
        "  '''\n",
        "  return 2**(loss/np.log(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHG18vvMfcXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "def train_model(model, model_name, hyperparams, loaders, save=True):\n",
        "    '''\n",
        "    function to train neural  LM\n",
        "    params:\n",
        "        @model: LM object\n",
        "        @model_name: str\n",
        "        @hyperparams: dictionary of hyperparameters set for the model\n",
        "        @loaders: DataLoader\n",
        "    '''\n",
        "    print(\"Training {}:\".format(model_name))\n",
        "    \n",
        "    # criterion:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "\n",
        "    PATH = model_name + '.pth'\n",
        "    if os.path.exists(PATH): # load pre-trained\n",
        "      checkpoint = torch.load(PATH)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    else:\n",
        "      # optimizer:\n",
        "      model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "      if hyperparams['optimizer'] == 'SGD':\n",
        "          optimizer = optim.SGD(model_params, lr=hyperparams['lr'], momentum=hyperparams['momentum'])\n",
        "      elif hyperparams['optimizer'] == 'Adam':\n",
        "          optimizer = optim.Adam(model_params, lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state_dict:\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "    # Print optimizer's state_dict\n",
        "    print(\"Optimizer's state_dict:\")\n",
        "    for var_name in optimizer.state_dict():\n",
        "        print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n",
        "    plot_cache = []\n",
        "    num_epochs = hyperparams['num_epochs']\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            # compute loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            # back-propogation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_log_cache.append(loss.item()) # store training loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                avg_train_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                avg_train_perplexity = perplexity(avg_train_loss)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_train_loss, prec=4))\n",
        "                print('Step {} avg train perplexity = {:.{prec}f}'.format(i, avg_train_perplexity, prec=4))\n",
        "                train_log_cache = []\n",
        "\n",
        "        #do validation\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(loaders['valid']):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                device = torch.device(\"cuda\")\n",
        "                logits = model(inp)\n",
        "                # compute loss\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item()) # store validation loss\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            avg_val_perplexity = perplexity(avg_val_loss)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch, avg_val_loss, prec=4))\n",
        "            print('Validation perplexity after {} epoch = {:.{prec}f}'.format(epoch, avg_val_perplexity, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_train_loss, avg_val_loss, avg_train_perplexity, avg_val_perplexity))\n",
        "        # # early stopping\n",
        "        # if len(plot_cache)>1:\n",
        "        #   np.abs((plot_cache[epoch][1] - plot_cache[epoch-1][1])/plot_cache[epoch-1][1]) <= 0.0005\n",
        "        #   print(\"Meets early stopping criteria: Finish training\")\n",
        "        #   return plot_cache\n",
        "    \n",
        "    if save:\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    print('Finished training')\n",
        "    return plot_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bYTI_8vfcXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss(losses):\n",
        "    epochs = np.array(list(range(len(losses))))\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    axes = fig.subplots(nrows=1, ncols=2)\n",
        "    # plot losses\n",
        "    axes[0].plot(epochs, [i[0] for i in losses], label='Train loss')\n",
        "    axes[0].plot(epochs, [i[1] for i in losses], label='Val loss')\n",
        "    axes[0].set_title(\"Training and Validation losses over time\")\n",
        "    axes[0].set_xlabel(\"Steps\")\n",
        "    axes[0].set_ylabel(\"Losses\")\n",
        "    axes[0].legend(loc='best')\n",
        "    # plot training & validation accuracy\n",
        "    axes[1].plot(epochs, [i[2] for i in losses], label='Train Perplexity')\n",
        "    axes[1].plot(epochs, [i[3] for i in losses], label='Val Perplexity')\n",
        "    axes[1].set_title(\"Training and Validation perplexity over time\")\n",
        "    axes[1].set_xlabel(\"Steps\")\n",
        "    axes[1].set_ylabel(\"Perplexity\")\n",
        "    axes[1].legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvEVnWnfcXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "7c78816e-0ede-4256-e67d-79d591e9dae4"
      },
      "source": [
        "# RNN with baseline hyperparameters\n",
        "baseline_hyperparams = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 10,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "model_rnn = RnnLM(options).to(current_device)\n",
        "print(model_rnn)\n",
        "base_rnn_losses = train_model(model_rnn, \"RNN_LM\", baseline_hyperparams, wikitext_loaders)\n",
        "plot_loss(base_rnn_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RnnLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training RNN_LM:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "rnn.weight_ih_l0 \t torch.Size([128, 64])\n",
            "rnn.weight_hh_l0 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l0 \t torch.Size([128])\n",
            "rnn.bias_hh_l0 \t torch.Size([128])\n",
            "rnn.weight_ih_l1 \t torch.Size([128, 128])\n",
            "rnn.weight_hh_l1 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l1 \t torch.Size([128])\n",
            "rnn.bias_hh_l1 \t torch.Size([128])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140704218383344, 140704218030152, 140704218033320, 140704218032960, 140704218032600, 140704218032888, 140704218030800, 140704218030512, 140704218031088, 140704218030656, 140704218031592]}]\n",
            "Step 0 avg train loss = 10.4280\n",
            "Step 0 avg train perplexity = 33792.2613\n",
            "Step 1000 avg train loss = 6.6343\n",
            "Step 1000 avg train perplexity = 760.7827\n",
            "Step 2000 avg train loss = 6.0561\n",
            "Step 2000 avg train perplexity = 426.7124\n",
            "Validation loss after 0 epoch = 5.7173\n",
            "Validation perplexity after 0 epoch = 304.0769\n",
            "Step 0 avg train loss = 5.7544\n",
            "Step 0 avg train perplexity = 315.5682\n",
            "Step 1000 avg train loss = 5.7078\n",
            "Step 1000 avg train perplexity = 301.1947\n",
            "Step 2000 avg train loss = 5.6420\n",
            "Step 2000 avg train perplexity = 282.0250\n",
            "Validation loss after 1 epoch = 5.5239\n",
            "Validation perplexity after 1 epoch = 250.6163\n",
            "Step 0 avg train loss = 5.3850\n",
            "Step 0 avg train perplexity = 218.1166\n",
            "Step 1000 avg train loss = 5.4272\n",
            "Step 1000 avg train perplexity = 227.5182\n",
            "Step 2000 avg train loss = 5.4071\n",
            "Step 2000 avg train perplexity = 222.9731\n",
            "Validation loss after 2 epoch = 5.4497\n",
            "Validation perplexity after 2 epoch = 232.6817\n",
            "Step 0 avg train loss = 5.2722\n",
            "Step 0 avg train perplexity = 194.8386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veibMk9afcXp",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4kjNcnr3w1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7778f3c5-09fb-43f3-b270-952d995ccb32"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = [64]\n",
        "hidden_dim = [128, 200]\n",
        "num_layers = [2, 5]\n",
        "dropout = [0.1, 0.3]\n",
        "options = {\n",
        "    'vocab_size': [len(wikitext_dict)],\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': [wikitext_dict.get_id('<pad>')],\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': [True],\n",
        "}\n",
        "\n",
        "regularized_hyperparams = {\n",
        "    'optimizer': ['Adam'],\n",
        "    'lr': [0.001],\n",
        "    'num_epochs': [10],\n",
        "    'weight_decay': [0, 0.05]\n",
        "}\n",
        "\n",
        "finetune_res = {}\n",
        "i=0\n",
        "for option in ParameterGrid(options):\n",
        "    for hyperparam in ParameterGrid(regularized_hyperparams):\n",
        "      # print({**option, **hyperparam})\n",
        "      model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "      print(model_lstm_tuned)\n",
        "      # train\n",
        "      model_name = 'LSTM_LM_Finetuned_' + str(i)\n",
        "      PATH = model_name + '.pth'\n",
        "      finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)\n",
        "      finetune_res[model_name]=({**option, **hyperparam}, finetune_lstm_losses)\n",
        "      i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_0:\n",
            "Step 0 avg train loss = 10.4045\n",
            "Step 0 avg train perplexity = 33008.8348\n",
            "Step 1000 avg train loss = 6.8793\n",
            "Step 1000 avg train perplexity = 971.9538\n",
            "Step 2000 avg train loss = 6.2653\n",
            "Step 2000 avg train perplexity = 526.0222\n",
            "Validation loss after 0 epoch = 5.8405\n",
            "Validation perplexity after 0 epoch = 343.9527\n",
            "Step 0 avg train loss = 5.8632\n",
            "Step 0 avg train perplexity = 351.8602\n",
            "Step 1000 avg train loss = 5.8763\n",
            "Step 1000 avg train perplexity = 356.4983\n",
            "Step 2000 avg train loss = 5.7495\n",
            "Step 2000 avg train perplexity = 314.0393\n",
            "Validation loss after 1 epoch = 5.5504\n",
            "Validation perplexity after 1 epoch = 257.3284\n",
            "Step 0 avg train loss = 5.5302\n",
            "Step 0 avg train perplexity = 252.1958\n",
            "Step 1000 avg train loss = 5.5366\n",
            "Step 1000 avg train perplexity = 253.8093\n",
            "Step 2000 avg train loss = 5.4868\n",
            "Step 2000 avg train perplexity = 241.4948\n",
            "Validation loss after 2 epoch = 5.4252\n",
            "Validation perplexity after 2 epoch = 227.0674\n",
            "Step 0 avg train loss = 5.4429\n",
            "Step 0 avg train perplexity = 231.1087\n",
            "Step 1000 avg train loss = 5.3174\n",
            "Step 1000 avg train perplexity = 203.8433\n",
            "Step 2000 avg train loss = 5.3007\n",
            "Step 2000 avg train perplexity = 200.4700\n",
            "Validation loss after 3 epoch = 5.3461\n",
            "Validation perplexity after 3 epoch = 209.7904\n",
            "Step 0 avg train loss = 5.3403\n",
            "Step 0 avg train perplexity = 208.5796\n",
            "Step 1000 avg train loss = 5.1551\n",
            "Step 1000 avg train perplexity = 173.3086\n",
            "Step 2000 avg train loss = 5.1457\n",
            "Step 2000 avg train perplexity = 171.6913\n",
            "Validation loss after 4 epoch = 5.3036\n",
            "Validation perplexity after 4 epoch = 201.0501\n",
            "Step 0 avg train loss = 5.0305\n",
            "Step 0 avg train perplexity = 153.0163\n",
            "Step 1000 avg train loss = 5.0217\n",
            "Step 1000 avg train perplexity = 151.6663\n",
            "Step 2000 avg train loss = 5.0268\n",
            "Step 2000 avg train perplexity = 152.4489\n",
            "Validation loss after 5 epoch = 5.2840\n",
            "Validation perplexity after 5 epoch = 197.1648\n",
            "Step 0 avg train loss = 4.9005\n",
            "Step 0 avg train perplexity = 134.3627\n",
            "Step 1000 avg train loss = 4.9165\n",
            "Step 1000 avg train perplexity = 136.5194\n",
            "Step 2000 avg train loss = 4.9225\n",
            "Step 2000 avg train perplexity = 137.3395\n",
            "Validation loss after 6 epoch = 5.2765\n",
            "Validation perplexity after 6 epoch = 195.6820\n",
            "Step 0 avg train loss = 4.9155\n",
            "Step 0 avg train perplexity = 136.3943\n",
            "Step 1000 avg train loss = 4.8232\n",
            "Step 1000 avg train perplexity = 124.3597\n",
            "Step 2000 avg train loss = 4.8377\n",
            "Step 2000 avg train perplexity = 126.1809\n",
            "Validation loss after 7 epoch = 5.2754\n",
            "Validation perplexity after 7 epoch = 195.4664\n",
            "Step 0 avg train loss = 4.7784\n",
            "Step 0 avg train perplexity = 118.9170\n",
            "Step 1000 avg train loss = 4.7439\n",
            "Step 1000 avg train perplexity = 114.8812\n",
            "Step 2000 avg train loss = 4.7674\n",
            "Step 2000 avg train perplexity = 117.6098\n",
            "Validation loss after 8 epoch = 5.2789\n",
            "Validation perplexity after 8 epoch = 196.1448\n",
            "Step 0 avg train loss = 4.7988\n",
            "Step 0 avg train perplexity = 121.3627\n",
            "Step 1000 avg train loss = 4.6765\n",
            "Step 1000 avg train perplexity = 107.3984\n",
            "Step 2000 avg train loss = 4.7026\n",
            "Step 2000 avg train perplexity = 110.2380\n",
            "Validation loss after 9 epoch = 5.2947\n",
            "Validation perplexity after 9 epoch = 199.2787\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_1:\n",
            "Step 0 avg train loss = 10.4155\n",
            "Step 0 avg train perplexity = 33373.4482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LstmLM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1000 avg train loss = 8.1659\n",
            "Step 1000 avg train perplexity = 3518.8269\n",
            "Step 2000 avg train loss = 7.9725\n",
            "Step 2000 avg train perplexity = 2900.0823\n",
            "Validation loss after 0 epoch = 7.8720\n",
            "Validation perplexity after 0 epoch = 2622.8098\n",
            "Step 0 avg train loss = 8.0096\n",
            "Step 0 avg train perplexity = 3009.6460\n",
            "Step 1000 avg train loss = 7.9878\n",
            "Step 1000 avg train perplexity = 2944.8541\n",
            "Step 2000 avg train loss = 7.9897\n",
            "Step 2000 avg train perplexity = 2950.3447\n",
            "Validation loss after 1 epoch = 7.8930\n",
            "Validation perplexity after 1 epoch = 2678.5686\n",
            "Step 0 avg train loss = 8.1563\n",
            "Step 0 avg train perplexity = 3485.0996\n",
            "Step 1000 avg train loss = 7.9861\n",
            "Step 1000 avg train perplexity = 2939.9298\n",
            "Step 2000 avg train loss = 7.9911\n",
            "Step 2000 avg train perplexity = 2954.5875\n",
            "Validation loss after 2 epoch = 7.8775\n",
            "Validation perplexity after 2 epoch = 2637.3390\n",
            "Step 0 avg train loss = 8.0078\n",
            "Step 0 avg train perplexity = 3004.3551\n",
            "Step 1000 avg train loss = 7.9903\n",
            "Step 1000 avg train perplexity = 2952.1592\n",
            "Step 2000 avg train loss = 7.9897\n",
            "Step 2000 avg train perplexity = 2950.4052\n",
            "Validation loss after 3 epoch = 7.9243\n",
            "Validation perplexity after 3 epoch = 2763.6682\n",
            "Step 0 avg train loss = 8.0399\n",
            "Step 0 avg train perplexity = 3102.3083\n",
            "Step 1000 avg train loss = 7.9902\n",
            "Step 1000 avg train perplexity = 2952.0075\n",
            "Step 2000 avg train loss = 7.9915\n",
            "Step 2000 avg train perplexity = 2955.8145\n",
            "Validation loss after 4 epoch = 7.9085\n",
            "Validation perplexity after 4 epoch = 2720.2002\n",
            "Step 0 avg train loss = 8.0117\n",
            "Step 0 avg train perplexity = 3015.9413\n",
            "Step 1000 avg train loss = 7.9869\n",
            "Step 1000 avg train perplexity = 2942.1033\n",
            "Step 2000 avg train loss = 7.9907\n",
            "Step 2000 avg train perplexity = 2953.4430\n",
            "Validation loss after 5 epoch = 7.8901\n",
            "Validation perplexity after 5 epoch = 2670.5797\n",
            "Step 0 avg train loss = 7.8696\n",
            "Step 0 avg train perplexity = 2616.5832\n",
            "Step 1000 avg train loss = 7.9881\n",
            "Step 1000 avg train perplexity = 2945.6201\n",
            "Step 2000 avg train loss = 7.9945\n",
            "Step 2000 avg train perplexity = 2964.5379\n",
            "Validation loss after 6 epoch = 7.9134\n",
            "Validation perplexity after 6 epoch = 2733.7883\n",
            "Step 0 avg train loss = 8.0142\n",
            "Step 0 avg train perplexity = 3023.6940\n",
            "Step 1000 avg train loss = 7.9908\n",
            "Step 1000 avg train perplexity = 2953.5335\n",
            "Step 2000 avg train loss = 7.9888\n",
            "Step 2000 avg train perplexity = 2947.7894\n",
            "Validation loss after 7 epoch = 7.9136\n",
            "Validation perplexity after 7 epoch = 2734.1175\n",
            "Step 0 avg train loss = 8.0147\n",
            "Step 0 avg train perplexity = 3024.9775\n",
            "Step 1000 avg train loss = 7.9871\n",
            "Step 1000 avg train perplexity = 2942.6401\n",
            "Step 2000 avg train loss = 7.9914\n",
            "Step 2000 avg train perplexity = 2955.5465\n",
            "Validation loss after 8 epoch = 7.8825\n",
            "Validation perplexity after 8 epoch = 2650.4560\n",
            "Step 0 avg train loss = 7.9418\n",
            "Step 0 avg train perplexity = 2812.5353\n",
            "Step 1000 avg train loss = 7.9906\n",
            "Step 1000 avg train perplexity = 2952.9231\n",
            "Step 2000 avg train loss = 7.9881\n",
            "Step 2000 avg train perplexity = 2945.7389\n",
            "Validation loss after 9 epoch = 7.9184\n",
            "Validation perplexity after 9 epoch = 2747.3615\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_2:\n",
            "Step 0 avg train loss = 10.4087\n",
            "Step 0 avg train perplexity = 33148.2054\n",
            "Step 1000 avg train loss = 7.0818\n",
            "Step 1000 avg train perplexity = 1190.1095\n",
            "Step 2000 avg train loss = 6.9147\n",
            "Step 2000 avg train perplexity = 1006.9370\n",
            "Validation loss after 0 epoch = 6.7177\n",
            "Validation perplexity after 0 epoch = 826.8771\n",
            "Step 0 avg train loss = 6.8864\n",
            "Step 0 avg train perplexity = 978.8410\n",
            "Step 1000 avg train loss = 6.8719\n",
            "Step 1000 avg train perplexity = 964.8163\n",
            "Step 2000 avg train loss = 6.8774\n",
            "Step 2000 avg train perplexity = 970.0671\n",
            "Validation loss after 1 epoch = 6.7070\n",
            "Validation perplexity after 1 epoch = 818.0745\n",
            "Step 0 avg train loss = 6.8066\n",
            "Step 0 avg train perplexity = 903.7579\n",
            "Step 1000 avg train loss = 6.8539\n",
            "Step 1000 avg train perplexity = 947.6095\n",
            "Step 2000 avg train loss = 6.8607\n",
            "Step 2000 avg train perplexity = 954.0571\n",
            "Validation loss after 2 epoch = 6.7091\n",
            "Validation perplexity after 2 epoch = 819.8487\n",
            "Step 0 avg train loss = 6.8129\n",
            "Step 0 avg train perplexity = 909.5324\n",
            "Step 1000 avg train loss = 6.8409\n",
            "Step 1000 avg train perplexity = 935.3397\n",
            "Step 2000 avg train loss = 6.8484\n",
            "Step 2000 avg train perplexity = 942.3971\n",
            "Validation loss after 3 epoch = 6.7129\n",
            "Validation perplexity after 3 epoch = 822.9806\n",
            "Step 0 avg train loss = 6.6584\n",
            "Step 0 avg train perplexity = 779.3406\n",
            "Step 1000 avg train loss = 6.8321\n",
            "Step 1000 avg train perplexity = 927.1632\n",
            "Step 2000 avg train loss = 6.8413\n",
            "Step 2000 avg train perplexity = 935.6645\n",
            "Validation loss after 4 epoch = 6.7131\n",
            "Validation perplexity after 4 epoch = 823.0852\n",
            "Step 0 avg train loss = 6.8460\n",
            "Step 0 avg train perplexity = 940.1319\n",
            "Step 1000 avg train loss = 6.8251\n",
            "Step 1000 avg train perplexity = 920.6990\n",
            "Step 2000 avg train loss = 6.8364\n",
            "Step 2000 avg train perplexity = 931.1135\n",
            "Validation loss after 5 epoch = 6.7120\n",
            "Validation perplexity after 5 epoch = 822.1918\n",
            "Step 0 avg train loss = 6.8157\n",
            "Step 0 avg train perplexity = 912.0113\n",
            "Step 1000 avg train loss = 6.8160\n",
            "Step 1000 avg train perplexity = 912.3019\n",
            "Step 2000 avg train loss = 6.8253\n",
            "Step 2000 avg train perplexity = 920.8792\n",
            "Validation loss after 6 epoch = 6.7177\n",
            "Validation perplexity after 6 epoch = 826.9441\n",
            "Step 0 avg train loss = 6.8107\n",
            "Step 0 avg train perplexity = 907.4898\n",
            "Step 1000 avg train loss = 6.8075\n",
            "Step 1000 avg train perplexity = 904.6198\n",
            "Step 2000 avg train loss = 6.8194\n",
            "Step 2000 avg train perplexity = 915.4241\n",
            "Validation loss after 7 epoch = 6.7246\n",
            "Validation perplexity after 7 epoch = 832.6267\n",
            "Step 0 avg train loss = 6.7258\n",
            "Step 0 avg train perplexity = 833.6204\n",
            "Step 1000 avg train loss = 6.8018\n",
            "Step 1000 avg train perplexity = 899.4367\n",
            "Step 2000 avg train loss = 6.8116\n",
            "Step 2000 avg train perplexity = 908.3186\n",
            "Validation loss after 8 epoch = 6.7303\n",
            "Validation perplexity after 8 epoch = 837.3750\n",
            "Step 0 avg train loss = 6.7954\n",
            "Step 0 avg train perplexity = 893.6931\n",
            "Step 1000 avg train loss = 6.7909\n",
            "Step 1000 avg train perplexity = 889.7469\n",
            "Step 2000 avg train loss = 6.8096\n",
            "Step 2000 avg train perplexity = 906.5489\n",
            "Validation loss after 9 epoch = 6.7361\n",
            "Validation perplexity after 9 epoch = 842.2673\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_3:\n",
            "Step 0 avg train loss = 10.4022\n",
            "Step 0 avg train perplexity = 32931.9252\n",
            "Step 1000 avg train loss = 8.2417\n",
            "Step 1000 avg train perplexity = 3795.9626\n",
            "Step 2000 avg train loss = 7.9923\n",
            "Step 2000 avg train perplexity = 2957.9873\n",
            "Validation loss after 0 epoch = 7.8854\n",
            "Validation perplexity after 0 epoch = 2658.1512\n",
            "Step 0 avg train loss = 7.9404\n",
            "Step 0 avg train perplexity = 2808.5269\n",
            "Step 1000 avg train loss = 7.9903\n",
            "Step 1000 avg train perplexity = 2952.3074\n",
            "Step 2000 avg train loss = 7.9900\n",
            "Step 2000 avg train perplexity = 2951.2180\n",
            "Validation loss after 1 epoch = 7.8988\n",
            "Validation perplexity after 1 epoch = 2693.9252\n",
            "Step 0 avg train loss = 8.0774\n",
            "Step 0 avg train perplexity = 3220.7044\n",
            "Step 1000 avg train loss = 7.9899\n",
            "Step 1000 avg train perplexity = 2950.8735\n",
            "Step 2000 avg train loss = 7.9864\n",
            "Step 2000 avg train perplexity = 2940.6583\n",
            "Validation loss after 2 epoch = 7.9011\n",
            "Validation perplexity after 2 epoch = 2700.2923\n",
            "Step 0 avg train loss = 7.8850\n",
            "Step 0 avg train perplexity = 2657.0826\n",
            "Step 1000 avg train loss = 7.9907\n",
            "Step 1000 avg train perplexity = 2953.2994\n",
            "Step 2000 avg train loss = 7.9886\n",
            "Step 2000 avg train perplexity = 2947.2880\n",
            "Validation loss after 3 epoch = 7.8661\n",
            "Validation perplexity after 3 epoch = 2607.4609\n",
            "Step 0 avg train loss = 7.9729\n",
            "Step 0 avg train perplexity = 2901.2434\n",
            "Step 1000 avg train loss = 7.9884\n",
            "Step 1000 avg train perplexity = 2946.6766\n",
            "Step 2000 avg train loss = 7.9932\n",
            "Step 2000 avg train perplexity = 2960.7020\n",
            "Validation loss after 4 epoch = 7.8966\n",
            "Validation perplexity after 4 epoch = 2688.1766\n",
            "Step 0 avg train loss = 8.0403\n",
            "Step 0 avg train perplexity = 3103.5630\n",
            "Step 1000 avg train loss = 7.9914\n",
            "Step 1000 avg train perplexity = 2955.5598\n",
            "Step 2000 avg train loss = 7.9877\n",
            "Step 2000 avg train perplexity = 2944.4286\n",
            "Validation loss after 5 epoch = 7.8861\n",
            "Validation perplexity after 5 epoch = 2660.0066\n",
            "Step 0 avg train loss = 7.9596\n",
            "Step 0 avg train perplexity = 2863.0367\n",
            "Step 1000 avg train loss = 7.9904\n",
            "Step 1000 avg train perplexity = 2952.4562\n",
            "Step 2000 avg train loss = 7.9890\n",
            "Step 2000 avg train perplexity = 2948.2283\n",
            "Validation loss after 6 epoch = 7.9297\n",
            "Validation perplexity after 6 epoch = 2778.6463\n",
            "Step 0 avg train loss = 7.9938\n",
            "Step 0 avg train perplexity = 2962.6750\n",
            "Step 1000 avg train loss = 7.9892\n",
            "Step 1000 avg train perplexity = 2948.8678\n",
            "Step 2000 avg train loss = 7.9890\n",
            "Step 2000 avg train perplexity = 2948.3181\n",
            "Validation loss after 7 epoch = 7.9022\n",
            "Validation perplexity after 7 epoch = 2703.1462\n",
            "Step 0 avg train loss = 7.9800\n",
            "Step 0 avg train perplexity = 2921.7960\n",
            "Step 1000 avg train loss = 7.9906\n",
            "Step 1000 avg train perplexity = 2952.9315\n",
            "Step 2000 avg train loss = 7.9925\n",
            "Step 2000 avg train perplexity = 2958.7432\n",
            "Validation loss after 8 epoch = 7.9006\n",
            "Validation perplexity after 8 epoch = 2698.9059\n",
            "Step 0 avg train loss = 7.9511\n",
            "Step 0 avg train perplexity = 2838.7848\n",
            "Step 1000 avg train loss = 7.9897\n",
            "Step 1000 avg train perplexity = 2950.5491\n",
            "Step 2000 avg train loss = 7.9895\n",
            "Step 2000 avg train perplexity = 2949.8309\n",
            "Validation loss after 9 epoch = 7.9064\n",
            "Validation perplexity after 9 epoch = 2714.6347\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_4:\n",
            "Step 0 avg train loss = 10.4194\n",
            "Step 0 avg train perplexity = 33503.9724\n",
            "Step 1000 avg train loss = 6.8830\n",
            "Step 1000 avg train perplexity = 975.5902\n",
            "Step 2000 avg train loss = 6.2030\n",
            "Step 2000 avg train perplexity = 494.2347\n",
            "Validation loss after 0 epoch = 5.7710\n",
            "Validation perplexity after 0 epoch = 320.8474\n",
            "Step 0 avg train loss = 5.9074\n",
            "Step 0 avg train perplexity = 367.7599\n",
            "Step 1000 avg train loss = 5.7645\n",
            "Step 1000 avg train perplexity = 318.7787\n",
            "Step 2000 avg train loss = 5.6415\n",
            "Step 2000 avg train perplexity = 281.8777\n",
            "Validation loss after 1 epoch = 5.4758\n",
            "Validation perplexity after 1 epoch = 238.8431\n",
            "Step 0 avg train loss = 5.5333\n",
            "Step 0 avg train perplexity = 252.9889\n",
            "Step 1000 avg train loss = 5.3971\n",
            "Step 1000 avg train perplexity = 220.7672\n",
            "Step 2000 avg train loss = 5.3374\n",
            "Step 2000 avg train perplexity = 207.9811\n",
            "Validation loss after 2 epoch = 5.3513\n",
            "Validation perplexity after 2 epoch = 210.8802\n",
            "Step 0 avg train loss = 5.0901\n",
            "Step 0 avg train perplexity = 162.4037\n",
            "Step 1000 avg train loss = 5.1367\n",
            "Step 1000 avg train perplexity = 170.1458\n",
            "Step 2000 avg train loss = 5.1227\n",
            "Step 2000 avg train perplexity = 167.7885\n",
            "Validation loss after 3 epoch = 5.2895\n",
            "Validation perplexity after 3 epoch = 198.2385\n",
            "Step 0 avg train loss = 4.9328\n",
            "Step 0 avg train perplexity = 138.7729\n",
            "Step 1000 avg train loss = 4.9456\n",
            "Step 1000 avg train perplexity = 140.5614\n",
            "Step 2000 avg train loss = 4.9517\n",
            "Step 2000 avg train perplexity = 141.4177\n",
            "Validation loss after 4 epoch = 5.2735\n",
            "Validation perplexity after 4 epoch = 195.0956\n",
            "Step 0 avg train loss = 4.6360\n",
            "Step 0 avg train perplexity = 103.1271\n",
            "Step 1000 avg train loss = 4.7966\n",
            "Step 1000 avg train perplexity = 121.0955\n",
            "Step 2000 avg train loss = 4.8136\n",
            "Step 2000 avg train perplexity = 123.1730\n",
            "Validation loss after 5 epoch = 5.2806\n",
            "Validation perplexity after 5 epoch = 196.4867\n",
            "Step 0 avg train loss = 4.7537\n",
            "Step 0 avg train perplexity = 116.0174\n",
            "Step 1000 avg train loss = 4.6698\n",
            "Step 1000 avg train perplexity = 106.6762\n",
            "Step 2000 avg train loss = 4.6960\n",
            "Step 2000 avg train perplexity = 109.5110\n",
            "Validation loss after 6 epoch = 5.2967\n",
            "Validation perplexity after 6 epoch = 199.6742\n",
            "Step 0 avg train loss = 4.6207\n",
            "Step 0 avg train perplexity = 101.5620\n",
            "Step 1000 avg train loss = 4.5668\n",
            "Step 1000 avg train perplexity = 96.2359\n",
            "Step 2000 avg train loss = 4.5917\n",
            "Step 2000 avg train perplexity = 98.6591\n",
            "Validation loss after 7 epoch = 5.3249\n",
            "Validation perplexity after 7 epoch = 205.3794\n",
            "Step 0 avg train loss = 4.2544\n",
            "Step 0 avg train perplexity = 70.4138\n",
            "Step 1000 avg train loss = 4.4752\n",
            "Step 1000 avg train perplexity = 87.8164\n",
            "Step 2000 avg train loss = 4.5063\n",
            "Step 2000 avg train perplexity = 90.5895\n",
            "Validation loss after 8 epoch = 5.3550\n",
            "Validation perplexity after 8 epoch = 211.6540\n",
            "Step 0 avg train loss = 4.2979\n",
            "Step 0 avg train perplexity = 73.5484\n",
            "Step 1000 avg train loss = 4.3942\n",
            "Step 1000 avg train perplexity = 80.9832\n",
            "Step 2000 avg train loss = 4.4348\n",
            "Step 2000 avg train perplexity = 84.3349\n",
            "Validation loss after 9 epoch = 5.3999\n",
            "Validation perplexity after 9 epoch = 221.3878\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_5:\n",
            "Step 0 avg train loss = 10.4065\n",
            "Step 0 avg train perplexity = 33074.0936\n",
            "Step 1000 avg train loss = 8.0544\n",
            "Step 1000 avg train perplexity = 3147.6084\n",
            "Step 2000 avg train loss = 7.9183\n",
            "Step 2000 avg train perplexity = 2746.9646\n",
            "Validation loss after 0 epoch = 7.8484\n",
            "Validation perplexity after 0 epoch = 2561.5412\n",
            "Step 0 avg train loss = 7.8774\n",
            "Step 0 avg train perplexity = 2636.9594\n",
            "Step 1000 avg train loss = 7.9402\n",
            "Step 1000 avg train perplexity = 2807.8523\n",
            "Step 2000 avg train loss = 7.9375\n",
            "Step 2000 avg train perplexity = 2800.2736\n",
            "Validation loss after 1 epoch = 7.9023\n",
            "Validation perplexity after 1 epoch = 2703.5606\n",
            "Step 0 avg train loss = 7.7949\n",
            "Step 0 avg train perplexity = 2428.2559\n",
            "Step 1000 avg train loss = 7.9361\n",
            "Step 1000 avg train perplexity = 2796.5331\n",
            "Step 2000 avg train loss = 7.9404\n",
            "Step 2000 avg train perplexity = 2808.4463\n",
            "Validation loss after 2 epoch = 7.8673\n",
            "Validation perplexity after 2 epoch = 2610.4136\n",
            "Step 0 avg train loss = 8.0397\n",
            "Step 0 avg train perplexity = 3101.8232\n",
            "Step 1000 avg train loss = 7.9422\n",
            "Step 1000 avg train perplexity = 2813.4549\n",
            "Step 2000 avg train loss = 7.9385\n",
            "Step 2000 avg train perplexity = 2803.0843\n",
            "Validation loss after 3 epoch = 7.8620\n",
            "Validation perplexity after 3 epoch = 2596.7462\n",
            "Step 0 avg train loss = 7.8486\n",
            "Step 0 avg train perplexity = 2562.0664\n",
            "Step 1000 avg train loss = 7.9391\n",
            "Step 1000 avg train perplexity = 2804.7549\n",
            "Step 2000 avg train loss = 7.9427\n",
            "Step 2000 avg train perplexity = 2815.0411\n",
            "Validation loss after 4 epoch = 7.8420\n",
            "Validation perplexity after 4 epoch = 2545.3374\n",
            "Step 0 avg train loss = 7.9574\n",
            "Step 0 avg train perplexity = 2856.5171\n",
            "Step 1000 avg train loss = 7.9396\n",
            "Step 1000 avg train perplexity = 2806.1872\n",
            "Step 2000 avg train loss = 7.9386\n",
            "Step 2000 avg train perplexity = 2803.3565\n",
            "Validation loss after 5 epoch = 7.8475\n",
            "Validation perplexity after 5 epoch = 2559.3158\n",
            "Step 0 avg train loss = 8.0692\n",
            "Step 0 avg train perplexity = 3194.4158\n",
            "Step 1000 avg train loss = 7.9411\n",
            "Step 1000 avg train perplexity = 2810.5372\n",
            "Step 2000 avg train loss = 7.9378\n",
            "Step 2000 avg train perplexity = 2801.2569\n",
            "Validation loss after 6 epoch = 7.8339\n",
            "Validation perplexity after 6 epoch = 2524.8568\n",
            "Step 0 avg train loss = 8.0327\n",
            "Step 0 avg train perplexity = 3079.9366\n",
            "Step 1000 avg train loss = 7.9393\n",
            "Step 1000 avg train perplexity = 2805.3388\n",
            "Step 2000 avg train loss = 7.9420\n",
            "Step 2000 avg train perplexity = 2812.9136\n",
            "Validation loss after 7 epoch = 7.8714\n",
            "Validation perplexity after 7 epoch = 2621.1562\n",
            "Step 0 avg train loss = 7.8534\n",
            "Step 0 avg train perplexity = 2574.5310\n",
            "Step 1000 avg train loss = 7.9373\n",
            "Step 1000 avg train perplexity = 2799.8043\n",
            "Step 2000 avg train loss = 7.9430\n",
            "Step 2000 avg train perplexity = 2815.6817\n",
            "Validation loss after 8 epoch = 7.8322\n",
            "Validation perplexity after 8 epoch = 2520.3983\n",
            "Step 0 avg train loss = 7.9228\n",
            "Step 0 avg train perplexity = 2759.3659\n",
            "Step 1000 avg train loss = 7.9417\n",
            "Step 1000 avg train perplexity = 2812.1836\n",
            "Step 2000 avg train loss = 7.9352\n",
            "Step 2000 avg train perplexity = 2794.0296\n",
            "Validation loss after 9 epoch = 7.8164\n",
            "Validation perplexity after 9 epoch = 2480.9592\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_6:\n",
            "Step 0 avg train loss = 10.4098\n",
            "Step 0 avg train perplexity = 33183.1874\n",
            "Step 1000 avg train loss = 7.0681\n",
            "Step 1000 avg train perplexity = 1173.8871\n",
            "Step 2000 avg train loss = 6.9136\n",
            "Step 2000 avg train perplexity = 1005.8454\n",
            "Validation loss after 0 epoch = 6.7228\n",
            "Validation perplexity after 0 epoch = 831.1034\n",
            "Step 0 avg train loss = 6.9344\n",
            "Step 0 avg train perplexity = 1027.0113\n",
            "Step 1000 avg train loss = 6.8681\n",
            "Step 1000 avg train perplexity = 961.1579\n",
            "Step 2000 avg train loss = 6.8733\n",
            "Step 2000 avg train perplexity = 966.1072\n",
            "Validation loss after 1 epoch = 6.7073\n",
            "Validation perplexity after 1 epoch = 818.3844\n",
            "Step 0 avg train loss = 6.6329\n",
            "Step 0 avg train perplexity = 759.6674\n",
            "Step 1000 avg train loss = 6.8493\n",
            "Step 1000 avg train perplexity = 943.2448\n",
            "Step 2000 avg train loss = 6.8605\n",
            "Step 2000 avg train perplexity = 953.8781\n",
            "Validation loss after 2 epoch = 6.7079\n",
            "Validation perplexity after 2 epoch = 818.8722\n",
            "Step 0 avg train loss = 6.7744\n",
            "Step 0 avg train perplexity = 875.1597\n",
            "Step 1000 avg train loss = 6.8397\n",
            "Step 1000 avg train perplexity = 934.1945\n",
            "Step 2000 avg train loss = 6.8459\n",
            "Step 2000 avg train perplexity = 940.0068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jayNuTfjHYs-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b22843bd-7227-428e-fe4a-5ae84f5843d0"
      },
      "source": [
        "# find best comb (lowest validation loss)\n",
        "sorted(finetune_res.items(), key=lambda x: x[1][1][-1][1])[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a2b0615fc324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetune_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'finetune_res' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bO3PC2KfcXs",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAundSFoka2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def compute_cosine_similarity(emb_matrix):\n",
        "  distance_matrix = np.zeros(emb_matrix.shape[0], emb_matrix.shape[0])\n",
        "  for i in range(emb_matrix.shape[0]):\n",
        "    for j in range(emb_matrix.shape[0]):\n",
        "      distance_matrix[i,j] = 1-spatial.distance.cosine(emb_matrix[i], emb_matrix[j])\n",
        "\n",
        "  return distance_matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvEFhwJntn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PATH=\n",
        "best_model = torch.load(PATH)\n",
        "distance_matrix=compute_cosine_similarity(best_model.state_dict()['lookup.weight'])\n",
        "words = ['the', 'run', 'dog', 'where', 'quick']\n",
        "for word in words:\n",
        "  row_distance = distance_matrix[wikitext_dict.get_id[word]]\n",
        "  closest_words = wikitext_dict.get_token[i for i in row_distance.argsort()[-10:][::-1]]\n",
        "  furtherest_words = wikitext_dict.get_token[i for i in row_distance.argsort()[10:]]\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words)\n",
        "  print(\"the least similar words are: \", furtherest_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jh1-FBYfcXt",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc2COexlfcXt",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7b85774-3646-4998-e6e0-d4228bd9ccd3"
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsPkFZOkfcXw",
        "colab_type": "code",
        "colab": {},
        "outputId": "a83c599e-a276-4d76-b00c-d7fc0a5d091b"
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LJWwkZcfcXy",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLIuLUvfcX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64VdHasfcX4",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22lYpwzsfcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDwB3jHfcYA",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCzDobrwfcYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q_6ZeVbfcYD",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtgckC9xfcYD",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpkn6pkMfcYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MDN0NX4fcYH",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5U6hOImfcYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvfYp0nHfcYS",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qrGetifcYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZcyAJYfcYV",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfmkPJ4fcYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBA1Z5kfcYZ",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2n-ec9fcYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7i24wZxfcYi",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyes6PHWfcYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}