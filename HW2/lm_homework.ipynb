{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimiw0821/DS-GA_1011_NLP/blob/master/HW2/lm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLYAo3OBfcVk",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKioHyAAfcVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfhwSMNPfcVt",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpEk23hBfcVv",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3FyElwEfcVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlA8mEVfcV1",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WHZwSwkfcV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCNXYS4lfcWF",
        "colab_type": "code",
        "outputId": "56de24a6-5aeb-4803-b31c-bd5ab4d539fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTM0XO8IfcWK",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xTnBuAfcWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, alpha, gamma, vsize):\n",
        "        self.n = n\n",
        "#         self.lam = lam\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.vsize = vsize\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for n in range(1, self.n+1):\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        \n",
        "        if self.total[prefix] > 0:\n",
        "            prob = (self.count[prefix][word] / self.total[prefix]) * self.alpha\n",
        "        else:\n",
        "            prob = self.gamma * self.ngram_prob(ngram[1:])\n",
        "        \n",
        "#         if len(ngram) >= 2:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*self.ngram_prob(ngram[1:])\n",
        "#         elif len(ngram) == 1:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*1./self.vsize\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuS45FMOfcWM",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAj7JKyfcWN",
        "colab_type": "code",
        "outputId": "3c5972dd-be14-48f3-8f3c-f89f245f9ba6",
        "colab": {}
      },
      "source": [
        "n = 2\n",
        "for lambda_ in np.linspace(0.1,1,10):\n",
        "    lm2 = NGramInterpolation(n=n, lam=lambda_, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm2.estimate(datasets['train'])\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['train'])))\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Valid Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When lambda = 0.1\n",
            "Baseline (Interpolation, n=2, delta=0.1000)) Train Perplexity: 517.683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3a96ce3a6820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Valid Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-98f3e2a94251>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogp_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogp_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_logp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mn_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogp_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36msequence_logp\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtotal_logp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_logp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36mngram_prob\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqUJUo4mfcWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox4t2hMXfcWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPLub2RsfcWY",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbG5lE3CfcWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM9ArzbEfcWc",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR28L4vPfcWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43KrHPW1fcWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlckZW9zfcWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otoIoQF0fcWo",
        "colab_type": "code",
        "outputId": "98e38f31-2d6b-498c-bf15-b0130e7a1023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:10<00:00, 601.89it/s]\n",
            "100%|██████████| 8464/8464 [00:10<00:00, 831.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x7xOMnbfcWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw1YalFTfcWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = wikitext_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4qo745ffcWx",
        "colab_type": "code",
        "outputId": "7e9a86dc-bbb1-419b-a02d-64872a445d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:01<00:00, 56034.30it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 159930.57it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 151690.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiahWa0lfcW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ak-71_1fcW5",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXh2hKp7fcW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making a FFNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRxfJc-EfcXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RnnLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.rnn = nn.RNN(self.embed_dim, self.hidden_dim, self.num_layers, dropout=self.p, batch_first=self.batch_first)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmYZceKfcXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmLM(torch.nn.Module):\n",
        "    def __init__(self, options):\n",
        "        '''\n",
        "        params:\n",
        "            @options: dictionary of model parameters\n",
        "        '''\n",
        "        super(LstmLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim, self.num_layers, batch_first=self.batch_first, dropout=self.p) # lstm takes word embeddings as inputs and outputs hidden states (dim=hidden_dinm)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size) # linear layer maps from hidden states to word space\n",
        "\n",
        "    def forward(self, encoded_input_sequence):\n",
        "        '''\n",
        "        Forwrad method process the input from token ids to logits\n",
        "        params:\n",
        "            @inp: input sentence\n",
        "        '''\n",
        "        embedded = self.lookup(encoded_input_sequence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        logits = self.projection(lstm_out)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY90VuhCfcXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(logits):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags(words).\"\"\"\n",
        "    # Create softmax (F.softmax) function\n",
        "    softmax_output = F.softmax(logits, axis=-1)\n",
        "    predictions = torch.multinomial(softmax_output, num_samples=1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEbfMa_ffcXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a model, criterion and optimizer\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGd7jN0LfcXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model parameters -- options\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "options = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrCgjyqfcXc",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLMmIAY2Zwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "  '''\n",
        "  function that computes perplexity\n",
        "  '''\n",
        "  return 2**(loss/np.log(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHG18vvMfcXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "def train_model(model, model_name, hyperparams, loaders, save=True):\n",
        "    '''\n",
        "    function to train neural  LM\n",
        "    params:\n",
        "        @model: LM object\n",
        "        @model_name: str\n",
        "        @hyperparams: dictionary of hyperparameters set for the model\n",
        "        @loaders: DataLoader\n",
        "    '''\n",
        "    print(\"Training {}:\".format(model_name))\n",
        "    \n",
        "    # criterion:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "\n",
        "    PATH = model_name + '.pth'\n",
        "    if os.path.exists(PATH): # load pre-trained\n",
        "      checkpoint = torch.load(PATH)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    else:\n",
        "      # optimizer:\n",
        "      model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "      if hyperparams['optimizer'] == 'SGD':\n",
        "          optimizer = optim.SGD(model_params, lr=hyperparams['lr'], momentum=hyperparams['momentum'])\n",
        "      elif hyperparams['optimizer'] == 'Adam':\n",
        "          optimizer = optim.Adam(model_params, lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state_dict:\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "    # Print optimizer's state_dict\n",
        "    print(\"Optimizer's state_dict:\")\n",
        "    for var_name in optimizer.state_dict():\n",
        "        print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n",
        "    plot_cache = []\n",
        "    num_epochs = hyperparams['num_epochs']\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            # compute loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            # back-propogation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_log_cache.append(loss.item()) # store training loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                avg_train_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                avg_train_perplexity = perplexity(avg_train_loss)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_train_loss, prec=4))\n",
        "                print('Step {} avg train perplexity = {:.{prec}f}'.format(i, avg_train_perplexity, prec=4))\n",
        "                train_log_cache = []\n",
        "\n",
        "        #do validation\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(loaders['valid']):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                device = torch.device(\"cuda\")\n",
        "                logits = model(inp)\n",
        "                # compute loss\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item()) # store validation loss\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            avg_val_perplexity = perplexity(avg_val_loss)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch, avg_val_loss, prec=4))\n",
        "            print('Validation perplexity after {} epoch = {:.{prec}f}'.format(epoch, avg_val_perplexity, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_train_loss, avg_val_loss, avg_train_perplexity, avg_val_perplexity))\n",
        "        # # early stopping\n",
        "        # if len(plot_cache)>1:\n",
        "        #   np.abs((plot_cache[epoch][1] - plot_cache[epoch-1][1])/plot_cache[epoch-1][1]) <= 0.0005\n",
        "        #   print(\"Meets early stopping criteria: Finish training\")\n",
        "        #   return plot_cache\n",
        "    \n",
        "    if save:\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    print('Finished training')\n",
        "    return plot_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bYTI_8vfcXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss(losses):\n",
        "    epochs = np.array(list(range(len(losses))))\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    axes = fig.subplots(nrows=1, ncols=2)\n",
        "    # plot losses\n",
        "    axes[0].plot(epochs, [i[0] for i in losses], label='Train loss')\n",
        "    axes[0].plot(epochs, [i[1] for i in losses], label='Val loss')\n",
        "    axes[0].set_title(\"Training and Validation losses over time\")\n",
        "    axes[0].set_xlabel(\"Steps\")\n",
        "    axes[0].set_ylabel(\"Losses\")\n",
        "    axes[0].legend(loc='best')\n",
        "    # plot training & validation accuracy\n",
        "    axes[1].plot(epochs, [i[2] for i in losses], label='Train Perplexity')\n",
        "    axes[1].plot(epochs, [i[3] for i in losses], label='Val Perplexity')\n",
        "    axes[1].set_title(\"Training and Validation perplexity over time\")\n",
        "    axes[1].set_xlabel(\"Steps\")\n",
        "    axes[1].set_ylabel(\"Perplexity\")\n",
        "    axes[1].legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvEVnWnfcXj",
        "colab_type": "code",
        "outputId": "7c78816e-0ede-4256-e67d-79d591e9dae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# RNN with baseline hyperparameters\n",
        "baseline_hyperparams = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 10,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "model_rnn = RnnLM(options).to(current_device)\n",
        "print(model_rnn)\n",
        "base_rnn_losses = train_model(model_rnn, \"RNN_LM\", baseline_hyperparams, wikitext_loaders)\n",
        "plot_loss(base_rnn_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RnnLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training RNN_LM:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "rnn.weight_ih_l0 \t torch.Size([128, 64])\n",
            "rnn.weight_hh_l0 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l0 \t torch.Size([128])\n",
            "rnn.bias_hh_l0 \t torch.Size([128])\n",
            "rnn.weight_ih_l1 \t torch.Size([128, 128])\n",
            "rnn.weight_hh_l1 \t torch.Size([128, 128])\n",
            "rnn.bias_ih_l1 \t torch.Size([128])\n",
            "rnn.bias_hh_l1 \t torch.Size([128])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140704218383344, 140704218030152, 140704218033320, 140704218032960, 140704218032600, 140704218032888, 140704218030800, 140704218030512, 140704218031088, 140704218030656, 140704218031592]}]\n",
            "Step 0 avg train loss = 10.4280\n",
            "Step 0 avg train perplexity = 33792.2613\n",
            "Step 1000 avg train loss = 6.6343\n",
            "Step 1000 avg train perplexity = 760.7827\n",
            "Step 2000 avg train loss = 6.0561\n",
            "Step 2000 avg train perplexity = 426.7124\n",
            "Validation loss after 0 epoch = 5.7173\n",
            "Validation perplexity after 0 epoch = 304.0769\n",
            "Step 0 avg train loss = 5.7544\n",
            "Step 0 avg train perplexity = 315.5682\n",
            "Step 1000 avg train loss = 5.7078\n",
            "Step 1000 avg train perplexity = 301.1947\n",
            "Step 2000 avg train loss = 5.6420\n",
            "Step 2000 avg train perplexity = 282.0250\n",
            "Validation loss after 1 epoch = 5.5239\n",
            "Validation perplexity after 1 epoch = 250.6163\n",
            "Step 0 avg train loss = 5.3850\n",
            "Step 0 avg train perplexity = 218.1166\n",
            "Step 1000 avg train loss = 5.4272\n",
            "Step 1000 avg train perplexity = 227.5182\n",
            "Step 2000 avg train loss = 5.4071\n",
            "Step 2000 avg train perplexity = 222.9731\n",
            "Validation loss after 2 epoch = 5.4497\n",
            "Validation perplexity after 2 epoch = 232.6817\n",
            "Step 0 avg train loss = 5.2722\n",
            "Step 0 avg train perplexity = 194.8386\n",
            "Step 1000 avg train loss = 5.2463\n",
            "Step 1000 avg train perplexity = 189.8650\n",
            "Step 2000 avg train loss = 5.2526\n",
            "Step 2000 avg train perplexity = 191.0550\n",
            "Validation loss after 3 epoch = 5.3830\n",
            "Validation perplexity after 3 epoch = 217.6670\n",
            "Step 0 avg train loss = 5.0770\n",
            "Step 0 avg train perplexity = 160.2960\n",
            "Step 1000 avg train loss = 5.1142\n",
            "Step 1000 avg train perplexity = 166.3723\n",
            "Step 2000 avg train loss = 5.1364\n",
            "Step 2000 avg train perplexity = 170.1002\n",
            "Validation loss after 4 epoch = 5.3569\n",
            "Validation perplexity after 4 epoch = 212.0733\n",
            "Step 0 avg train loss = 5.0265\n",
            "Step 0 avg train perplexity = 152.3934\n",
            "Step 1000 avg train loss = 5.0086\n",
            "Step 1000 avg train perplexity = 149.6914\n",
            "Step 2000 avg train loss = 5.0457\n",
            "Step 2000 avg train perplexity = 155.3585\n",
            "Validation loss after 5 epoch = 5.3476\n",
            "Validation perplexity after 5 epoch = 210.1077\n",
            "Step 0 avg train loss = 4.7588\n",
            "Step 0 avg train perplexity = 116.6020\n",
            "Step 1000 avg train loss = 4.9215\n",
            "Step 1000 avg train perplexity = 137.2104\n",
            "Step 2000 avg train loss = 4.9666\n",
            "Step 2000 avg train perplexity = 143.5377\n",
            "Validation loss after 6 epoch = 5.3342\n",
            "Validation perplexity after 6 epoch = 207.3157\n",
            "Step 0 avg train loss = 4.8287\n",
            "Step 0 avg train perplexity = 125.0502\n",
            "Step 1000 avg train loss = 4.8506\n",
            "Step 1000 avg train perplexity = 127.8133\n",
            "Step 2000 avg train loss = 4.9033\n",
            "Step 2000 avg train perplexity = 134.7383\n",
            "Validation loss after 7 epoch = 5.3396\n",
            "Validation perplexity after 7 epoch = 208.4335\n",
            "Step 0 avg train loss = 4.6493\n",
            "Step 0 avg train perplexity = 104.5068\n",
            "Step 1000 avg train loss = 4.7969\n",
            "Step 1000 avg train perplexity = 121.1361\n",
            "Step 2000 avg train loss = 4.8431\n",
            "Step 2000 avg train perplexity = 126.8627\n",
            "Validation loss after 8 epoch = 5.3259\n",
            "Validation perplexity after 8 epoch = 205.6034\n",
            "Step 0 avg train loss = 4.6967\n",
            "Step 0 avg train perplexity = 109.5801\n",
            "Step 1000 avg train loss = 4.7424\n",
            "Step 1000 avg train perplexity = 114.7119\n",
            "Step 2000 avg train loss = 4.7929\n",
            "Step 2000 avg train perplexity = 120.6479\n",
            "Validation loss after 9 epoch = 5.3343\n",
            "Validation perplexity after 9 epoch = 207.3237\n",
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAFNCAYAAAC0ZpNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veibMk9afcXp",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4kjNcnr3w1",
        "colab_type": "code",
        "outputId": "3fecf06a-6e5c-4f1e-e735-28c3d54d43eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = [64]\n",
        "hidden_dim = [128, 200]\n",
        "num_layers = [2, 5]\n",
        "dropout = [0.1, 0.3]\n",
        "options = {\n",
        "    'vocab_size': [len(wikitext_dict)],\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': [wikitext_dict.get_id('<pad>')],\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': [True],\n",
        "}\n",
        "\n",
        "regularized_hyperparams = {\n",
        "    'optimizer': ['Adam'],\n",
        "    'lr': [0.001],\n",
        "    'num_epochs': [10],\n",
        "    'weight_decay': [0, 0.05]\n",
        "}\n",
        "\n",
        "finetune_res = {}\n",
        "i=0\n",
        "for option in ParameterGrid(options):\n",
        "    for hyperparam in ParameterGrid(regularized_hyperparams):\n",
        "      # print({**option, **hyperparam})\n",
        "      model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "      print(model_lstm_tuned)\n",
        "      # train\n",
        "      model_name = 'LSTM_LM_Finetuned_' + str(i)\n",
        "      PATH = model_name + '.pth'\n",
        "      finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)\n",
        "      finetune_res[model_name]=({**option, **hyperparam}, finetune_lstm_losses)\n",
        "      i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_0:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139639685281184, 139639685281256, 139639685281328, 139639685281400, 139639685281472, 139639685281544, 139639685281616, 139639685281688, 139639685281760, 139639685281904, 139639685281976]}]\n",
            "Step 0 avg train loss = 10.4023\n",
            "Step 0 avg train perplexity = 32933.8725\n",
            "Step 1000 avg train loss = 6.9048\n",
            "Step 1000 avg train perplexity = 997.0204\n",
            "Step 2000 avg train loss = 6.3108\n",
            "Step 2000 avg train perplexity = 550.5003\n",
            "Validation loss after 0 epoch = 5.8762\n",
            "Validation perplexity after 0 epoch = 356.4365\n",
            "Step 0 avg train loss = 6.2166\n",
            "Step 0 avg train perplexity = 501.0034\n",
            "Step 1000 avg train loss = 5.9080\n",
            "Step 1000 avg train perplexity = 367.9688\n",
            "Step 2000 avg train loss = 5.7882\n",
            "Step 2000 avg train perplexity = 326.4286\n",
            "Validation loss after 1 epoch = 5.5787\n",
            "Validation perplexity after 1 epoch = 264.7265\n",
            "Step 0 avg train loss = 5.6440\n",
            "Step 0 avg train perplexity = 282.5782\n",
            "Step 1000 avg train loss = 5.5765\n",
            "Step 1000 avg train perplexity = 264.1421\n",
            "Step 2000 avg train loss = 5.5203\n",
            "Step 2000 avg train perplexity = 249.7071\n",
            "Validation loss after 2 epoch = 5.4447\n",
            "Validation perplexity after 2 epoch = 231.5314\n",
            "Step 0 avg train loss = 5.2886\n",
            "Step 0 avg train perplexity = 198.0733\n",
            "Step 1000 avg train loss = 5.3599\n",
            "Step 1000 avg train perplexity = 212.6935\n",
            "Step 2000 avg train loss = 5.3315\n",
            "Step 2000 avg train perplexity = 206.7397\n",
            "Validation loss after 3 epoch = 5.3694\n",
            "Validation perplexity after 3 epoch = 214.7356\n",
            "Step 0 avg train loss = 5.1674\n",
            "Step 0 avg train perplexity = 175.4542\n",
            "Step 1000 avg train loss = 5.1934\n",
            "Step 1000 avg train perplexity = 180.0851\n",
            "Step 2000 avg train loss = 5.1806\n",
            "Step 2000 avg train perplexity = 177.7946\n",
            "Validation loss after 4 epoch = 5.3233\n",
            "Validation perplexity after 4 epoch = 205.0556\n",
            "Step 0 avg train loss = 4.9078\n",
            "Step 0 avg train perplexity = 135.3442\n",
            "Step 1000 avg train loss = 5.0562\n",
            "Step 1000 avg train perplexity = 156.9889\n",
            "Step 2000 avg train loss = 5.0559\n",
            "Step 2000 avg train perplexity = 156.9424\n",
            "Validation loss after 5 epoch = 5.3023\n",
            "Validation perplexity after 5 epoch = 200.8071\n",
            "Step 0 avg train loss = 4.9911\n",
            "Step 0 avg train perplexity = 147.0937\n",
            "Step 1000 avg train loss = 4.9475\n",
            "Step 1000 avg train perplexity = 140.8279\n",
            "Step 2000 avg train loss = 4.9562\n",
            "Step 2000 avg train perplexity = 142.0581\n",
            "Validation loss after 6 epoch = 5.3005\n",
            "Validation perplexity after 6 epoch = 200.4274\n",
            "Step 0 avg train loss = 4.8707\n",
            "Step 0 avg train perplexity = 130.4161\n",
            "Step 1000 avg train loss = 4.8488\n",
            "Step 1000 avg train perplexity = 127.5922\n",
            "Step 2000 avg train loss = 4.8687\n",
            "Step 2000 avg train perplexity = 130.1522\n",
            "Validation loss after 7 epoch = 5.2959\n",
            "Validation perplexity after 7 epoch = 199.5081\n",
            "Step 0 avg train loss = 4.7705\n",
            "Step 0 avg train perplexity = 117.9770\n",
            "Step 1000 avg train loss = 4.7695\n",
            "Step 1000 avg train perplexity = 117.8606\n",
            "Step 2000 avg train loss = 4.7944\n",
            "Step 2000 avg train perplexity = 120.8333\n",
            "Validation loss after 8 epoch = 5.3054\n",
            "Validation perplexity after 8 epoch = 201.4140\n",
            "Step 0 avg train loss = 5.0257\n",
            "Step 0 avg train perplexity = 152.2730\n",
            "Step 1000 avg train loss = 4.7007\n",
            "Step 1000 avg train perplexity = 110.0262\n",
            "Step 2000 avg train loss = 4.7225\n",
            "Step 2000 avg train perplexity = 112.4496\n",
            "Validation loss after 9 epoch = 5.3115\n",
            "Validation perplexity after 9 epoch = 202.6503\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_1:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.05, 'amsgrad': False, 'params': [139639685349240, 139639685349168, 139639685349024, 139639685348952, 139639685348808, 139639685348736, 139639685348592, 139639685348520, 139639685348376, 139639685348304, 139639685346432]}]\n",
            "Step 0 avg train loss = 10.4114\n",
            "Step 0 avg train perplexity = 33237.1242\n",
            "Step 1000 avg train loss = 8.1749\n",
            "Step 1000 avg train perplexity = 3550.8006\n",
            "Step 2000 avg train loss = 7.9800\n",
            "Step 2000 avg train perplexity = 2922.0749\n",
            "Validation loss after 0 epoch = 7.9016\n",
            "Validation perplexity after 0 epoch = 2701.5674\n",
            "Step 0 avg train loss = 7.9984\n",
            "Step 0 avg train perplexity = 2976.0567\n",
            "Step 1000 avg train loss = 7.9888\n",
            "Step 1000 avg train perplexity = 2947.8666\n",
            "Step 2000 avg train loss = 7.9890\n",
            "Step 2000 avg train perplexity = 2948.2162\n",
            "Validation loss after 1 epoch = 7.8739\n",
            "Validation perplexity after 1 epoch = 2627.7628\n",
            "Step 0 avg train loss = 8.0224\n",
            "Step 0 avg train perplexity = 3048.3974\n",
            "Step 1000 avg train loss = 7.9876\n",
            "Step 1000 avg train perplexity = 2944.2590\n",
            "Step 2000 avg train loss = 7.9898\n",
            "Step 2000 avg train perplexity = 2950.7229\n",
            "Validation loss after 2 epoch = 7.9244\n",
            "Validation perplexity after 2 epoch = 2763.9813\n",
            "Step 0 avg train loss = 8.0559\n",
            "Step 0 avg train perplexity = 3152.3108\n",
            "Step 1000 avg train loss = 7.9894\n",
            "Step 1000 avg train perplexity = 2949.6389\n",
            "Step 2000 avg train loss = 7.9895\n",
            "Step 2000 avg train perplexity = 2949.7803\n",
            "Validation loss after 3 epoch = 7.8874\n",
            "Validation perplexity after 3 epoch = 2663.4579\n",
            "Step 0 avg train loss = 7.9081\n",
            "Step 0 avg train perplexity = 2719.2103\n",
            "Step 1000 avg train loss = 7.9904\n",
            "Step 1000 avg train perplexity = 2952.5152\n",
            "Step 2000 avg train loss = 7.9892\n",
            "Step 2000 avg train perplexity = 2948.8758\n",
            "Validation loss after 4 epoch = 7.8918\n",
            "Validation perplexity after 4 epoch = 2675.3101\n",
            "Step 0 avg train loss = 8.0211\n",
            "Step 0 avg train perplexity = 3044.3969\n",
            "Step 1000 avg train loss = 7.9909\n",
            "Step 1000 avg train perplexity = 2953.8439\n",
            "Step 2000 avg train loss = 7.9912\n",
            "Step 2000 avg train perplexity = 2954.9490\n",
            "Validation loss after 5 epoch = 7.8613\n",
            "Validation perplexity after 5 epoch = 2594.7802\n",
            "Step 0 avg train loss = 7.9196\n",
            "Step 0 avg train perplexity = 2750.6994\n",
            "Step 1000 avg train loss = 7.9910\n",
            "Step 1000 avg train perplexity = 2954.3432\n",
            "Step 2000 avg train loss = 7.9875\n",
            "Step 2000 avg train perplexity = 2943.9229\n",
            "Validation loss after 6 epoch = 7.8979\n",
            "Validation perplexity after 6 epoch = 2691.7192\n",
            "Step 0 avg train loss = 7.9758\n",
            "Step 0 avg train perplexity = 2909.5627\n",
            "Step 1000 avg train loss = 7.9929\n",
            "Step 1000 avg train perplexity = 2959.9801\n",
            "Step 2000 avg train loss = 7.9858\n",
            "Step 2000 avg train perplexity = 2939.0456\n",
            "Validation loss after 7 epoch = 7.8991\n",
            "Validation perplexity after 7 epoch = 2694.9445\n",
            "Step 0 avg train loss = 8.1746\n",
            "Step 0 avg train perplexity = 3549.5895\n",
            "Step 1000 avg train loss = 7.9920\n",
            "Step 1000 avg train perplexity = 2957.2759\n",
            "Step 2000 avg train loss = 7.9889\n",
            "Step 2000 avg train perplexity = 2948.0946\n",
            "Validation loss after 8 epoch = 7.9177\n",
            "Validation perplexity after 8 epoch = 2745.4542\n",
            "Step 0 avg train loss = 7.8590\n",
            "Step 0 avg train perplexity = 2588.9640\n",
            "Step 1000 avg train loss = 7.9849\n",
            "Step 1000 avg train perplexity = 2936.4033\n",
            "Step 2000 avg train loss = 7.9914\n",
            "Step 2000 avg train perplexity = 2955.5356\n",
            "Validation loss after 9 epoch = 7.9286\n",
            "Validation perplexity after 9 epoch = 2775.4726\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_2:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "lstm.weight_ih_l2 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l2 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l2 \t torch.Size([512])\n",
            "lstm.bias_hh_l2 \t torch.Size([512])\n",
            "lstm.weight_ih_l3 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l3 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l3 \t torch.Size([512])\n",
            "lstm.bias_hh_l3 \t torch.Size([512])\n",
            "lstm.weight_ih_l4 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l4 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l4 \t torch.Size([512])\n",
            "lstm.bias_hh_l4 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139639685281256, 139639685281328, 139639685281400, 139639685281544, 139639685281616, 139639685281472, 139639685281184, 139639685281688, 139639685281760, 139641636344384, 139639685334432, 139639685334576, 139639685333424, 139639685336088, 139639685335944, 139639685335368, 139639685333352, 139639685334864, 139639685334720, 139639685334000, 139639685333784, 139639685335224, 139639685335584]}]\n",
            "Step 0 avg train loss = 10.4024\n",
            "Step 0 avg train perplexity = 32939.7777\n",
            "Step 1000 avg train loss = 7.0809\n",
            "Step 1000 avg train perplexity = 1189.0074\n",
            "Step 2000 avg train loss = 6.9205\n",
            "Step 2000 avg train perplexity = 1012.8632\n",
            "Validation loss after 0 epoch = 6.7192\n",
            "Validation perplexity after 0 epoch = 828.1510\n",
            "Step 0 avg train loss = 6.8378\n",
            "Step 0 avg train perplexity = 932.4596\n",
            "Step 1000 avg train loss = 6.8737\n",
            "Step 1000 avg train perplexity = 966.5287\n",
            "Step 2000 avg train loss = 6.8748\n",
            "Step 2000 avg train perplexity = 967.5595\n",
            "Validation loss after 1 epoch = 6.7072\n",
            "Validation perplexity after 1 epoch = 818.2871\n",
            "Step 0 avg train loss = 6.7796\n",
            "Step 0 avg train perplexity = 879.7043\n",
            "Step 1000 avg train loss = 6.8527\n",
            "Step 1000 avg train perplexity = 946.4183\n",
            "Step 2000 avg train loss = 6.8582\n",
            "Step 2000 avg train perplexity = 951.6877\n",
            "Validation loss after 2 epoch = 6.7111\n",
            "Validation perplexity after 2 epoch = 821.4359\n",
            "Step 0 avg train loss = 6.8423\n",
            "Step 0 avg train perplexity = 936.6208\n",
            "Step 1000 avg train loss = 6.8422\n",
            "Step 1000 avg train perplexity = 936.5297\n",
            "Step 2000 avg train loss = 6.8482\n",
            "Step 2000 avg train perplexity = 942.1472\n",
            "Validation loss after 3 epoch = 6.7088\n",
            "Validation perplexity after 3 epoch = 819.5734\n",
            "Step 0 avg train loss = 6.7083\n",
            "Step 0 avg train perplexity = 819.2012\n",
            "Step 1000 avg train loss = 6.8330\n",
            "Step 1000 avg train perplexity = 927.9838\n",
            "Step 2000 avg train loss = 6.8399\n",
            "Step 2000 avg train perplexity = 934.3753\n",
            "Validation loss after 4 epoch = 6.7130\n",
            "Validation perplexity after 4 epoch = 823.0538\n",
            "Step 0 avg train loss = 6.7773\n",
            "Step 0 avg train perplexity = 877.6630\n",
            "Step 1000 avg train loss = 6.8208\n",
            "Step 1000 avg train perplexity = 916.7109\n",
            "Step 2000 avg train loss = 6.8376\n",
            "Step 2000 avg train perplexity = 932.2550\n",
            "Validation loss after 5 epoch = 6.7153\n",
            "Validation perplexity after 5 epoch = 824.9411\n",
            "Step 0 avg train loss = 6.7310\n",
            "Step 0 avg train perplexity = 837.9604\n",
            "Step 1000 avg train loss = 6.8164\n",
            "Step 1000 avg train perplexity = 912.6742\n",
            "Step 2000 avg train loss = 6.8248\n",
            "Step 2000 avg train perplexity = 920.3639\n",
            "Validation loss after 6 epoch = 6.7197\n",
            "Validation perplexity after 6 epoch = 828.6009\n",
            "Step 0 avg train loss = 6.8930\n",
            "Step 0 avg train perplexity = 985.3663\n",
            "Step 1000 avg train loss = 6.8090\n",
            "Step 1000 avg train perplexity = 905.9863\n",
            "Step 2000 avg train loss = 6.5363\n",
            "Step 2000 avg train perplexity = 689.6968\n",
            "Validation loss after 7 epoch = 6.1593\n",
            "Validation perplexity after 7 epoch = 473.1173\n",
            "Step 0 avg train loss = 6.1326\n",
            "Step 0 avg train perplexity = 460.6517\n",
            "Step 1000 avg train loss = 6.1556\n",
            "Step 1000 avg train perplexity = 471.3344\n",
            "Step 2000 avg train loss = 6.0601\n",
            "Step 2000 avg train perplexity = 428.4263\n",
            "Validation loss after 8 epoch = 5.8755\n",
            "Validation perplexity after 8 epoch = 356.2049\n",
            "Step 0 avg train loss = 5.6866\n",
            "Step 0 avg train perplexity = 294.8794\n",
            "Step 1000 avg train loss = 5.8486\n",
            "Step 1000 avg train perplexity = 346.7494\n",
            "Step 2000 avg train loss = 5.7910\n",
            "Step 2000 avg train perplexity = 327.3368\n",
            "Validation loss after 9 epoch = 5.7186\n",
            "Validation perplexity after 9 epoch = 304.4795\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_3:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "lstm.weight_ih_l2 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l2 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l2 \t torch.Size([512])\n",
            "lstm.bias_hh_l2 \t torch.Size([512])\n",
            "lstm.weight_ih_l3 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l3 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l3 \t torch.Size([512])\n",
            "lstm.bias_hh_l3 \t torch.Size([512])\n",
            "lstm.weight_ih_l4 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l4 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l4 \t torch.Size([512])\n",
            "lstm.bias_hh_l4 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.05, 'amsgrad': False, 'params': [139639685281904, 139639685348664, 139639685348088, 139639685348592, 139639685348736, 139639685349240, 139639685348304, 139639685348520, 139639685346072, 139639685348808, 139639685347152, 139639685348376, 139639685349096, 139639685348880, 139639685346216, 139639685347512, 139639685346504, 139639685345856, 139639685349168, 139639685346000, 139639685345352, 139639685347296, 139639685348952]}]\n",
            "Step 0 avg train loss = 10.4198\n",
            "Step 0 avg train perplexity = 33517.0752\n",
            "Step 1000 avg train loss = 8.2474\n",
            "Step 1000 avg train perplexity = 3817.5797\n",
            "Step 2000 avg train loss = 7.9862\n",
            "Step 2000 avg train perplexity = 2940.0479\n",
            "Validation loss after 0 epoch = 7.8962\n",
            "Validation perplexity after 0 epoch = 2687.1562\n",
            "Step 0 avg train loss = 8.1875\n",
            "Step 0 avg train perplexity = 3595.8349\n",
            "Step 1000 avg train loss = 7.9885\n",
            "Step 1000 avg train perplexity = 2946.9603\n",
            "Step 2000 avg train loss = 7.9888\n",
            "Step 2000 avg train perplexity = 2947.6582\n",
            "Validation loss after 1 epoch = 7.9377\n",
            "Validation perplexity after 1 epoch = 2800.9211\n",
            "Step 0 avg train loss = 8.0675\n",
            "Step 0 avg train perplexity = 3188.9856\n",
            "Step 1000 avg train loss = 7.9900\n",
            "Step 1000 avg train perplexity = 2951.2432\n",
            "Step 2000 avg train loss = 7.9898\n",
            "Step 2000 avg train perplexity = 2950.6238\n",
            "Validation loss after 2 epoch = 7.9097\n",
            "Validation perplexity after 2 epoch = 2723.5212\n",
            "Step 0 avg train loss = 7.9555\n",
            "Step 0 avg train perplexity = 2851.1623\n",
            "Step 1000 avg train loss = 7.9874\n",
            "Step 1000 avg train perplexity = 2943.5633\n",
            "Step 2000 avg train loss = 7.9892\n",
            "Step 2000 avg train perplexity = 2949.0599\n",
            "Validation loss after 3 epoch = 7.9385\n",
            "Validation perplexity after 3 epoch = 2803.2179\n",
            "Step 0 avg train loss = 7.9309\n",
            "Step 0 avg train perplexity = 2781.8236\n",
            "Step 1000 avg train loss = 7.9881\n",
            "Step 1000 avg train perplexity = 2945.6536\n",
            "Step 2000 avg train loss = 7.9916\n",
            "Step 2000 avg train perplexity = 2956.0573\n",
            "Validation loss after 4 epoch = 7.9104\n",
            "Validation perplexity after 4 epoch = 2725.3938\n",
            "Step 0 avg train loss = 8.0028\n",
            "Step 0 avg train perplexity = 2989.4617\n",
            "Step 1000 avg train loss = 7.9913\n",
            "Step 1000 avg train perplexity = 2955.1862\n",
            "Step 2000 avg train loss = 7.9881\n",
            "Step 2000 avg train perplexity = 2945.8134\n",
            "Validation loss after 5 epoch = 7.8877\n",
            "Validation perplexity after 5 epoch = 2664.3078\n",
            "Step 0 avg train loss = 8.0140\n",
            "Step 0 avg train perplexity = 3022.9501\n",
            "Step 1000 avg train loss = 7.9893\n",
            "Step 1000 avg train perplexity = 2949.1488\n",
            "Step 2000 avg train loss = 7.9892\n",
            "Step 2000 avg train perplexity = 2948.8340\n",
            "Validation loss after 6 epoch = 7.8787\n",
            "Validation perplexity after 6 epoch = 2640.3480\n",
            "Step 0 avg train loss = 8.0208\n",
            "Step 0 avg train perplexity = 3043.6856\n",
            "Step 1000 avg train loss = 7.9889\n",
            "Step 1000 avg train perplexity = 2948.0818\n",
            "Step 2000 avg train loss = 7.9887\n",
            "Step 2000 avg train perplexity = 2947.5704\n",
            "Validation loss after 7 epoch = 7.9006\n",
            "Validation perplexity after 7 epoch = 2699.0259\n",
            "Step 0 avg train loss = 8.0060\n",
            "Step 0 avg train perplexity = 2998.9791\n",
            "Step 1000 avg train loss = 7.9915\n",
            "Step 1000 avg train perplexity = 2955.8393\n",
            "Step 2000 avg train loss = 7.9885\n",
            "Step 2000 avg train perplexity = 2946.9104\n",
            "Validation loss after 8 epoch = 7.8888\n",
            "Validation perplexity after 8 epoch = 2667.1728\n",
            "Step 0 avg train loss = 8.0392\n",
            "Step 0 avg train perplexity = 3100.1523\n",
            "Step 1000 avg train loss = 7.9917\n",
            "Step 1000 avg train perplexity = 2956.4239\n",
            "Step 2000 avg train loss = 7.9851\n",
            "Step 2000 avg train perplexity = 2936.8922\n",
            "Validation loss after 9 epoch = 7.8858\n",
            "Validation perplexity after 9 epoch = 2659.1959\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_4:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([800, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l0 \t torch.Size([800])\n",
            "lstm.bias_hh_l0 \t torch.Size([800])\n",
            "lstm.weight_ih_l1 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l1 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l1 \t torch.Size([800])\n",
            "lstm.bias_hh_l1 \t torch.Size([800])\n",
            "projection.weight \t torch.Size([33181, 200])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139639685334576, 139639685333928, 139639685333424, 139639685335944, 139639685334864, 139639685334000, 139639685335368, 139639685333352, 139639685334432, 139639685334720, 139639685333784]}]\n",
            "Step 0 avg train loss = 10.4132\n",
            "Step 0 avg train perplexity = 33295.0541\n",
            "Step 1000 avg train loss = 6.8615\n",
            "Step 1000 avg train perplexity = 954.8118\n",
            "Step 2000 avg train loss = 6.2213\n",
            "Step 2000 avg train perplexity = 503.3814\n",
            "Validation loss after 0 epoch = 5.7888\n",
            "Validation perplexity after 0 epoch = 326.6065\n",
            "Step 0 avg train loss = 5.8550\n",
            "Step 0 avg train perplexity = 348.9919\n",
            "Step 1000 avg train loss = 5.7943\n",
            "Step 1000 avg train perplexity = 328.4134\n",
            "Step 2000 avg train loss = 5.6729\n",
            "Step 2000 avg train perplexity = 290.8643\n",
            "Validation loss after 1 epoch = 5.4929\n",
            "Validation perplexity after 1 epoch = 242.9718\n",
            "Step 0 avg train loss = 5.4831\n",
            "Step 0 avg train perplexity = 240.5883\n",
            "Step 1000 avg train loss = 5.4237\n",
            "Step 1000 avg train perplexity = 226.7256\n",
            "Step 2000 avg train loss = 5.3633\n",
            "Step 2000 avg train perplexity = 213.4197\n",
            "Validation loss after 2 epoch = 5.3631\n",
            "Validation perplexity after 2 epoch = 213.3793\n",
            "Step 0 avg train loss = 5.2740\n",
            "Step 0 avg train perplexity = 195.1858\n",
            "Step 1000 avg train loss = 5.1633\n",
            "Step 1000 avg train perplexity = 174.7429\n",
            "Step 2000 avg train loss = 5.1392\n",
            "Step 2000 avg train perplexity = 170.5768\n",
            "Validation loss after 3 epoch = 5.2891\n",
            "Validation perplexity after 3 epoch = 198.1653\n",
            "Step 0 avg train loss = 5.1780\n",
            "Step 0 avg train perplexity = 177.3286\n",
            "Step 1000 avg train loss = 4.9603\n",
            "Step 1000 avg train perplexity = 142.6399\n",
            "Step 2000 avg train loss = 4.9636\n",
            "Step 2000 avg train perplexity = 143.1075\n",
            "Validation loss after 4 epoch = 5.2702\n",
            "Validation perplexity after 4 epoch = 194.4529\n",
            "Step 0 avg train loss = 4.8655\n",
            "Step 0 avg train perplexity = 129.7349\n",
            "Step 1000 avg train loss = 4.8063\n",
            "Step 1000 avg train perplexity = 122.2774\n",
            "Step 2000 avg train loss = 4.8159\n",
            "Step 2000 avg train perplexity = 123.4540\n",
            "Validation loss after 5 epoch = 5.2789\n",
            "Validation perplexity after 5 epoch = 196.1498\n",
            "Step 0 avg train loss = 4.9155\n",
            "Step 0 avg train perplexity = 136.3876\n",
            "Step 1000 avg train loss = 4.6773\n",
            "Step 1000 avg train perplexity = 107.4752\n",
            "Step 2000 avg train loss = 4.6970\n",
            "Step 2000 avg train perplexity = 109.6219\n",
            "Validation loss after 6 epoch = 5.2891\n",
            "Validation perplexity after 6 epoch = 198.1592\n",
            "Step 0 avg train loss = 4.5340\n",
            "Step 0 avg train perplexity = 93.1273\n",
            "Step 1000 avg train loss = 4.5607\n",
            "Step 1000 avg train perplexity = 95.6515\n",
            "Step 2000 avg train loss = 4.5997\n",
            "Step 2000 avg train perplexity = 99.4580\n",
            "Validation loss after 7 epoch = 5.3107\n",
            "Validation perplexity after 7 epoch = 202.4849\n",
            "Step 0 avg train loss = 4.3602\n",
            "Step 0 avg train perplexity = 78.2763\n",
            "Step 1000 avg train loss = 4.4718\n",
            "Step 1000 avg train perplexity = 87.5159\n",
            "Step 2000 avg train loss = 4.5111\n",
            "Step 2000 avg train perplexity = 91.0211\n",
            "Validation loss after 8 epoch = 5.3497\n",
            "Validation perplexity after 8 epoch = 210.5441\n",
            "Step 0 avg train loss = 4.3137\n",
            "Step 0 avg train perplexity = 74.7175\n",
            "Step 1000 avg train loss = 4.3896\n",
            "Step 1000 avg train perplexity = 80.6048\n",
            "Step 2000 avg train loss = 4.4376\n",
            "Step 2000 avg train perplexity = 84.5713\n",
            "Validation loss after 9 epoch = 5.3775\n",
            "Validation perplexity after 9 epoch = 216.4836\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_5:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([800, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l0 \t torch.Size([800])\n",
            "lstm.bias_hh_l0 \t torch.Size([800])\n",
            "lstm.weight_ih_l1 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l1 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l1 \t torch.Size([800])\n",
            "lstm.bias_hh_l1 \t torch.Size([800])\n",
            "projection.weight \t torch.Size([33181, 200])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.05, 'amsgrad': False, 'params': [139639685335224, 139639685333280, 139639685333496, 139639673913136, 139639673912848, 139639673912632, 139639673910616, 139639673911480, 139639673909320, 139639673912704, 139639673911624]}]\n",
            "Step 0 avg train loss = 10.3987\n",
            "Step 0 avg train perplexity = 32816.4899\n",
            "Step 1000 avg train loss = 8.0299\n",
            "Step 1000 avg train perplexity = 3071.2981\n",
            "Step 2000 avg train loss = 7.8850\n",
            "Step 2000 avg train perplexity = 2657.1842\n",
            "Validation loss after 0 epoch = 7.9053\n",
            "Validation perplexity after 0 epoch = 2711.6122\n",
            "Step 0 avg train loss = 7.9164\n",
            "Step 0 avg train perplexity = 2741.8118\n",
            "Step 1000 avg train loss = 7.9381\n",
            "Step 1000 avg train perplexity = 2802.1262\n",
            "Step 2000 avg train loss = 7.9366\n",
            "Step 2000 avg train perplexity = 2797.9643\n",
            "Validation loss after 1 epoch = 7.8115\n",
            "Validation perplexity after 1 epoch = 2468.7473\n",
            "Step 0 avg train loss = 7.9424\n",
            "Step 0 avg train perplexity = 2814.0592\n",
            "Step 1000 avg train loss = 7.9398\n",
            "Step 1000 avg train perplexity = 2806.9377\n",
            "Step 2000 avg train loss = 7.9379\n",
            "Step 2000 avg train perplexity = 2801.4596\n",
            "Validation loss after 2 epoch = 7.7969\n",
            "Validation perplexity after 2 epoch = 2432.9880\n",
            "Step 0 avg train loss = 7.6697\n",
            "Step 0 avg train perplexity = 2142.4522\n",
            "Step 1000 avg train loss = 7.9430\n",
            "Step 1000 avg train perplexity = 2815.8634\n",
            "Step 2000 avg train loss = 7.9382\n",
            "Step 2000 avg train perplexity = 2802.2631\n",
            "Validation loss after 3 epoch = 7.8604\n",
            "Validation perplexity after 3 epoch = 2592.5575\n",
            "Step 0 avg train loss = 7.9276\n",
            "Step 0 avg train perplexity = 2772.8723\n",
            "Step 1000 avg train loss = 7.9371\n",
            "Step 1000 avg train perplexity = 2799.1114\n",
            "Step 2000 avg train loss = 7.9386\n",
            "Step 2000 avg train perplexity = 2803.4484\n",
            "Validation loss after 4 epoch = 7.8399\n",
            "Validation perplexity after 4 epoch = 2540.0473\n",
            "Step 0 avg train loss = 7.9016\n",
            "Step 0 avg train perplexity = 2701.7325\n",
            "Step 1000 avg train loss = 7.9416\n",
            "Step 1000 avg train perplexity = 2811.9433\n",
            "Step 2000 avg train loss = 7.9387\n",
            "Step 2000 avg train perplexity = 2803.6468\n",
            "Validation loss after 5 epoch = 7.8467\n",
            "Validation perplexity after 5 epoch = 2557.2784\n",
            "Step 0 avg train loss = 7.7883\n",
            "Step 0 avg train perplexity = 2412.3181\n",
            "Step 1000 avg train loss = 7.9405\n",
            "Step 1000 avg train perplexity = 2808.6269\n",
            "Step 2000 avg train loss = 7.9380\n",
            "Step 2000 avg train perplexity = 2801.8715\n",
            "Validation loss after 6 epoch = 7.8522\n",
            "Validation perplexity after 6 epoch = 2571.2672\n",
            "Step 0 avg train loss = 7.9828\n",
            "Step 0 avg train perplexity = 2930.0303\n",
            "Step 1000 avg train loss = 7.9416\n",
            "Step 1000 avg train perplexity = 2811.7767\n",
            "Step 2000 avg train loss = 7.9378\n",
            "Step 2000 avg train perplexity = 2801.0537\n",
            "Validation loss after 7 epoch = 7.8444\n",
            "Validation perplexity after 7 epoch = 2551.3384\n",
            "Step 0 avg train loss = 8.0540\n",
            "Step 0 avg train perplexity = 3146.5080\n",
            "Step 1000 avg train loss = 7.9414\n",
            "Step 1000 avg train perplexity = 2811.3071\n",
            "Step 2000 avg train loss = 7.9365\n",
            "Step 2000 avg train perplexity = 2797.5740\n",
            "Validation loss after 8 epoch = 7.8161\n",
            "Validation perplexity after 8 epoch = 2480.1864\n",
            "Step 0 avg train loss = 7.9672\n",
            "Step 0 avg train perplexity = 2884.8630\n",
            "Step 1000 avg train loss = 7.9414\n",
            "Step 1000 avg train perplexity = 2811.3113\n",
            "Step 2000 avg train loss = 7.9378\n",
            "Step 2000 avg train perplexity = 2801.1647\n",
            "Validation loss after 9 epoch = 7.8523\n",
            "Validation perplexity after 9 epoch = 2571.5956\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_6:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([800, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l0 \t torch.Size([800])\n",
            "lstm.bias_hh_l0 \t torch.Size([800])\n",
            "lstm.weight_ih_l1 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l1 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l1 \t torch.Size([800])\n",
            "lstm.bias_hh_l1 \t torch.Size([800])\n",
            "lstm.weight_ih_l2 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l2 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l2 \t torch.Size([800])\n",
            "lstm.bias_hh_l2 \t torch.Size([800])\n",
            "lstm.weight_ih_l3 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l3 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l3 \t torch.Size([800])\n",
            "lstm.bias_hh_l3 \t torch.Size([800])\n",
            "lstm.weight_ih_l4 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l4 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l4 \t torch.Size([800])\n",
            "lstm.bias_hh_l4 \t torch.Size([800])\n",
            "projection.weight \t torch.Size([33181, 200])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139639685333928, 139639685333352, 139639685334864, 139639685334432, 139639685335512, 139639685334072, 139639685334576, 139639685334000, 139639685333856, 139639685333424, 139639685334648, 139639685335944, 139639685334504, 139639673911048, 139639673910832, 139639673913280, 139639673910760, 139639673911552, 139639673911912, 139639673909824, 139639673911984, 139639673912056, 139639673912920]}]\n",
            "Step 0 avg train loss = 10.4100\n",
            "Step 0 avg train perplexity = 33189.0740\n",
            "Step 1000 avg train loss = 7.0741\n",
            "Step 1000 avg train perplexity = 1181.0281\n",
            "Step 2000 avg train loss = 6.9141\n",
            "Step 2000 avg train perplexity = 1006.3384\n",
            "Validation loss after 0 epoch = 6.7193\n",
            "Validation perplexity after 0 epoch = 828.1996\n",
            "Step 0 avg train loss = 6.8431\n",
            "Step 0 avg train perplexity = 937.4268\n",
            "Step 1000 avg train loss = 6.8713\n",
            "Step 1000 avg train perplexity = 964.1896\n",
            "Step 2000 avg train loss = 6.8682\n",
            "Step 2000 avg train perplexity = 961.1825\n",
            "Validation loss after 1 epoch = 6.7093\n",
            "Validation perplexity after 1 epoch = 819.9986\n",
            "Step 0 avg train loss = 6.7390\n",
            "Step 0 avg train perplexity = 844.7445\n",
            "Step 1000 avg train loss = 6.8480\n",
            "Step 1000 avg train perplexity = 942.0264\n",
            "Step 2000 avg train loss = 6.8581\n",
            "Step 2000 avg train perplexity = 951.5990\n",
            "Validation loss after 2 epoch = 6.7060\n",
            "Validation perplexity after 2 epoch = 817.3031\n",
            "Step 0 avg train loss = 6.9343\n",
            "Step 0 avg train perplexity = 1026.8590\n",
            "Step 1000 avg train loss = 6.8401\n",
            "Step 1000 avg train perplexity = 934.5932\n",
            "Step 2000 avg train loss = 6.8482\n",
            "Step 2000 avg train perplexity = 942.1688\n",
            "Validation loss after 3 epoch = 6.7096\n",
            "Validation perplexity after 3 epoch = 820.2785\n",
            "Step 0 avg train loss = 6.7111\n",
            "Step 0 avg train perplexity = 821.4821\n",
            "Step 1000 avg train loss = 6.8281\n",
            "Step 1000 avg train perplexity = 923.4723\n",
            "Step 2000 avg train loss = 6.8363\n",
            "Step 2000 avg train perplexity = 931.0076\n",
            "Validation loss after 4 epoch = 6.7128\n",
            "Validation perplexity after 4 epoch = 822.8960\n",
            "Step 0 avg train loss = 6.8371\n",
            "Step 0 avg train perplexity = 931.7844\n",
            "Step 1000 avg train loss = 6.8165\n",
            "Step 1000 avg train perplexity = 912.7422\n",
            "Step 2000 avg train loss = 6.8329\n",
            "Step 2000 avg train perplexity = 927.8883\n",
            "Validation loss after 5 epoch = 6.7178\n",
            "Validation perplexity after 5 epoch = 826.9823\n",
            "Step 0 avg train loss = 6.7663\n",
            "Step 0 avg train perplexity = 868.0743\n",
            "Step 1000 avg train loss = 6.8102\n",
            "Step 1000 avg train perplexity = 907.0879\n",
            "Step 2000 avg train loss = 6.8248\n",
            "Step 2000 avg train perplexity = 920.4274\n",
            "Validation loss after 6 epoch = 6.7189\n",
            "Validation perplexity after 6 epoch = 827.9362\n",
            "Step 0 avg train loss = 6.7938\n",
            "Step 0 avg train perplexity = 892.2914\n",
            "Step 1000 avg train loss = 6.8006\n",
            "Step 1000 avg train perplexity = 898.3848\n",
            "Step 2000 avg train loss = 6.8194\n",
            "Step 2000 avg train perplexity = 915.4717\n",
            "Validation loss after 7 epoch = 6.7331\n",
            "Validation perplexity after 7 epoch = 839.7224\n",
            "Step 0 avg train loss = 6.8854\n",
            "Step 0 avg train perplexity = 977.8814\n",
            "Step 1000 avg train loss = 6.7985\n",
            "Step 1000 avg train perplexity = 896.5359\n",
            "Step 2000 avg train loss = 6.8045\n",
            "Step 2000 avg train perplexity = 901.9321\n",
            "Validation loss after 8 epoch = 6.3478\n",
            "Validation perplexity after 8 epoch = 571.2517\n",
            "Step 0 avg train loss = 6.5288\n",
            "Step 0 avg train perplexity = 684.5772\n",
            "Step 1000 avg train loss = 6.2595\n",
            "Step 1000 avg train perplexity = 522.9605\n",
            "Step 2000 avg train loss = 6.1067\n",
            "Step 2000 avg train perplexity = 448.8437\n",
            "Validation loss after 9 epoch = 5.9187\n",
            "Validation perplexity after 9 epoch = 371.9278\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_7:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([800, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l0 \t torch.Size([800])\n",
            "lstm.bias_hh_l0 \t torch.Size([800])\n",
            "lstm.weight_ih_l1 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l1 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l1 \t torch.Size([800])\n",
            "lstm.bias_hh_l1 \t torch.Size([800])\n",
            "lstm.weight_ih_l2 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l2 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l2 \t torch.Size([800])\n",
            "lstm.bias_hh_l2 \t torch.Size([800])\n",
            "lstm.weight_ih_l3 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l3 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l3 \t torch.Size([800])\n",
            "lstm.bias_hh_l3 \t torch.Size([800])\n",
            "lstm.weight_ih_l4 \t torch.Size([800, 200])\n",
            "lstm.weight_hh_l4 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l4 \t torch.Size([800])\n",
            "lstm.bias_hh_l4 \t torch.Size([800])\n",
            "projection.weight \t torch.Size([33181, 200])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.05, 'amsgrad': False, 'params': [139639685334720, 139639673911120, 139639673913064, 139639673909464, 139639673909536, 139639673909752, 139639673911408, 139639673911624, 139639673911480, 139639673912128, 139639673909320, 139639673912704, 139639673911192, 139639673912632, 139639673913136, 139639673912416, 139639673912344, 139639673910328, 139639673911336, 139639673909392, 139639673911696, 139639673912200, 139639673910112]}]\n",
            "Step 0 avg train loss = 10.4120\n",
            "Step 0 avg train perplexity = 33255.6406\n",
            "Step 1000 avg train loss = 8.1460\n",
            "Step 1000 avg train perplexity = 3449.5494\n",
            "Step 2000 avg train loss = 7.9396\n",
            "Step 2000 avg train perplexity = 2806.1662\n",
            "Validation loss after 0 epoch = 7.8459\n",
            "Validation perplexity after 0 epoch = 2555.1659\n",
            "Step 0 avg train loss = 8.0055\n",
            "Step 0 avg train perplexity = 2997.4322\n",
            "Step 1000 avg train loss = 7.9425\n",
            "Step 1000 avg train perplexity = 2814.3939\n",
            "Step 2000 avg train loss = 7.9374\n",
            "Step 2000 avg train perplexity = 2800.0058\n",
            "Validation loss after 1 epoch = 7.8291\n",
            "Validation perplexity after 1 epoch = 2512.7337\n",
            "Step 0 avg train loss = 7.8548\n",
            "Step 0 avg train perplexity = 2578.1575\n",
            "Step 1000 avg train loss = 7.9386\n",
            "Step 1000 avg train perplexity = 2803.4341\n",
            "Step 2000 avg train loss = 7.9409\n",
            "Step 2000 avg train perplexity = 2809.8837\n",
            "Validation loss after 2 epoch = 7.8621\n",
            "Validation perplexity after 2 epoch = 2596.8799\n",
            "Step 0 avg train loss = 7.8645\n",
            "Step 0 avg train perplexity = 2603.3253\n",
            "Step 1000 avg train loss = 7.9401\n",
            "Step 1000 avg train perplexity = 2807.5955\n",
            "Step 2000 avg train loss = 7.9425\n",
            "Step 2000 avg train perplexity = 2814.3848\n",
            "Validation loss after 3 epoch = 7.8728\n",
            "Validation perplexity after 3 epoch = 2624.9876\n",
            "Step 0 avg train loss = 7.8965\n",
            "Step 0 avg train perplexity = 2687.9330\n",
            "Step 1000 avg train loss = 7.9393\n",
            "Step 1000 avg train perplexity = 2805.3942\n",
            "Step 2000 avg train loss = 7.9387\n",
            "Step 2000 avg train perplexity = 2803.6016\n",
            "Validation loss after 4 epoch = 7.8645\n",
            "Validation perplexity after 4 epoch = 2603.3343\n",
            "Step 0 avg train loss = 7.9559\n",
            "Step 0 avg train perplexity = 2852.4609\n",
            "Step 1000 avg train loss = 7.9403\n",
            "Step 1000 avg train perplexity = 2808.2080\n",
            "Step 2000 avg train loss = 7.9377\n",
            "Step 2000 avg train perplexity = 2800.9465\n",
            "Validation loss after 5 epoch = 7.8586\n",
            "Validation perplexity after 5 epoch = 2587.8154\n",
            "Step 0 avg train loss = 7.8354\n",
            "Step 0 avg train perplexity = 2528.5084\n",
            "Step 1000 avg train loss = 7.9402\n",
            "Step 1000 avg train perplexity = 2808.0603\n",
            "Step 2000 avg train loss = 7.9410\n",
            "Step 2000 avg train perplexity = 2810.3007\n",
            "Validation loss after 6 epoch = 7.8192\n",
            "Validation perplexity after 6 epoch = 2488.0257\n",
            "Step 0 avg train loss = 7.9190\n",
            "Step 0 avg train perplexity = 2748.9359\n",
            "Step 1000 avg train loss = 7.9350\n",
            "Step 1000 avg train perplexity = 2793.4606\n",
            "Step 2000 avg train loss = 7.9452\n",
            "Step 2000 avg train perplexity = 2821.8790\n",
            "Validation loss after 7 epoch = 7.8285\n",
            "Validation perplexity after 7 epoch = 2511.1976\n",
            "Step 0 avg train loss = 7.8133\n",
            "Step 0 avg train perplexity = 2473.3747\n",
            "Step 1000 avg train loss = 7.9383\n",
            "Step 1000 avg train perplexity = 2802.5846\n",
            "Step 2000 avg train loss = 7.9397\n",
            "Step 2000 avg train perplexity = 2806.4916\n",
            "Validation loss after 8 epoch = 7.8722\n",
            "Validation perplexity after 8 epoch = 2623.2110\n",
            "Step 0 avg train loss = 7.9938\n",
            "Step 0 avg train perplexity = 2962.5577\n",
            "Step 1000 avg train loss = 7.9353\n",
            "Step 1000 avg train perplexity = 2794.1167\n",
            "Step 2000 avg train loss = 7.9412\n",
            "Step 2000 avg train perplexity = 2810.6139\n",
            "Validation loss after 9 epoch = 7.8176\n",
            "Validation perplexity after 9 epoch = 2483.8234\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_8:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139639685333352, 139639685334648, 139639685334000, 139639685333856, 139639685334576, 139639685334072, 139639685334432, 139639685333928, 139639685335368, 139639685333424, 139639685333280]}]\n",
            "Step 0 avg train loss = 10.4216\n",
            "Step 0 avg train perplexity = 33578.2788\n",
            "Step 1000 avg train loss = 6.8888\n",
            "Step 1000 avg train perplexity = 981.2017\n",
            "Step 2000 avg train loss = 6.3042\n",
            "Step 2000 avg train perplexity = 546.8769\n",
            "Validation loss after 0 epoch = 5.8785\n",
            "Validation perplexity after 0 epoch = 357.2734\n",
            "Step 0 avg train loss = 6.2727\n",
            "Step 0 avg train perplexity = 529.9076\n",
            "Step 1000 avg train loss = 5.9199\n",
            "Step 1000 avg train perplexity = 372.3882\n",
            "Step 2000 avg train loss = 5.7963\n",
            "Step 2000 avg train perplexity = 329.0879\n",
            "Validation loss after 1 epoch = 5.5841\n",
            "Validation perplexity after 1 epoch = 266.1672\n",
            "Step 0 avg train loss = 5.5216\n",
            "Step 0 avg train perplexity = 250.0465\n",
            "Step 1000 avg train loss = 5.5919\n",
            "Step 1000 avg train perplexity = 268.2543\n",
            "Step 2000 avg train loss = 5.5509\n",
            "Step 2000 avg train perplexity = 257.4721\n",
            "Validation loss after 2 epoch = 5.4549\n",
            "Validation perplexity after 2 epoch = 233.9046\n",
            "Step 0 avg train loss = 5.4436\n",
            "Step 0 avg train perplexity = 231.2629\n",
            "Step 1000 avg train loss = 5.3925\n",
            "Step 1000 avg train perplexity = 219.7493\n",
            "Step 2000 avg train loss = 5.3696\n",
            "Step 2000 avg train perplexity = 214.7844\n",
            "Validation loss after 3 epoch = 5.3808\n",
            "Validation perplexity after 3 epoch = 217.1955\n",
            "Step 0 avg train loss = 5.4737\n",
            "Step 0 avg train perplexity = 238.3451\n",
            "Step 1000 avg train loss = 5.2372\n",
            "Step 1000 avg train perplexity = 188.1403\n",
            "Step 2000 avg train loss = 5.2364\n",
            "Step 2000 avg train perplexity = 188.0005\n",
            "Validation loss after 4 epoch = 5.3376\n",
            "Validation perplexity after 4 epoch = 208.0048\n",
            "Step 0 avg train loss = 5.0852\n",
            "Step 0 avg train perplexity = 161.6102\n",
            "Step 1000 avg train loss = 5.1210\n",
            "Step 1000 avg train perplexity = 167.5107\n",
            "Step 2000 avg train loss = 5.1253\n",
            "Step 2000 avg train perplexity = 168.2272\n",
            "Validation loss after 5 epoch = 5.3169\n",
            "Validation perplexity after 5 epoch = 203.7612\n",
            "Step 0 avg train loss = 4.9380\n",
            "Step 0 avg train perplexity = 139.4939\n",
            "Step 1000 avg train loss = 5.0222\n",
            "Step 1000 avg train perplexity = 151.7463\n",
            "Step 2000 avg train loss = 5.0349\n",
            "Step 2000 avg train perplexity = 153.6867\n",
            "Validation loss after 6 epoch = 5.3127\n",
            "Validation perplexity after 6 epoch = 202.8976\n",
            "Step 0 avg train loss = 4.9742\n",
            "Step 0 avg train perplexity = 144.6281\n",
            "Step 1000 avg train loss = 4.9384\n",
            "Step 1000 avg train perplexity = 139.5522\n",
            "Step 2000 avg train loss = 4.9571\n",
            "Step 2000 avg train perplexity = 142.1846\n",
            "Validation loss after 7 epoch = 5.3070\n",
            "Validation perplexity after 7 epoch = 201.7413\n",
            "Step 0 avg train loss = 4.7590\n",
            "Step 0 avg train perplexity = 116.6244\n",
            "Step 1000 avg train loss = 4.8651\n",
            "Step 1000 avg train perplexity = 129.6904\n",
            "Step 2000 avg train loss = 4.8934\n",
            "Step 2000 avg train perplexity = 133.4053\n",
            "Validation loss after 8 epoch = 5.3067\n",
            "Validation perplexity after 8 epoch = 201.6896\n",
            "Step 0 avg train loss = 4.8335\n",
            "Step 0 avg train perplexity = 125.6555\n",
            "Step 1000 avg train loss = 4.8068\n",
            "Step 1000 avg train perplexity = 122.3403\n",
            "Step 2000 avg train loss = 4.8405\n",
            "Step 2000 avg train perplexity = 126.5367\n",
            "Validation loss after 9 epoch = 5.3158\n",
            "Validation perplexity after 9 epoch = 203.5224\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM_LM_Finetuned_9:\n",
            "Model's state_dict:\n",
            "lookup.weight \t torch.Size([33181, 64])\n",
            "lstm.weight_ih_l0 \t torch.Size([512, 64])\n",
            "lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l0 \t torch.Size([512])\n",
            "lstm.bias_hh_l0 \t torch.Size([512])\n",
            "lstm.weight_ih_l1 \t torch.Size([512, 128])\n",
            "lstm.weight_hh_l1 \t torch.Size([512, 128])\n",
            "lstm.bias_ih_l1 \t torch.Size([512])\n",
            "lstm.bias_hh_l1 \t torch.Size([512])\n",
            "projection.weight \t torch.Size([33181, 128])\n",
            "projection.bias \t torch.Size([33181])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.05, 'amsgrad': False, 'params': [139639685281544, 139639685281400, 139639685281472, 139639685281256, 139639685281760, 139639685281184, 139639685282552, 139639685281904, 139639685281328, 139639685364760, 139639685365696]}]\n",
            "Step 0 avg train loss = 10.4228\n",
            "Step 0 avg train perplexity = 33616.5678\n",
            "Step 1000 avg train loss = 8.1809\n",
            "Step 1000 avg train perplexity = 3571.9444\n",
            "Step 2000 avg train loss = 7.9797\n",
            "Step 2000 avg train perplexity = 2920.9698\n",
            "Validation loss after 0 epoch = 7.9246\n",
            "Validation perplexity after 0 epoch = 2764.5297\n",
            "Step 0 avg train loss = 8.0431\n",
            "Step 0 avg train perplexity = 3112.1998\n",
            "Step 1000 avg train loss = 7.9865\n",
            "Step 1000 avg train perplexity = 2940.9346\n",
            "Step 2000 avg train loss = 7.9926\n",
            "Step 2000 avg train perplexity = 2959.1171\n",
            "Validation loss after 1 epoch = 7.8942\n",
            "Validation perplexity after 1 epoch = 2681.5756\n",
            "Step 0 avg train loss = 7.9533\n",
            "Step 0 avg train perplexity = 2845.0224\n",
            "Step 1000 avg train loss = 7.9887\n",
            "Step 1000 avg train perplexity = 2947.4787\n",
            "Step 2000 avg train loss = 7.9887\n",
            "Step 2000 avg train perplexity = 2947.5871\n",
            "Validation loss after 2 epoch = 7.9016\n",
            "Validation perplexity after 2 epoch = 2701.4713\n",
            "Step 0 avg train loss = 8.0202\n",
            "Step 0 avg train perplexity = 3041.7124\n",
            "Step 1000 avg train loss = 7.9921\n",
            "Step 1000 avg train perplexity = 2957.5604\n",
            "Step 2000 avg train loss = 7.9897\n",
            "Step 2000 avg train perplexity = 2950.5064\n",
            "Validation loss after 3 epoch = 7.9241\n",
            "Validation perplexity after 3 epoch = 2763.2035\n",
            "Step 0 avg train loss = 8.0161\n",
            "Step 0 avg train perplexity = 3029.4321\n",
            "Step 1000 avg train loss = 7.9929\n",
            "Step 1000 avg train perplexity = 2959.8026\n",
            "Step 2000 avg train loss = 7.9889\n",
            "Step 2000 avg train perplexity = 2947.9175\n",
            "Validation loss after 4 epoch = 7.8843\n",
            "Validation perplexity after 4 epoch = 2655.2517\n",
            "Step 0 avg train loss = 7.8994\n",
            "Step 0 avg train perplexity = 2695.5674\n",
            "Step 1000 avg train loss = 7.9936\n",
            "Step 1000 avg train perplexity = 2962.0285\n",
            "Step 2000 avg train loss = 7.9859\n",
            "Step 2000 avg train perplexity = 2939.1355\n",
            "Validation loss after 5 epoch = 7.8785\n",
            "Validation perplexity after 5 epoch = 2639.9509\n",
            "Step 0 avg train loss = 7.9432\n",
            "Step 0 avg train perplexity = 2816.2997\n",
            "Step 1000 avg train loss = 7.9902\n",
            "Step 1000 avg train perplexity = 2951.9234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jayNuTfjHYs-",
        "colab_type": "code",
        "outputId": "b22843bd-7227-428e-fe4a-5ae84f5843d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "# find best comb (lowest validation loss)\n",
        "sorted(finetune_res.items(), key=lambda x: x[1][1][-1][1])[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a2b0615fc324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetune_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'finetune_res' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bO3PC2KfcXs",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jh1-FBYfcXt",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc2COexlfcXt",
        "colab_type": "code",
        "outputId": "b7b85774-3646-4998-e6e0-d4228bd9ccd3",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsPkFZOkfcXw",
        "colab_type": "code",
        "outputId": "a83c599e-a276-4d76-b00c-d7fc0a5d091b",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LJWwkZcfcXy",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLIuLUvfcX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def compute_cosine_similarity(emb_matrix):\n",
        "  product = torch.mm(emb_matrix, emb_matrix.T) \n",
        "  # distance_matrix = np.zeros(emb_matrix.shape[0], emb_matrix.shape[0])\n",
        "  # for i in range(emb_matrix.shape[0]):\n",
        "  #   for j in range(emb_matrix.shape[0]):\n",
        "  #     distance_matrix[i,j] = 1-spatial.distance.cosine(emb_matrix[i], emb_matrix[j])\n",
        "\n",
        "  return product "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u2IQKseLPw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PATH=\n",
        "best_model = torch.load(PATH)\n",
        "distance_matrix=compute_cosine_similarity(best_model['lookup.weight'])\n",
        "words = ['the', 'run', 'dog', 'where', 'quick']\n",
        "words_ids = [wikitext_dict.get_id(i) for i in words]\n",
        "closest_words = []\n",
        "furtherest_words = []\n",
        "for word in words:\n",
        "  row_distance = distance_matrix[wikitext_dict.get_id[word]]\n",
        "  if current_device = 'CUDA':\n",
        "    row_distance=row_distance.cpu()\n",
        "  for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "    closest_words.append(wikitext_dict.get_token(i))\n",
        "  for i in row_distance.numpy().argsort()[10:]:\n",
        "    furtherest_words.append(wikitext_dict.get_token(i))\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words)\n",
        "  print(\"the least similar words are: \", furtherest_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64VdHasfcX4",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22lYpwzsfcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_selected = words + closest_words + furtherest_words\n",
        "words_selected_ids = [wikitext_dict.get_id(i) for i in words_selected]\n",
        "umap_plot(distance_matrix, words_selected_ids, words_selected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDwB3jHfcYA",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCzDobrwfcYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distributed_distance_matrix=compute_cosine_similarity(best_model['projection.weight'])\n",
        "closest_words2 = []\n",
        "furtherest_words2 = []\n",
        "for word in words:\n",
        "  row_distance = distributed_distance_matrix[wikitext_dict.get_id[word]]\n",
        "  if current_device = 'CUDA':\n",
        "    row_distance=row_distance.cpu()\n",
        "  for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "    closest_words2.append(wikitext_dict.get_token(i))\n",
        "  for i in row_distance.numpy().argsort()[10:]:\n",
        "    furtherest_words2.append(wikitext_dict.get_token(i))\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words2)\n",
        "  print(\"the least similar words are: \", furtherest_words2)\n",
        "\n",
        "words_selected2 = words + closest_words2 + furtherest_words2\n",
        "words_selected_ids2 = [wikitext_dict.get_id(i) for i in words_selected2]\n",
        "umap_plot(distributed_distance_matrix, words_selected_ids2, words_selected2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q_6ZeVbfcYD",
        "colab_type": "text"
      },
      "source": [
        "Discussion of Results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtgckC9xfcYD",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XsuFyfyS8hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_score(logits):\n",
        "  return F.log_softmax(logits, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MDN0NX4fcYH",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 \n",
        "Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpkn6pkMfcYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=best_model\n",
        "model.eval()\n",
        "scores = {} # key: sequence, value: score\n",
        "with torch.no_grad():\n",
        "    for i, (inp, target) in enumerate(loaders['valid']):\n",
        "        inp = inp.to(current_device)\n",
        "        target = target.to(current_device)\n",
        "        device = torch.device(\"cuda\")\n",
        "        logits = model(inp)\n",
        "        # compute score\n",
        "        seq_score = F.log_softmax(logits, dim=1)\n",
        "        scores[(inp,target)] = seq_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMBtQSr5ShyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_scores = sorted(scores.items(), key=lambda x: x[1], inverse=True)[:10]\n",
        "top_sequences = [seq+tag for seq_tag, score in topscores for (seq, tag) in seq_tag]\n",
        "lowest_scores = sorted(scores.items(), key=lambda x: x[1])[:10]\n",
        "lowest_sequences = [seq+tag for seq_tag, score in lowest_scores for (seq, tag) in seq_tag]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5U6hOImfcYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvfYp0nHfcYS",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qrGetifcYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZcyAJYfcYV",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfmkPJ4fcYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n_samples in range(1000):\n",
        "    context = ('<bos>', '<bos>')\n",
        "    output = context\n",
        "    while output[-1] != '<eos>':\n",
        "        # Form conditional distribution to sample from\n",
        "        probs, tokens = [], []\n",
        "        for token in count[context]:\n",
        "            p = count[context][token] / total[context]\n",
        "            probs.append(p)\n",
        "            tokens.append(token)\n",
        "        # Sample\n",
        "        wt = np.random.choice(tokens, p=probs)\n",
        "        output = output + (wt,)\n",
        "        context = context[1:] + (wt,)\n",
        "    \n",
        "    print(' '.join(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBA1Z5kfcYZ",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2n-ec9fcYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7i24wZxfcYi",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyes6PHWfcYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}