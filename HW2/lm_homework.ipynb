{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimiw0821/DS-GA_1011_NLP/blob/master/HW2/lm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XLYAo3OBfcVk"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KKioHyAAfcVn",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfhwSMNPfcVt"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpEk23hBfcVv"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l3FyElwEfcVx",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySlA8mEVfcV1"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5WHZwSwkfcV2",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wm36MLfmTh0P",
        "outputId": "5dba24d7-1a3b-4a83-bde0-f86c58d8a0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PCNXYS4lfcWF",
        "outputId": "a66b3ee8-f238-4511-a0ca-c0f22e58d3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e5fb4656f6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramAdditive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# +1 is for <eos>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTM0XO8IfcWK"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W5xTnBuAfcWK",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, alpha, gamma, vsize):\n",
        "        self.n = n\n",
        "#         self.lam = lam\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.vsize = vsize\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for n in range(1, self.n+1):\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        \n",
        "        if self.total[prefix] > 0:\n",
        "            prob = (self.count[prefix][word] / self.total[prefix]) * self.alpha\n",
        "        else:\n",
        "            prob = self.gamma * self.ngram_prob(ngram[1:])\n",
        "        \n",
        "#         if len(ngram) >= 2:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*self.ngram_prob(ngram[1:])\n",
        "#         elif len(ngram) == 1:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*1./self.vsize\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SuS45FMOfcWM"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oBAj7JKyfcWN",
        "outputId": "ad595933-c598-48da-8486-54a898c9b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# n = 2\n",
        "# for lambda_ in np.linspace(0.1,1,10):\n",
        "#     lm2 = NGramInterpolation(n=n, lam=lambda_, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm2.estimate(datasets['train'])\n",
        "#     print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['train'])))\n",
        "#     print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Valid Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['valid'])))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-da048ee97292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramInterpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# +1 is for <eos>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lam'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqUJUo4mfcWR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ox4t2hMXfcWU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yPLub2RsfcWY"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbG5lE3CfcWZ",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZM9ArzbEfcWc"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zR28L4vPfcWd",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43KrHPW1fcWh",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DlckZW9zfcWk",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otoIoQF0fcWo",
        "outputId": "c4100ff6-feac-4008-ee13-7a5548a9f438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:17<00:00, 568.53it/s]\n",
            "100%|██████████| 8464/8464 [00:11<00:00, 767.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_x7xOMnbfcWr",
        "colab": {}
      },
      "source": [
        "# Construct Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aw1YalFTfcWu",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = wikitext_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W4qo745ffcWx",
        "outputId": "8ff54502-d769-4416-af6b-1410fa5f4b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 95816.66it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 130559.11it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 127138.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OiahWa0lfcW1",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)#, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ak-71_1fcW5"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXh2hKp7fcW7",
        "colab": {}
      },
      "source": [
        "# making a FFNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRxfJc-EfcXC",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RnnLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.rnn = nn.RNN(self.embed_dim, self.hidden_dim, self.num_layers, dropout=self.p, batch_first=self.batch_first)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs, states = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits, states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SMmYZceKfcXI",
        "colab": {}
      },
      "source": [
        "class LstmLM(torch.nn.Module):\n",
        "    def __init__(self, options):\n",
        "        '''\n",
        "        params:\n",
        "            @options: dictionary of model parameters\n",
        "        '''\n",
        "        super(LstmLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim, self.num_layers, batch_first=self.batch_first, dropout=self.p) # lstm takes word embeddings as inputs and outputs hidden states (dim=hidden_dinm)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size) # linear layer maps from hidden states to word space\n",
        "\n",
        "    def forward(self, encoded_input_sequence):\n",
        "        '''\n",
        "        Forwrad method process the input from token ids to logits\n",
        "        params:\n",
        "            @inp: input sentence\n",
        "        '''\n",
        "        embedded = self.lookup(encoded_input_sequence)\n",
        "        lstm_out, states = self.lstm(embedded)\n",
        "        logits = self.projection(lstm_out)\n",
        "        return logits, states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY90VuhCfcXL",
        "colab": {}
      },
      "source": [
        "def compute_predictions(logits):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags(words).\"\"\"\n",
        "    # Create softmax (F.softmax) function\n",
        "    softmax_output = F.softmax(logits, axis=-1)\n",
        "    predictions = torch.multinomial(softmax_output, num_samples=1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jEbfMa_ffcXN",
        "colab": {}
      },
      "source": [
        "# defining what device to use\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGd7jN0LfcXQ",
        "colab": {}
      },
      "source": [
        "# define model parameters -- options\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "options = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "thrCgjyqfcXc"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umLMmIAY2Zwm",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "  '''\n",
        "  function that computes perplexity\n",
        "  '''\n",
        "  return 2**(loss/np.log(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uHG18vvMfcXd",
        "colab": {}
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "def train_model(model, model_name, hyperparams, loaders, save=True):\n",
        "    '''\n",
        "    function to train neural  LM\n",
        "    params:\n",
        "        @model: LM object\n",
        "        @model_name: str\n",
        "        @hyperparams: dictionary of hyperparameters set for the model\n",
        "        @loaders: DataLoader\n",
        "    '''\n",
        "    print(\"Training {}:\".format(model_name))\n",
        "    \n",
        "    # criterion:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "\n",
        "    PATH = model_name + '.pth'\n",
        "    if os.path.exists(PATH): # load pre-trained\n",
        "        print(\"PATH exists!\")\n",
        "        checkpoint = torch.load(PATH, map_location=current_device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    else:\n",
        "      # optimizer:\n",
        "        model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        if hyperparams['optimizer'] == 'SGD':\n",
        "            optimizer = optim.SGD(model_params, lr=hyperparams['lr'], momentum=hyperparams['momentum'])\n",
        "        elif hyperparams['optimizer'] == 'Adam':\n",
        "            optimizer = optim.Adam(model_params, lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "\n",
        "    plot_cache = []\n",
        "    num_epochs = hyperparams['num_epochs']\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits, _ = model(inp)\n",
        "            # compute loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            # back-propogation\n",
        "            loss.backward()\n",
        "            # gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "            optimizer.step()\n",
        "            train_log_cache.append(loss.item()) # store training loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                avg_train_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                avg_train_perplexity = perplexity(avg_train_loss)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_train_loss, prec=4))\n",
        "                print('Step {} avg train perplexity = {:.{prec}f}'.format(i, avg_train_perplexity, prec=4))\n",
        "                train_log_cache = []\n",
        "\n",
        "        #do validation\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(loaders['valid']):\n",
        "                # current_batch_size = len(inp)\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                device = torch.device(\"cuda\")\n",
        "                logits, _ = model(inp)\n",
        "                # compute loss\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item()) # store validation loss\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            avg_val_perplexity = perplexity(avg_val_loss)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch, avg_val_loss, prec=4))\n",
        "            print('Validation perplexity after {} epoch = {:.{prec}f}'.format(epoch, avg_val_perplexity, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_train_loss, avg_val_loss, avg_train_perplexity, avg_val_perplexity))\n",
        "        # # early stopping\n",
        "        # if len(plot_cache)>1:\n",
        "        #   np.abs((plot_cache[epoch][1] - plot_cache[epoch-1][1])/plot_cache[epoch-1][1]) <= 0.0005\n",
        "        #   print(\"Meets early stopping criteria: Finish training\")\n",
        "        #   return plot_cache\n",
        "    \n",
        "    if save:\n",
        "        torch.save({\n",
        "            'epoch':  hyperparams['num_epochs'],\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': plot_cache}, PATH)\n",
        "\n",
        "    print('Finished training')\n",
        "    return plot_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PEdZfQfuiIix",
        "colab": {}
      },
      "source": [
        "# ### TESTING\n",
        "\n",
        "# # Fine tuning hyperparameters for LSTM\n",
        "# # fine tune: regularization\n",
        "# embed_dim = 64\n",
        "# hidden_dim = 200\n",
        "# num_layers = 2\n",
        "# dropout = 0.5\n",
        "# option = {\n",
        "#     'vocab_size': len(wikitext_dict),\n",
        "#     'embed_dim': embed_dim,\n",
        "#     'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "#     'hidden_dim': hidden_dim,\n",
        "#     'num_layers': num_layers,\n",
        "#     'dropout': dropout,\n",
        "#     'batch_first': True,\n",
        "# }\n",
        "\n",
        "# hyperparam = {\n",
        "#     'optimizer': 'Adam',\n",
        "#     'lr': 0.001,\n",
        "#     'num_epochs': 5,\n",
        "#     'weight_decay': 0\n",
        "# }\n",
        "\n",
        "# # train\n",
        "# model_name = 'LSTM_temp'\n",
        "# model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "# finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bYTI_8vfcXg",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss(losses):\n",
        "    epochs = np.array(list(range(len(losses))))\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    axes = fig.subplots(nrows=1, ncols=2)\n",
        "    # plot losses\n",
        "    axes[0].plot(epochs, [i[0] for i in losses], label='Train loss')\n",
        "    axes[0].plot(epochs, [i[1] for i in losses], label='Val loss')\n",
        "    axes[0].set_title(\"Training and Validation losses over time\")\n",
        "    axes[0].set_xlabel(\"Steps\")\n",
        "    axes[0].set_ylabel(\"Losses\")\n",
        "    axes[0].legend(loc='best')\n",
        "    # plot training & validation accuracy\n",
        "    axes[1].plot(epochs, [i[2] for i in losses], label='Train Perplexity')\n",
        "    axes[1].plot(epochs, [i[3] for i in losses], label='Val Perplexity')\n",
        "    axes[1].set_title(\"Training and Validation perplexity over time\")\n",
        "    axes[1].set_xlabel(\"Steps\")\n",
        "    axes[1].set_ylabel(\"Perplexity\")\n",
        "    axes[1].legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJvEVnWnfcXj",
        "outputId": "e2ec2383-96e9-48da-d215-dbea50988868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "# RNN with baseline hyperparameters\n",
        "baseline_hyperparams = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 10,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "model_rnn = RnnLM(options).to(current_device)\n",
        "print(model_rnn)\n",
        "base_rnn_losses = train_model(model_rnn, \"RNN_LM\", baseline_hyperparams, wikitext_loaders)\n",
        "plot_loss(base_rnn_losses)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RnnLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training RNN_LM:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-85b61e77f139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRnnLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbase_rnn_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RNN_LM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwikitext_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_rnn_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-caa1f857beaf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, hyperparams, loaders, save)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d4dfa6c671aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoded_input_sequence)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mrnn_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "veibMk9afcXp"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zC4kjNcnr3w1",
        "outputId": "bef26fb4-d58b-4f34-915e-9aec057ceead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = [64]\n",
        "hidden_dim = [128, 150]\n",
        "num_layers = [2]\n",
        "dropout = [0.3]\n",
        "options = {\n",
        "    'vocab_size': [len(wikitext_dict)],\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': [wikitext_dict.get_id('<pad>')],\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': [True],\n",
        "}\n",
        "\n",
        "regularized_hyperparams = {\n",
        "    'optimizer': ['Adam'],\n",
        "    'lr': [0.001],\n",
        "    'num_epochs': [10],\n",
        "    'weight_decay': [0]\n",
        "}\n",
        "\n",
        "finetune_res = {}\n",
        "i=0\n",
        "for option in ParameterGrid(options):\n",
        "    for hyperparam in ParameterGrid(regularized_hyperparams):\n",
        "        model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "        print(model_lstm_tuned)\n",
        "        print('option: ', option)\n",
        "        print('hyperparam: ', hyperparam)\n",
        "        # train\n",
        "        model_name = 'LSTM_Finetuned_' + str(i)\n",
        "        PATH = model_name + '.pth'\n",
        "        finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)\n",
        "        finetune_res[model_name]=({**option, **hyperparam}, finetune_lstm_losses)\n",
        "        i+=1"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.3, 'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 10, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_0:\n",
            "Step 0 avg train loss = 10.4249\n",
            "Step 0 avg train perplexity = 33686.5297\n",
            "Step 1000 avg train loss = 6.9037\n",
            "Step 1000 avg train perplexity = 995.9430\n",
            "Step 2000 avg train loss = 6.3189\n",
            "Step 2000 avg train perplexity = 554.9572\n",
            "Validation loss after 0 epoch = 5.9051\n",
            "Validation perplexity after 0 epoch = 366.8968\n",
            "Step 0 avg train loss = 5.8679\n",
            "Step 0 avg train perplexity = 353.5174\n",
            "Step 1000 avg train loss = 5.9531\n",
            "Step 1000 avg train perplexity = 384.9305\n",
            "Step 2000 avg train loss = 5.8127\n",
            "Step 2000 avg train perplexity = 334.5287\n",
            "Validation loss after 1 epoch = 5.5949\n",
            "Validation perplexity after 1 epoch = 269.0423\n",
            "Step 0 avg train loss = 5.4256\n",
            "Step 0 avg train perplexity = 227.1578\n",
            "Step 1000 avg train loss = 5.6026\n",
            "Step 1000 avg train perplexity = 271.1259\n",
            "Step 2000 avg train loss = 5.5494\n",
            "Step 2000 avg train perplexity = 257.0872\n",
            "Validation loss after 2 epoch = 5.4639\n",
            "Validation perplexity after 2 epoch = 236.0262\n",
            "Step 0 avg train loss = 5.5291\n",
            "Step 0 avg train perplexity = 251.9095\n",
            "Step 1000 avg train loss = 5.4006\n",
            "Step 1000 avg train perplexity = 221.5356\n",
            "Step 2000 avg train loss = 5.3708\n",
            "Step 2000 avg train perplexity = 215.0367\n",
            "Validation loss after 3 epoch = 5.3834\n",
            "Validation perplexity after 3 epoch = 217.7662\n",
            "Step 0 avg train loss = 5.4095\n",
            "Step 0 avg train perplexity = 223.5304\n",
            "Step 1000 avg train loss = 5.2363\n",
            "Step 1000 avg train perplexity = 187.9683\n",
            "Step 2000 avg train loss = 5.2331\n",
            "Step 2000 avg train perplexity = 187.3682\n",
            "Validation loss after 4 epoch = 5.3381\n",
            "Validation perplexity after 4 epoch = 208.1163\n",
            "Step 0 avg train loss = 4.9568\n",
            "Step 0 avg train perplexity = 142.1396\n",
            "Step 1000 avg train loss = 5.1154\n",
            "Step 1000 avg train perplexity = 166.5755\n",
            "Step 2000 avg train loss = 5.1211\n",
            "Step 2000 avg train perplexity = 167.5146\n",
            "Validation loss after 5 epoch = 5.3182\n",
            "Validation perplexity after 5 epoch = 204.0188\n",
            "Step 0 avg train loss = 5.1258\n",
            "Step 0 avg train perplexity = 168.3076\n",
            "Step 1000 avg train loss = 5.0195\n",
            "Step 1000 avg train perplexity = 151.3287\n",
            "Step 2000 avg train loss = 5.0312\n",
            "Step 2000 avg train perplexity = 153.1210\n",
            "Validation loss after 6 epoch = 5.2991\n",
            "Validation perplexity after 6 epoch = 200.1655\n",
            "Step 0 avg train loss = 5.1876\n",
            "Step 0 avg train perplexity = 179.0301\n",
            "Step 1000 avg train loss = 4.9346\n",
            "Step 1000 avg train perplexity = 139.0185\n",
            "Step 2000 avg train loss = 4.9549\n",
            "Step 2000 avg train perplexity = 141.8702\n",
            "Validation loss after 7 epoch = 5.2952\n",
            "Validation perplexity after 7 epoch = 199.3774\n",
            "Step 0 avg train loss = 4.7864\n",
            "Step 0 avg train perplexity = 119.8660\n",
            "Step 1000 avg train loss = 4.8609\n",
            "Step 1000 avg train perplexity = 129.1350\n",
            "Step 2000 avg train loss = 4.8885\n",
            "Step 2000 avg train perplexity = 132.7580\n",
            "Validation loss after 8 epoch = 5.2951\n",
            "Validation perplexity after 8 epoch = 199.3650\n",
            "Step 0 avg train loss = 4.7738\n",
            "Step 0 avg train perplexity = 118.3713\n",
            "Step 1000 avg train loss = 4.8076\n",
            "Step 1000 avg train perplexity = 122.4352\n",
            "Step 2000 avg train loss = 4.8262\n",
            "Step 2000 avg train perplexity = 124.7415\n",
            "Validation loss after 9 epoch = 5.3064\n",
            "Validation perplexity after 9 epoch = 201.6221\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 150, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (projection): Linear(in_features=150, out_features=33181, bias=True)\n",
            ")\n",
            "option:  {'batch_first': True, 'dropout': 0.3, 'embed_dim': 64, 'hidden_dim': 150, 'num_layers': 2, 'padding_idx': 2, 'vocab_size': 33181}\n",
            "hyperparam:  {'lr': 0.001, 'num_epochs': 10, 'optimizer': 'Adam', 'weight_decay': 0}\n",
            "Training LSTM_Finetuned_1:\n",
            "Step 0 avg train loss = 10.3988\n",
            "Step 0 avg train perplexity = 32821.4977\n",
            "Step 1000 avg train loss = 6.8929\n",
            "Step 1000 avg train perplexity = 985.3033\n",
            "Step 2000 avg train loss = 6.2694\n",
            "Step 2000 avg train perplexity = 528.1485\n",
            "Validation loss after 0 epoch = 5.8435\n",
            "Validation perplexity after 0 epoch = 344.9949\n",
            "Step 0 avg train loss = 5.9966\n",
            "Step 0 avg train perplexity = 402.0613\n",
            "Step 1000 avg train loss = 5.8865\n",
            "Step 1000 avg train perplexity = 360.1352\n",
            "Step 2000 avg train loss = 5.7662\n",
            "Step 2000 avg train perplexity = 319.3311\n",
            "Validation loss after 1 epoch = 5.5581\n",
            "Validation perplexity after 1 epoch = 259.3386\n",
            "Step 0 avg train loss = 5.5881\n",
            "Step 0 avg train perplexity = 267.2220\n",
            "Step 1000 avg train loss = 5.5579\n",
            "Step 1000 avg train perplexity = 259.2674\n",
            "Step 2000 avg train loss = 5.5067\n",
            "Step 2000 avg train perplexity = 246.3326\n",
            "Validation loss after 2 epoch = 5.4291\n",
            "Validation perplexity after 2 epoch = 227.9400\n",
            "Step 0 avg train loss = 5.3233\n",
            "Step 0 avg train perplexity = 205.0594\n",
            "Step 1000 avg train loss = 5.3525\n",
            "Step 1000 avg train perplexity = 211.1430\n",
            "Step 2000 avg train loss = 5.3227\n",
            "Step 2000 avg train perplexity = 204.9329\n",
            "Validation loss after 3 epoch = 5.3563\n",
            "Validation perplexity after 3 epoch = 211.9346\n",
            "Step 0 avg train loss = 5.1706\n",
            "Step 0 avg train perplexity = 176.0253\n",
            "Step 1000 avg train loss = 5.1822\n",
            "Step 1000 avg train perplexity = 178.0809\n",
            "Step 2000 avg train loss = 5.1784\n",
            "Step 2000 avg train perplexity = 177.4060\n",
            "Validation loss after 4 epoch = 5.3206\n",
            "Validation perplexity after 4 epoch = 204.5146\n",
            "Step 0 avg train loss = 5.0742\n",
            "Step 0 avg train perplexity = 159.8389\n",
            "Step 1000 avg train loss = 5.0566\n",
            "Step 1000 avg train perplexity = 157.0585\n",
            "Step 2000 avg train loss = 5.0610\n",
            "Step 2000 avg train perplexity = 157.7461\n",
            "Validation loss after 5 epoch = 5.2966\n",
            "Validation perplexity after 5 epoch = 199.6588\n",
            "Step 0 avg train loss = 4.8879\n",
            "Step 0 avg train perplexity = 132.6743\n",
            "Step 1000 avg train loss = 4.9519\n",
            "Step 1000 avg train perplexity = 141.4455\n",
            "Step 2000 avg train loss = 4.9546\n",
            "Step 2000 avg train perplexity = 141.8291\n",
            "Validation loss after 6 epoch = 5.2898\n",
            "Validation perplexity after 6 epoch = 198.3104\n",
            "Step 0 avg train loss = 4.8568\n",
            "Step 0 avg train perplexity = 128.6056\n",
            "Step 1000 avg train loss = 4.8589\n",
            "Step 1000 avg train perplexity = 128.8868\n",
            "Step 2000 avg train loss = 4.8804\n",
            "Step 2000 avg train perplexity = 131.6879\n",
            "Validation loss after 7 epoch = 5.2936\n",
            "Validation perplexity after 7 epoch = 199.0630\n",
            "Step 0 avg train loss = 4.8704\n",
            "Step 0 avg train perplexity = 130.3705\n",
            "Step 1000 avg train loss = 4.7876\n",
            "Step 1000 avg train perplexity = 120.0184\n",
            "Step 2000 avg train loss = 4.8058\n",
            "Step 2000 avg train perplexity = 122.2126\n",
            "Validation loss after 8 epoch = 5.2988\n",
            "Validation perplexity after 8 epoch = 200.0927\n",
            "Step 0 avg train loss = 4.7227\n",
            "Step 0 avg train perplexity = 112.4748\n",
            "Step 1000 avg train loss = 4.7188\n",
            "Step 1000 avg train perplexity = 112.0297\n",
            "Step 2000 avg train loss = 4.7437\n",
            "Step 2000 avg train perplexity = 114.8638\n",
            "Validation loss after 9 epoch = 5.3092\n",
            "Validation perplexity after 9 epoch = 202.1867\n",
            "Finished training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jayNuTfjHYs-",
        "outputId": "d240a4d0-7371-4dde-fc71-5a14e2ce8aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# find best comb (lowest validation loss)\n",
        "sorted(finetune_res.items(), key=lambda x: x[1][1][-1][1])[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('LSTM_Finetuned_2',\n",
              " ({'batch_first': True,\n",
              "   'dropout': 0.1,\n",
              "   'embed_dim': 64,\n",
              "   'hidden_dim': 200,\n",
              "   'lr': 0.001,\n",
              "   'num_epochs': 5,\n",
              "   'num_layers': 2,\n",
              "   'optimizer': 'Adam',\n",
              "   'padding_idx': 2,\n",
              "   'vocab_size': 33181,\n",
              "   'weight_decay': 0},\n",
              "  [(6.187857599258423,\n",
              "    5.7617946318860325,\n",
              "    486.80206313338266,\n",
              "    317.9183636962075),\n",
              "   (5.64312326002121, 5.47789755947185, 282.3431739613465, 239.3429736567724),\n",
              "   (5.345199208259583,\n",
              "    5.347554626104967,\n",
              "    209.59963441865992,\n",
              "    210.09391102340302),\n",
              "   (5.121877596855164,\n",
              "    5.282769904946381,\n",
              "    167.64985312909596,\n",
              "    196.91455524837647),\n",
              "   (4.94967449760437,\n",
              "    5.259970715360821,\n",
              "    141.12901861057756,\n",
              "    192.47585462875875)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-bO3PC2KfcXs"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Jh1-FBYfcXt"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_cq4NDNTTh0",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import umap\n",
        "except:\n",
        "    !pip install umap-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qc2COexlfcXt",
        "outputId": "abec60b6-1216-46c8-d0eb-3d10f87d191d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "%pylab inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['split']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EsPkFZOkfcXw",
        "outputId": "339f29b6-9359-402a-ae41-8f4b12fa46a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2LJWwkZcfcXy"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ioLIuLUvfcX0",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def compute_cosine_similarity(emb_matrix, word):\n",
        "    if current_device == 'CUDA':\n",
        "        emb_matrix = emb_matrix.cpu()\n",
        "    word_idx = wikitext_dict.get_id(word)\n",
        "    row_distance = torch.mm(emb_matrix[word_idx], emb_matrix.T)\n",
        "    # set the corresponding index of row distance of that word to 0 (cuz the same word must be the closest the word)\n",
        "    row_distance[word_idx] = 0\n",
        "    return row_distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9u2IQKseLPw9",
        "outputId": "5f8eccd2-012d-443a-b36b-fc4282c3cb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "PATH = 'LSTM_LM_Finetuned_0.pth'\n",
        "best_model = torch.load(PATH, map_location=current_device)\n",
        "emb_matrix = best_model['lookup.weight']\n",
        "words = ['the', 'run', 'dog', 'where', 'quick']\n",
        "words_ids = [wikitext_dict.get_id(i) for i in words]\n",
        "closest_words = []\n",
        "furtherest_words = []\n",
        "for word in words:\n",
        "    row_distance = compute_cosine_similarity(emb_matrix, word)\n",
        "    # row_distance = distance_matrix[wikitext_dict.get_id(word)]\n",
        "    if current_device == 'CUDA':\n",
        "        row_distance=row_distance.cpu()\n",
        "    for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "        closest_words.append(wikitext_dict.get_token(i))\n",
        "    for i in row_distance.numpy().argsort()[:10]:\n",
        "        furtherest_words.append(wikitext_dict.get_token(i))\n",
        "    print(\"For <{}>:\".format(word))\n",
        "    print(\"the most similar words are: \", closest_words)\n",
        "    print(\"the least similar words are: \", furtherest_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For <the>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose']\n",
            "For <run>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv']\n",
            "For <dog>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits']\n",
            "For <where>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help', 'where', 'what', ',', 'injuring', 'hires', 'joint', 'they', 'Whitehead', 'ensure', 'whom']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits', 'Livin', 'co', 'Publishers', '5191', 'Nature', 'pesos', '5', 'au', 'widening', 'Corey']\n",
            "For <quick>:\n",
            "the most similar words are:  ['the', 'The', 'thy', 'Allenton', 'his', 'implicit', '1,000th', 'Chrono', 'Plantain', 'localised', 'run', 'freeway', 'ending', 'Träumerei', 'Horgan', 'unproven', 'chronology', 'Schrute', 'IV', 'violate', 'dog', 'numismatic', 'stegosaurid', 'snowfall', 'rolled', 'threatening', 'Relative', 'Going', 'impurities', 'help', 'where', 'what', ',', 'injuring', 'hires', 'joint', 'they', 'Whitehead', 'ensure', 'whom', 'quick', 'bestselling', 'Tawakal', 'reserve', 'peaked', 'Djan', 'Cassino', 'lens', 'NBA', '364']\n",
            "the least similar words are:  ['Due', 'chin', 'Aware', 'fail', 'Associate', 'attempts', 'renditions', 'blacks', 'say', 'rose', 'unequivocally', 'would', 'Taj', 'Pablo', 'Wrigley', 'Raiden', 'soulful', 'Chas', 'desperately', 'Torv', '<', '!', '76th', 'Paterson', 'adolescent', 'Bg7', 'quasi', 'TTZ', '\"', 'gaits', 'Livin', 'co', 'Publishers', '5191', 'Nature', 'pesos', '5', 'au', 'widening', 'Corey', 'Megastore', 'because', 'Carre', 'Model', 'Smithy', 'Indonesia', 'what', 'Beloved', 'Swamp', 'anatomy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B64VdHasfcX4"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "22lYpwzsfcX5",
        "colab": {}
      },
      "source": [
        "words_selected = words + closest_words + furtherest_words\n",
        "words_selected_ids = [wikitext_dict.get_id(i) for i in words_selected]\n",
        "umap_plot(distance_matrix, words_selected_ids, words_selected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6aDwB3jHfcYA"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCzDobrwfcYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "21752cd5-b244-4198-cfdd-1e119a9d8e82"
      },
      "source": [
        "emb_matrix = best_model['projection.weight']\n",
        "closest_words2 = []\n",
        "furtherest_words2 = []\n",
        "for word in words:\n",
        "  row_distance = compute_cosine_similarity(emb_matrix, word)\n",
        "  # row_distance = distributed_distance_matrix[wikitext_dict.get_id(word)]\n",
        "  if current_device = 'CUDA':\n",
        "    row_distance=row_distance.cpu()\n",
        "  for i in row_distance.numpy().argsort()[-10:][::-1]:\n",
        "    closest_words2.append(wikitext_dict.get_token(i))\n",
        "  for i in row_distance.numpy().argsort()[10:]:\n",
        "    furtherest_words2.append(wikitext_dict.get_token(i))\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words2)\n",
        "  print(\"the least similar words are: \", furtherest_words2)\n",
        "\n",
        "words_selected2 = words + closest_words2 + furtherest_words2\n",
        "words_selected_ids2 = [wikitext_dict.get_id(i) for i in words_selected2]\n",
        "umap_plot(distributed_distance_matrix, words_selected_ids2, words_selected2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-06537fd6d334>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    if current_device = 'CUDA':\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-q_6ZeVbfcYD"
      },
      "source": [
        "Discussion of Results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HtgckC9xfcYD"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XsuFyfyS8hg",
        "colab": {}
      },
      "source": [
        "def compute_score(logits):\n",
        "  return F.log_softmax(logits, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7MDN0NX4fcYH"
      },
      "source": [
        "#### II.3.2 \n",
        "Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jpkn6pkMfcYE",
        "colab": {}
      },
      "source": [
        "model=best_model\n",
        "model.eval()\n",
        "scores = {} # key: sequence, value: score\n",
        "with torch.no_grad():\n",
        "    for i, (inp, target) in enumerate(loaders['valid']):\n",
        "        inp = inp.to(current_device)\n",
        "        target = target.to(current_device)\n",
        "        device = torch.device(\"cuda\")\n",
        "        logits = model(inp)\n",
        "        # compute score\n",
        "        seq_score = F.log_softmax(logits, dim=1)\n",
        "        scores[(inp,target)] = seq_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMBtQSr5ShyS",
        "colab": {}
      },
      "source": [
        "top_scores = sorted(scores.items(), key=lambda x: x[1], inverse=True)[:10]\n",
        "top_sequences = [seq+tag for seq_tag, score in topscores for (seq, tag) in seq_tag]\n",
        "lowest_scores = sorted(scores.items(), key=lambda x: x[1])[:10]\n",
        "lowest_sequences = [seq+tag for seq_tag, score in lowest_scores for (seq, tag) in seq_tag]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5U6hOImfcYI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bvfYp0nHfcYS"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "96qrGetifcYS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UDZcyAJYfcYV"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxWR05Rt7I_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5b851a74-4865-44e4-e3f6-18bcfd1c7ef9"
      },
      "source": [
        "best_model finetune_res['LSTM_Finetuned_0'][0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_first': True,\n",
              " 'dropout': 0.3,\n",
              " 'embed_dim': 64,\n",
              " 'hidden_dim': 128,\n",
              " 'lr': 0.001,\n",
              " 'num_epochs': 10,\n",
              " 'num_layers': 2,\n",
              " 'optimizer': 'Adam',\n",
              " 'padding_idx': 2,\n",
              " 'vocab_size': 33181,\n",
              " 'weight_decay': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DySt_zUe7t5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_options = {'batch_first': True,\n",
        " 'dropout': 0.3,\n",
        " 'embed_dim': 64,\n",
        " 'hidden_dim': 128,\n",
        " 'lr': 0.001,\n",
        " 'num_epochs': 10,\n",
        " 'num_layers': 2,\n",
        " 'optimizer': 'Adam',\n",
        " 'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        " 'vocab_size': 33181,\n",
        " 'weight_decay': 0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLbk5inDqUsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "18444f79-9fb7-47cf-f115-83ee4e82ec61"
      },
      "source": [
        "best_model = LstmLM(best_options).to(current_device)\n",
        "# initialize states and word\n",
        "states = torch.zeros(best_model.num_layers, 1, best_model.hidden_dim)\n",
        "word = '<bos>'\n",
        "samples = []\n",
        "sentence = ''\n",
        "for _ in range(1000): # generate 1000 sequences\n",
        "  while word is not '<eos>':\n",
        "    logits, states = best_model(wikitext_dict.get_id(word), states)\n",
        "    word = wikitext_dict.get_token(compute_predictions(logits))\n",
        "    sentence += ' '\n",
        "    sentence += word\n",
        "  samples.append(sentence)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-43fb98089a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# generate 1000 sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikitext_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikitext_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYazCjIqtT-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly sample 1000 examples from the validation set\n",
        "from random import sample \n",
        "valid_samples = sample(datasets['valid'], 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8HBA1Z5kfcYZ"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sr2n-ec9fcYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f5c729b3-d44b-40b0-9718-3b883c737032"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# validation set\n",
        "all_words_valid = [word for lst in valid_samples for word in lst]\n",
        "num_unique_valid = len(set(all_words_valid))\n",
        "print(num_unique_valid)\n",
        "seq_len_valid = [len(lst) for lst in valid_samples]\n",
        "print(max(seq_len_valid))\n",
        "print(min(seq_len_valid))\n",
        "plt.hist(seq_len_valid)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4915\n",
            "88\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([286., 317., 221., 104.,  34.,  23.,   8.,   5.,   1.,   1.]),\n",
              " array([ 8., 16., 24., 32., 40., 48., 56., 64., 72., 80., 88.]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mdM+glqXMGvSR1zqCXpM79Hw6n/Id3sWyTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqtVEaR0xyq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7i24wZxfcYi"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fyes6PHWfcYi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}