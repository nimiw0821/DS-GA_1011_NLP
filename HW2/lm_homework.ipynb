{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimiw0821/DS-GA_1011_NLP/blob/master/HW2/lm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLYAo3OBfcVk",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKioHyAAfcVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "aa51d7fc-665f-4bb6-a297-01656e9074a0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing the package, RESTART THIS CELL\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfhwSMNPfcVt",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpEk23hBfcVv",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3FyElwEfcVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlA8mEVfcV1",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WHZwSwkfcV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCNXYS4lfcWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "f7a90682-f601-426a-ebf5-1b15140115de"
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-30 00:10:54--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.24.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.24.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 00:10:54--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 00:10:55--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.24.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.24.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!bX_TsBozVG3IJtFXBTOUGP3-SCba63QbmEbC82xJu4M5BAWmWbfXjQj6GR13QmPng2PhU7vf3f3BPLR_-zjQZZpbt5vAFeruzq2ANDMh5vPrM0qkUEGnUE73aueHnXNiEhYgxMN9BFEqDQKdRdcoB9Pv7tCgrNd-rDqD4WyOFcqXE1M6p5G0c1LJ8MqLW5egeF8TyYpg1rO6HonWacwlHS2tnSkDoIvLi9_ob1bKxzl6MK5NXOP79eq87HbWKAse59ugNlDX07ugsG4gKjnPvH6nKUpaHzoU96cyChckZH9BlJjk_duBe8BwQ7KVjx-EsjUmpACLnkZBqss7-Zj1mF1QLb1XcEZPbODvb5y3mUSG9jDTZo_ZJ8t5TQXPmAniMMN8EPDqnUVeHX5hOsewaOyz3fvKCEKzhtCfldGw-wfB6Ge1soTO4OvydDbe8yoHE921JvtGwXHV5OZBBaFznVvaOEgjsNVAEKKqPPHaQn8ZH8y0G5uBOnmIAUQCFXt166nZT9Bn4J8vawZM2sTbKU03SSyYc6MCkajY1y1FRxeNyFXcLvjWVMIm-D42j79IrwTZ3josQ6paW_VaS16GXgYg9_FWTgH6GKaSNC_R-KmnLXleeXYgYeJUYkx7fnd1huKAtpvUKfuO8XslFMWGH1PCTsKWZJ4cntU4gsUvCV_S6ZzjJP_b5alZvlTPGAi2IVHjs3n8HRccpdLsCRClozhSQ07jpz57izskLxEBkZAYAlue0hpxcMuXpNeou0veXYwY87LIdyTtoBoVjGp4x7Mw7pVQ6eApXjERiWN681FmGCl6S48BKK1upB1UtCEoP9UOi1B_AY5zq2k2ZL4VbqH7lIDCDunzamCXiMUQr9EQvlJOmXKKik32XOm_QhpFqEYJCxovy61nhANb5Fj4G9SrY4jb1E5vUhQ9v-RyzMWtJ_AYohmKp1SD-R4cqh8vRaACLTiZkYepi1mwIgKRNND1m9ZhWe42QLXBTTfg4ZNn9jp5xpD98CTMizg6okiIU1t_9y3zHn_1E41mMqz4G0TXw9X08z0xVGhs7VjTuMtL_JSkqW2RqbMCFL7HSEoB22qHJRUqjkOj7JaIxKO2C5DQRfyn7NEzC-I0mjai0Ns7WPEzqtQU_Ei9vNzHpO1AGp9PWPUOY7Wr8Rb__f65QkP9vfKwPjzC_kgrD8r0zr5xptw9D7bX9j0-ZPa8YDnl8l0fpcxzL21Spfymdz3FPtHCPNvRF99KWUjkcFLqKaCvJZwAXCUkifpo4zcXkj2xh7oFd9lGi-G_gbpStWaTLFwwMkQRIvMtRWI9RRqVspodfdtxtn6ecdEiIQ213ci7zzs_rzwOXq9htCSQCgXaroJjaxZl0OazgvasQB0IrQJK6ruwfvgpSg5hHF7PdaqW6jT3RA7Op_2aHGo./download [following]\n",
            "--2019-09-30 00:10:55--  https://public.boxcloud.com/d/1/b1!bX_TsBozVG3IJtFXBTOUGP3-SCba63QbmEbC82xJu4M5BAWmWbfXjQj6GR13QmPng2PhU7vf3f3BPLR_-zjQZZpbt5vAFeruzq2ANDMh5vPrM0qkUEGnUE73aueHnXNiEhYgxMN9BFEqDQKdRdcoB9Pv7tCgrNd-rDqD4WyOFcqXE1M6p5G0c1LJ8MqLW5egeF8TyYpg1rO6HonWacwlHS2tnSkDoIvLi9_ob1bKxzl6MK5NXOP79eq87HbWKAse59ugNlDX07ugsG4gKjnPvH6nKUpaHzoU96cyChckZH9BlJjk_duBe8BwQ7KVjx-EsjUmpACLnkZBqss7-Zj1mF1QLb1XcEZPbODvb5y3mUSG9jDTZo_ZJ8t5TQXPmAniMMN8EPDqnUVeHX5hOsewaOyz3fvKCEKzhtCfldGw-wfB6Ge1soTO4OvydDbe8yoHE921JvtGwXHV5OZBBaFznVvaOEgjsNVAEKKqPPHaQn8ZH8y0G5uBOnmIAUQCFXt166nZT9Bn4J8vawZM2sTbKU03SSyYc6MCkajY1y1FRxeNyFXcLvjWVMIm-D42j79IrwTZ3josQ6paW_VaS16GXgYg9_FWTgH6GKaSNC_R-KmnLXleeXYgYeJUYkx7fnd1huKAtpvUKfuO8XslFMWGH1PCTsKWZJ4cntU4gsUvCV_S6ZzjJP_b5alZvlTPGAi2IVHjs3n8HRccpdLsCRClozhSQ07jpz57izskLxEBkZAYAlue0hpxcMuXpNeou0veXYwY87LIdyTtoBoVjGp4x7Mw7pVQ6eApXjERiWN681FmGCl6S48BKK1upB1UtCEoP9UOi1B_AY5zq2k2ZL4VbqH7lIDCDunzamCXiMUQr9EQvlJOmXKKik32XOm_QhpFqEYJCxovy61nhANb5Fj4G9SrY4jb1E5vUhQ9v-RyzMWtJ_AYohmKp1SD-R4cqh8vRaACLTiZkYepi1mwIgKRNND1m9ZhWe42QLXBTTfg4ZNn9jp5xpD98CTMizg6okiIU1t_9y3zHn_1E41mMqz4G0TXw9X08z0xVGhs7VjTuMtL_JSkqW2RqbMCFL7HSEoB22qHJRUqjkOj7JaIxKO2C5DQRfyn7NEzC-I0mjai0Ns7WPEzqtQU_Ei9vNzHpO1AGp9PWPUOY7Wr8Rb__f65QkP9vfKwPjzC_kgrD8r0zr5xptw9D7bX9j0-ZPa8YDnl8l0fpcxzL21Spfymdz3FPtHCPNvRF99KWUjkcFLqKaCvJZwAXCUkifpo4zcXkj2xh7oFd9lGi-G_gbpStWaTLFwwMkQRIvMtRWI9RRqVspodfdtxtn6ecdEiIQ213ci7zzs_rzwOXq9htCSQCgXaroJjaxZl0OazgvasQB0IrQJK6ruwfvgpSg5hHF7PdaqW6jT3RA7Op_2aHGo./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.24.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.24.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  23.3MB/s    in 0.5s    \n",
            "\n",
            "2019-09-30 00:10:56 (23.3 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTM0XO8IfcWK",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xTnBuAfcWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, alpha, gamma, vsize):\n",
        "        self.n = n\n",
        "#         self.lam = lam\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.vsize = vsize\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for n in range(1, self.n+1):\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        \n",
        "        if self.total[prefix] > 0:\n",
        "            prob = (self.count[prefix][word] / self.total[prefix]) * self.alpha\n",
        "        else:\n",
        "            prob = self.gamma * self.ngram_prob(ngram[1:])\n",
        "        \n",
        "#         if len(ngram) >= 2:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*self.ngram_prob(ngram[1:])\n",
        "#         elif len(ngram) == 1:\n",
        "#             prob = (self.count[prefix][word] / self.total[prefix]) * self.lam + (1-self.lam)*1./self.vsize\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuS45FMOfcWM",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAj7JKyfcWN",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c5972dd-be14-48f3-8f3c-f89f245f9ba6"
      },
      "source": [
        "n = 2\n",
        "for lambda_ in np.linspace(0.1,1,10):\n",
        "    lm2 = NGramInterpolation(n=n, lam=lambda_, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm2.estimate(datasets['train'])\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Train Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['train'])))\n",
        "    print(\"Baseline (Interpolation, n=%d, lambda=%.4f)) Valid Perplexity: %.3f\" % (n, lambda_, perplexity(lm2, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When lambda = 0.1\n",
            "Baseline (Interpolation, n=2, delta=0.1000)) Train Perplexity: 517.683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3a96ce3a6820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline (Interpolation, n=%d, delta=%.4f)) Valid Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-98f3e2a94251>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogp_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogp_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_logp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mn_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogp_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36msequence_logp\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtotal_logp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_logp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-0c7bba0949d5>\u001b[0m in \u001b[0;36mngram_prob\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqUJUo4mfcWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox4t2hMXfcWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPLub2RsfcWY",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbG5lE3CfcWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM9ArzbEfcWc",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR28L4vPfcWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43KrHPW1fcWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlckZW9zfcWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otoIoQF0fcWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "1e588ddc-6693-4247-c2f5-c92b53636cdc"
      },
      "source": [
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:15<00:00, 578.97it/s]\n",
            "100%|██████████| 8464/8464 [00:10<00:00, 792.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x7xOMnbfcWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct Datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw1YalFTfcWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = wikitext_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4qo745ffcWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "43d1f079-9998-4d7a-853b-220a3ff2f660"
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 123491.21it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 11123.70it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 123536.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiahWa0lfcW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ak-71_1fcW5",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXh2hKp7fcW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making a FFNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRxfJc-EfcXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RnnLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.rnn = nn.RNN(self.embed_dim, self.hidden_dim, self.num_layers, dropout=self.p, batch_first=self.batch_first)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmYZceKfcXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmLM(torch.nn.Module):\n",
        "    def __init__(self, options):\n",
        "        '''\n",
        "        params:\n",
        "            @options: dictionary of model parameters\n",
        "        '''\n",
        "        super(LstmLM, self).__init__()\n",
        "        self.hidden_dim = options['hidden_dim']\n",
        "        self.vocab_size = options['vocab_size']\n",
        "        self.padding_idx = options['padding_idx']\n",
        "        self.num_layers = options['num_layers']\n",
        "        self.batch_first = options['batch_first'] # boolean\n",
        "        self.embed_dim = options['embed_dim']\n",
        "        self.p = options['dropout']\n",
        "        \n",
        "        self.lookup = nn.Embedding(self.vocab_size, self.embed_dim, self.padding_idx)\n",
        "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim, self.num_layers, batch_first=self.batch_first, dropout=self.p) # lstm takes word embeddings as inputs and outputs hidden states (dim=hidden_dinm)\n",
        "        self.projection = nn.Linear(self.hidden_dim, self.vocab_size) # linear layer maps from hidden states to word space\n",
        "\n",
        "    def forward(self, encoded_input_sequence):\n",
        "        '''\n",
        "        Forwrad method process the input from token ids to logits\n",
        "        params:\n",
        "            @inp: input sentence\n",
        "        '''\n",
        "        embedded = self.lookup(encoded_input_sequence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        logits = self.projection(lstm_out)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY90VuhCfcXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(logits):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags(words).\"\"\"\n",
        "    # Create softmax (F.softmax) function\n",
        "    softmax_output = F.softmax(logits, axis=-1)\n",
        "    predictions = torch.argmax(softmax_output, axis=-1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEbfMa_ffcXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a model, criterion and optimizer\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGd7jN0LfcXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model parameters -- options\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "options = {\n",
        "    'vocab_size': len(wikitext_dict),\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrCgjyqfcXc",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLMmIAY2Zwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(loss):\n",
        "  '''\n",
        "  function that computes perplexity\n",
        "  '''\n",
        "  return 2**(loss/np.log(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHG18vvMfcXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "def train_model(model, model_name, hyperparams, loaders, save=True):\n",
        "    '''\n",
        "    function to train neural  LM\n",
        "    params:\n",
        "        @model: LM object\n",
        "        @model_name: str\n",
        "        @hyperparams: dictionary of hyperparameters set for the model\n",
        "        @loaders: DataLoader\n",
        "    '''\n",
        "    print(\"Training {}:\".format(model_name))\n",
        "    \n",
        "    # criterion:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "    \n",
        "    # optimizer:\n",
        "    model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    if hyperparams['optimizer'] == 'SGD':\n",
        "        optimizer = optim.SGD(model_params, lr=hyperparams['lr'], momentum=hyperparams['momentum'])\n",
        "    elif hyperparams['optimizer'] == 'Adam':\n",
        "        optimizer = optim.Adam(model_params, lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "    \n",
        "    plot_cache = []\n",
        "    num_epochs = hyperparams['num_epochs']\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            # compute loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            # back-propogation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_log_cache.append(loss.item()) # store training loss\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                avg_train_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                avg_train_perplexity = perplexity(avg_train_loss)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_train_loss, prec=4))\n",
        "                print('Step {} avg train perplexity = {:.{prec}f}'.format(i, avg_train_perplexity, prec=4))\n",
        "                train_log_cache = []\n",
        "\n",
        "        #do validation\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(loaders['valid']):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                logits = model(inp)\n",
        "                # compute loss\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item()) # store validation loss\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            avg_val_perplexity = perplexity(avg_val_loss)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch, avg_val_loss, prec=4))\n",
        "            print('Validation perplexity after {} epoch = {:.{prec}f}'.format(epoch, avg_val_perplexity, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_train_loss, avg_val_loss, avg_train_perplexity, avg_val_perplexity))\n",
        "        # # early stopping\n",
        "        # if len(plot_cache)>1:\n",
        "        #   np.abs((plot_cache[epoch][1] - plot_cache[epoch-1][1])/plot_cache[epoch-1][1]) <= 0.0005\n",
        "        #   print(\"Meets early stopping criteria: Finish training\")\n",
        "        #   return plot_cache\n",
        "    \n",
        "    if save:\n",
        "      torch.save(model, model_name+'.pth')\n",
        "\n",
        "    print('Finished training')\n",
        "    return plot_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bYTI_8vfcXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_loss(losses):\n",
        "    epochs = np.array(list(range(len(losses))))\n",
        "    fig = plt.figure(figsize = (10,5))\n",
        "    axes = fig.subplots(nrows=1, ncols=2)\n",
        "    # plot losses\n",
        "    axes[0].plot(epochs, [i[0] for i in losses], label='Train loss')\n",
        "    axes[0].plot(epochs, [i[1] for i in losses], label='Val loss')\n",
        "    axes[0].set_title(\"Training and Validation losses over time\")\n",
        "    axes[0].set_xlabel(\"Steps\")\n",
        "    axes[0].set_ylabel(\"Losses\")\n",
        "    axes[0].legend(loc='best')\n",
        "    # plot training & validation accuracy\n",
        "    axes[1].plot(epochs, [i[2] for i in losses], label='Train Perplexity')\n",
        "    axes[1].plot(epochs, [i[3] for i in losses], label='Val Perplexity')\n",
        "    axes[1].set_title(\"Training and Validation perplexity over time\")\n",
        "    axes[1].set_xlabel(\"Steps\")\n",
        "    axes[1].set_ylabel(\"Perplexity\")\n",
        "    axes[1].legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvEVnWnfcXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97b13561-46e6-423a-dec0-7e1849aaf628"
      },
      "source": [
        "# RNN with baseline hyperparameters\n",
        "baseline_hyperparams = {\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 0.001,\n",
        "    'num_epochs': 10,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "model_rnn = RnnLM(options).to(current_device)\n",
        "print(model_rnn)\n",
        "base_rnn_losses = train_model(model_rnn, \"RNN LM\", baseline_hyperparams, wikitext_loaders)\n",
        "plot_loss(base_rnn_losses)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RnnLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training RNN LM:\n",
            "Step 0 avg train loss = 10.4270\n",
            "Step 0 avg train perplexity = 33758.1503\n",
            "Step 1000 avg train loss = 6.6126\n",
            "Step 1000 avg train perplexity = 744.4150\n",
            "Step 2000 avg train loss = 6.0335\n",
            "Step 2000 avg train perplexity = 417.1554\n",
            "Validation loss after 0 epoch = 5.6939\n",
            "Validation perplexity after 0 epoch = 297.0385\n",
            "Step 0 avg train loss = 5.6820\n",
            "Step 0 avg train perplexity = 293.5417\n",
            "Step 1000 avg train loss = 5.6971\n",
            "Step 1000 avg train perplexity = 298.0102\n",
            "Step 2000 avg train loss = 5.6176\n",
            "Step 2000 avg train perplexity = 275.2413\n",
            "Validation loss after 1 epoch = 5.5130\n",
            "Validation perplexity after 1 epoch = 247.8982\n",
            "Step 0 avg train loss = 5.3179\n",
            "Step 0 avg train perplexity = 203.9596\n",
            "Step 1000 avg train loss = 5.4113\n",
            "Step 1000 avg train perplexity = 223.9276\n",
            "Step 2000 avg train loss = 5.3958\n",
            "Step 2000 avg train perplexity = 220.4830\n",
            "Validation loss after 2 epoch = 5.4263\n",
            "Validation perplexity after 2 epoch = 227.2965\n",
            "Step 0 avg train loss = 5.2413\n",
            "Step 0 avg train perplexity = 188.9169\n",
            "Step 1000 avg train loss = 5.2330\n",
            "Step 1000 avg train perplexity = 187.3567\n",
            "Step 2000 avg train loss = 5.2359\n",
            "Step 2000 avg train perplexity = 187.9024\n",
            "Validation loss after 3 epoch = 5.3787\n",
            "Validation perplexity after 3 epoch = 216.7451\n",
            "Step 0 avg train loss = 4.7953\n",
            "Step 0 avg train perplexity = 120.9396\n",
            "Step 1000 avg train loss = 5.0958\n",
            "Step 1000 avg train perplexity = 163.3268\n",
            "Step 2000 avg train loss = 5.1287\n",
            "Step 2000 avg train perplexity = 168.8046\n",
            "Validation loss after 4 epoch = 5.3551\n",
            "Validation perplexity after 4 epoch = 211.6891\n",
            "Step 0 avg train loss = 4.7115\n",
            "Step 0 avg train perplexity = 111.2227\n",
            "Step 1000 avg train loss = 5.0011\n",
            "Step 1000 avg train perplexity = 148.5789\n",
            "Step 2000 avg train loss = 5.0294\n",
            "Step 2000 avg train perplexity = 152.8361\n",
            "Validation loss after 5 epoch = 5.3368\n",
            "Validation perplexity after 5 epoch = 207.8545\n",
            "Step 0 avg train loss = 4.7720\n",
            "Step 0 avg train perplexity = 118.1603\n",
            "Step 1000 avg train loss = 4.9093\n",
            "Step 1000 avg train perplexity = 135.5497\n",
            "Step 2000 avg train loss = 4.9561\n",
            "Step 2000 avg train perplexity = 142.0367\n",
            "Validation loss after 6 epoch = 5.3327\n",
            "Validation perplexity after 6 epoch = 206.9865\n",
            "Step 0 avg train loss = 4.7905\n",
            "Step 0 avg train perplexity = 120.3583\n",
            "Step 1000 avg train loss = 4.8429\n",
            "Step 1000 avg train perplexity = 126.8318\n",
            "Step 2000 avg train loss = 4.8850\n",
            "Step 2000 avg train perplexity = 132.2841\n",
            "Validation loss after 7 epoch = 5.3260\n",
            "Validation perplexity after 7 epoch = 205.6132\n",
            "Step 0 avg train loss = 4.7967\n",
            "Step 0 avg train perplexity = 121.1134\n",
            "Step 1000 avg train loss = 4.7820\n",
            "Step 1000 avg train perplexity = 119.3417\n",
            "Step 2000 avg train loss = 4.8291\n",
            "Step 2000 avg train perplexity = 125.0990\n",
            "Validation loss after 8 epoch = 5.3242\n",
            "Validation perplexity after 8 epoch = 205.2480\n",
            "Step 0 avg train loss = 4.8094\n",
            "Step 0 avg train perplexity = 122.6596\n",
            "Step 1000 avg train loss = 4.7301\n",
            "Step 1000 avg train perplexity = 113.3060\n",
            "Step 2000 avg train loss = 4.7750\n",
            "Step 2000 avg train perplexity = 118.5078\n",
            "Validation loss after 9 epoch = 5.3320\n",
            "Validation perplexity after 9 epoch = 206.8420\n",
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAFNCAYAAAC0ZpNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCYiIiISYBTQRERERALM/wdWIdIJF96vGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veibMk9afcXp",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4kjNcnr3w1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7778f3c5-09fb-43f3-b270-952d995ccb32"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections import defaultdict\n",
        "\n",
        "# Fine tuning hyperparameters for LSTM\n",
        "# fine tune: regularization\n",
        "embed_dim = [64]\n",
        "hidden_dim = [128, 200]\n",
        "num_layers = [2, 5]\n",
        "dropout = [0.1, 0.3]\n",
        "options = {\n",
        "    'vocab_size': [len(wikitext_dict)],\n",
        "    'embed_dim': embed_dim,\n",
        "    'padding_idx': [wikitext_dict.get_id('<pad>')],\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers,\n",
        "    'dropout': dropout,\n",
        "    'batch_first': [True],\n",
        "}\n",
        "\n",
        "regularized_hyperparams = {\n",
        "    'optimizer': ['Adam'],\n",
        "    'lr': [0.001],\n",
        "    'num_epochs': [10],\n",
        "    'weight_decay': [0, 0.05]\n",
        "}\n",
        "\n",
        "finetune_res = {}\n",
        "i=0\n",
        "for option in ParameterGrid(options):\n",
        "    for hyperparam in ParameterGrid(regularized_hyperparams):\n",
        "      # print({**option, **hyperparam})\n",
        "      model_lstm_tuned = LstmLM(option).to(current_device)\n",
        "      print(model_lstm_tuned)\n",
        "      # train\n",
        "      model_name = 'LSTM LM Finetuned_' + str(i)\n",
        "      model_path = model_name + '.pth'\n",
        "    #   if os.path.exists(model_path):\n",
        "    #     model_dict = torch.load(model_path)\n",
        "\n",
        "    # if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "    #     raise EOFError('Download pretrained model!')\n",
        "    # model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    # options = model_dict['options']\n",
        "    # model = RNNLanguageModel(options).to(current_device)\n",
        "    # model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "      finetune_lstm_losses = train_model(model_lstm_tuned, model_name, hyperparam, wikitext_loaders)\n",
        "      finetune_res[model_name]=({**option, **hyperparam}, finetune_lstm_losses)\n",
        "      i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_0:\n",
            "Step 0 avg train loss = 10.4045\n",
            "Step 0 avg train perplexity = 33008.8348\n",
            "Step 1000 avg train loss = 6.8793\n",
            "Step 1000 avg train perplexity = 971.9538\n",
            "Step 2000 avg train loss = 6.2653\n",
            "Step 2000 avg train perplexity = 526.0222\n",
            "Validation loss after 0 epoch = 5.8405\n",
            "Validation perplexity after 0 epoch = 343.9527\n",
            "Step 0 avg train loss = 5.8632\n",
            "Step 0 avg train perplexity = 351.8602\n",
            "Step 1000 avg train loss = 5.8763\n",
            "Step 1000 avg train perplexity = 356.4983\n",
            "Step 2000 avg train loss = 5.7495\n",
            "Step 2000 avg train perplexity = 314.0393\n",
            "Validation loss after 1 epoch = 5.5504\n",
            "Validation perplexity after 1 epoch = 257.3284\n",
            "Step 0 avg train loss = 5.5302\n",
            "Step 0 avg train perplexity = 252.1958\n",
            "Step 1000 avg train loss = 5.5366\n",
            "Step 1000 avg train perplexity = 253.8093\n",
            "Step 2000 avg train loss = 5.4868\n",
            "Step 2000 avg train perplexity = 241.4948\n",
            "Validation loss after 2 epoch = 5.4252\n",
            "Validation perplexity after 2 epoch = 227.0674\n",
            "Step 0 avg train loss = 5.4429\n",
            "Step 0 avg train perplexity = 231.1087\n",
            "Step 1000 avg train loss = 5.3174\n",
            "Step 1000 avg train perplexity = 203.8433\n",
            "Step 2000 avg train loss = 5.3007\n",
            "Step 2000 avg train perplexity = 200.4700\n",
            "Validation loss after 3 epoch = 5.3461\n",
            "Validation perplexity after 3 epoch = 209.7904\n",
            "Step 0 avg train loss = 5.3403\n",
            "Step 0 avg train perplexity = 208.5796\n",
            "Step 1000 avg train loss = 5.1551\n",
            "Step 1000 avg train perplexity = 173.3086\n",
            "Step 2000 avg train loss = 5.1457\n",
            "Step 2000 avg train perplexity = 171.6913\n",
            "Validation loss after 4 epoch = 5.3036\n",
            "Validation perplexity after 4 epoch = 201.0501\n",
            "Step 0 avg train loss = 5.0305\n",
            "Step 0 avg train perplexity = 153.0163\n",
            "Step 1000 avg train loss = 5.0217\n",
            "Step 1000 avg train perplexity = 151.6663\n",
            "Step 2000 avg train loss = 5.0268\n",
            "Step 2000 avg train perplexity = 152.4489\n",
            "Validation loss after 5 epoch = 5.2840\n",
            "Validation perplexity after 5 epoch = 197.1648\n",
            "Step 0 avg train loss = 4.9005\n",
            "Step 0 avg train perplexity = 134.3627\n",
            "Step 1000 avg train loss = 4.9165\n",
            "Step 1000 avg train perplexity = 136.5194\n",
            "Step 2000 avg train loss = 4.9225\n",
            "Step 2000 avg train perplexity = 137.3395\n",
            "Validation loss after 6 epoch = 5.2765\n",
            "Validation perplexity after 6 epoch = 195.6820\n",
            "Step 0 avg train loss = 4.9155\n",
            "Step 0 avg train perplexity = 136.3943\n",
            "Step 1000 avg train loss = 4.8232\n",
            "Step 1000 avg train perplexity = 124.3597\n",
            "Step 2000 avg train loss = 4.8377\n",
            "Step 2000 avg train perplexity = 126.1809\n",
            "Validation loss after 7 epoch = 5.2754\n",
            "Validation perplexity after 7 epoch = 195.4664\n",
            "Step 0 avg train loss = 4.7784\n",
            "Step 0 avg train perplexity = 118.9170\n",
            "Step 1000 avg train loss = 4.7439\n",
            "Step 1000 avg train perplexity = 114.8812\n",
            "Step 2000 avg train loss = 4.7674\n",
            "Step 2000 avg train perplexity = 117.6098\n",
            "Validation loss after 8 epoch = 5.2789\n",
            "Validation perplexity after 8 epoch = 196.1448\n",
            "Step 0 avg train loss = 4.7988\n",
            "Step 0 avg train perplexity = 121.3627\n",
            "Step 1000 avg train loss = 4.6765\n",
            "Step 1000 avg train perplexity = 107.3984\n",
            "Step 2000 avg train loss = 4.7026\n",
            "Step 2000 avg train perplexity = 110.2380\n",
            "Validation loss after 9 epoch = 5.2947\n",
            "Validation perplexity after 9 epoch = 199.2787\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_1:\n",
            "Step 0 avg train loss = 10.4155\n",
            "Step 0 avg train perplexity = 33373.4482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LstmLM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1000 avg train loss = 8.1659\n",
            "Step 1000 avg train perplexity = 3518.8269\n",
            "Step 2000 avg train loss = 7.9725\n",
            "Step 2000 avg train perplexity = 2900.0823\n",
            "Validation loss after 0 epoch = 7.8720\n",
            "Validation perplexity after 0 epoch = 2622.8098\n",
            "Step 0 avg train loss = 8.0096\n",
            "Step 0 avg train perplexity = 3009.6460\n",
            "Step 1000 avg train loss = 7.9878\n",
            "Step 1000 avg train perplexity = 2944.8541\n",
            "Step 2000 avg train loss = 7.9897\n",
            "Step 2000 avg train perplexity = 2950.3447\n",
            "Validation loss after 1 epoch = 7.8930\n",
            "Validation perplexity after 1 epoch = 2678.5686\n",
            "Step 0 avg train loss = 8.1563\n",
            "Step 0 avg train perplexity = 3485.0996\n",
            "Step 1000 avg train loss = 7.9861\n",
            "Step 1000 avg train perplexity = 2939.9298\n",
            "Step 2000 avg train loss = 7.9911\n",
            "Step 2000 avg train perplexity = 2954.5875\n",
            "Validation loss after 2 epoch = 7.8775\n",
            "Validation perplexity after 2 epoch = 2637.3390\n",
            "Step 0 avg train loss = 8.0078\n",
            "Step 0 avg train perplexity = 3004.3551\n",
            "Step 1000 avg train loss = 7.9903\n",
            "Step 1000 avg train perplexity = 2952.1592\n",
            "Step 2000 avg train loss = 7.9897\n",
            "Step 2000 avg train perplexity = 2950.4052\n",
            "Validation loss after 3 epoch = 7.9243\n",
            "Validation perplexity after 3 epoch = 2763.6682\n",
            "Step 0 avg train loss = 8.0399\n",
            "Step 0 avg train perplexity = 3102.3083\n",
            "Step 1000 avg train loss = 7.9902\n",
            "Step 1000 avg train perplexity = 2952.0075\n",
            "Step 2000 avg train loss = 7.9915\n",
            "Step 2000 avg train perplexity = 2955.8145\n",
            "Validation loss after 4 epoch = 7.9085\n",
            "Validation perplexity after 4 epoch = 2720.2002\n",
            "Step 0 avg train loss = 8.0117\n",
            "Step 0 avg train perplexity = 3015.9413\n",
            "Step 1000 avg train loss = 7.9869\n",
            "Step 1000 avg train perplexity = 2942.1033\n",
            "Step 2000 avg train loss = 7.9907\n",
            "Step 2000 avg train perplexity = 2953.4430\n",
            "Validation loss after 5 epoch = 7.8901\n",
            "Validation perplexity after 5 epoch = 2670.5797\n",
            "Step 0 avg train loss = 7.8696\n",
            "Step 0 avg train perplexity = 2616.5832\n",
            "Step 1000 avg train loss = 7.9881\n",
            "Step 1000 avg train perplexity = 2945.6201\n",
            "Step 2000 avg train loss = 7.9945\n",
            "Step 2000 avg train perplexity = 2964.5379\n",
            "Validation loss after 6 epoch = 7.9134\n",
            "Validation perplexity after 6 epoch = 2733.7883\n",
            "Step 0 avg train loss = 8.0142\n",
            "Step 0 avg train perplexity = 3023.6940\n",
            "Step 1000 avg train loss = 7.9908\n",
            "Step 1000 avg train perplexity = 2953.5335\n",
            "Step 2000 avg train loss = 7.9888\n",
            "Step 2000 avg train perplexity = 2947.7894\n",
            "Validation loss after 7 epoch = 7.9136\n",
            "Validation perplexity after 7 epoch = 2734.1175\n",
            "Step 0 avg train loss = 8.0147\n",
            "Step 0 avg train perplexity = 3024.9775\n",
            "Step 1000 avg train loss = 7.9871\n",
            "Step 1000 avg train perplexity = 2942.6401\n",
            "Step 2000 avg train loss = 7.9914\n",
            "Step 2000 avg train perplexity = 2955.5465\n",
            "Validation loss after 8 epoch = 7.8825\n",
            "Validation perplexity after 8 epoch = 2650.4560\n",
            "Step 0 avg train loss = 7.9418\n",
            "Step 0 avg train perplexity = 2812.5353\n",
            "Step 1000 avg train loss = 7.9906\n",
            "Step 1000 avg train perplexity = 2952.9231\n",
            "Step 2000 avg train loss = 7.9881\n",
            "Step 2000 avg train perplexity = 2945.7389\n",
            "Validation loss after 9 epoch = 7.9184\n",
            "Validation perplexity after 9 epoch = 2747.3615\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_2:\n",
            "Step 0 avg train loss = 10.4087\n",
            "Step 0 avg train perplexity = 33148.2054\n",
            "Step 1000 avg train loss = 7.0818\n",
            "Step 1000 avg train perplexity = 1190.1095\n",
            "Step 2000 avg train loss = 6.9147\n",
            "Step 2000 avg train perplexity = 1006.9370\n",
            "Validation loss after 0 epoch = 6.7177\n",
            "Validation perplexity after 0 epoch = 826.8771\n",
            "Step 0 avg train loss = 6.8864\n",
            "Step 0 avg train perplexity = 978.8410\n",
            "Step 1000 avg train loss = 6.8719\n",
            "Step 1000 avg train perplexity = 964.8163\n",
            "Step 2000 avg train loss = 6.8774\n",
            "Step 2000 avg train perplexity = 970.0671\n",
            "Validation loss after 1 epoch = 6.7070\n",
            "Validation perplexity after 1 epoch = 818.0745\n",
            "Step 0 avg train loss = 6.8066\n",
            "Step 0 avg train perplexity = 903.7579\n",
            "Step 1000 avg train loss = 6.8539\n",
            "Step 1000 avg train perplexity = 947.6095\n",
            "Step 2000 avg train loss = 6.8607\n",
            "Step 2000 avg train perplexity = 954.0571\n",
            "Validation loss after 2 epoch = 6.7091\n",
            "Validation perplexity after 2 epoch = 819.8487\n",
            "Step 0 avg train loss = 6.8129\n",
            "Step 0 avg train perplexity = 909.5324\n",
            "Step 1000 avg train loss = 6.8409\n",
            "Step 1000 avg train perplexity = 935.3397\n",
            "Step 2000 avg train loss = 6.8484\n",
            "Step 2000 avg train perplexity = 942.3971\n",
            "Validation loss after 3 epoch = 6.7129\n",
            "Validation perplexity after 3 epoch = 822.9806\n",
            "Step 0 avg train loss = 6.6584\n",
            "Step 0 avg train perplexity = 779.3406\n",
            "Step 1000 avg train loss = 6.8321\n",
            "Step 1000 avg train perplexity = 927.1632\n",
            "Step 2000 avg train loss = 6.8413\n",
            "Step 2000 avg train perplexity = 935.6645\n",
            "Validation loss after 4 epoch = 6.7131\n",
            "Validation perplexity after 4 epoch = 823.0852\n",
            "Step 0 avg train loss = 6.8460\n",
            "Step 0 avg train perplexity = 940.1319\n",
            "Step 1000 avg train loss = 6.8251\n",
            "Step 1000 avg train perplexity = 920.6990\n",
            "Step 2000 avg train loss = 6.8364\n",
            "Step 2000 avg train perplexity = 931.1135\n",
            "Validation loss after 5 epoch = 6.7120\n",
            "Validation perplexity after 5 epoch = 822.1918\n",
            "Step 0 avg train loss = 6.8157\n",
            "Step 0 avg train perplexity = 912.0113\n",
            "Step 1000 avg train loss = 6.8160\n",
            "Step 1000 avg train perplexity = 912.3019\n",
            "Step 2000 avg train loss = 6.8253\n",
            "Step 2000 avg train perplexity = 920.8792\n",
            "Validation loss after 6 epoch = 6.7177\n",
            "Validation perplexity after 6 epoch = 826.9441\n",
            "Step 0 avg train loss = 6.8107\n",
            "Step 0 avg train perplexity = 907.4898\n",
            "Step 1000 avg train loss = 6.8075\n",
            "Step 1000 avg train perplexity = 904.6198\n",
            "Step 2000 avg train loss = 6.8194\n",
            "Step 2000 avg train perplexity = 915.4241\n",
            "Validation loss after 7 epoch = 6.7246\n",
            "Validation perplexity after 7 epoch = 832.6267\n",
            "Step 0 avg train loss = 6.7258\n",
            "Step 0 avg train perplexity = 833.6204\n",
            "Step 1000 avg train loss = 6.8018\n",
            "Step 1000 avg train perplexity = 899.4367\n",
            "Step 2000 avg train loss = 6.8116\n",
            "Step 2000 avg train perplexity = 908.3186\n",
            "Validation loss after 8 epoch = 6.7303\n",
            "Validation perplexity after 8 epoch = 837.3750\n",
            "Step 0 avg train loss = 6.7954\n",
            "Step 0 avg train perplexity = 893.6931\n",
            "Step 1000 avg train loss = 6.7909\n",
            "Step 1000 avg train perplexity = 889.7469\n",
            "Step 2000 avg train loss = 6.8096\n",
            "Step 2000 avg train perplexity = 906.5489\n",
            "Validation loss after 9 epoch = 6.7361\n",
            "Validation perplexity after 9 epoch = 842.2673\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 128, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_3:\n",
            "Step 0 avg train loss = 10.4022\n",
            "Step 0 avg train perplexity = 32931.9252\n",
            "Step 1000 avg train loss = 8.2417\n",
            "Step 1000 avg train perplexity = 3795.9626\n",
            "Step 2000 avg train loss = 7.9923\n",
            "Step 2000 avg train perplexity = 2957.9873\n",
            "Validation loss after 0 epoch = 7.8854\n",
            "Validation perplexity after 0 epoch = 2658.1512\n",
            "Step 0 avg train loss = 7.9404\n",
            "Step 0 avg train perplexity = 2808.5269\n",
            "Step 1000 avg train loss = 7.9903\n",
            "Step 1000 avg train perplexity = 2952.3074\n",
            "Step 2000 avg train loss = 7.9900\n",
            "Step 2000 avg train perplexity = 2951.2180\n",
            "Validation loss after 1 epoch = 7.8988\n",
            "Validation perplexity after 1 epoch = 2693.9252\n",
            "Step 0 avg train loss = 8.0774\n",
            "Step 0 avg train perplexity = 3220.7044\n",
            "Step 1000 avg train loss = 7.9899\n",
            "Step 1000 avg train perplexity = 2950.8735\n",
            "Step 2000 avg train loss = 7.9864\n",
            "Step 2000 avg train perplexity = 2940.6583\n",
            "Validation loss after 2 epoch = 7.9011\n",
            "Validation perplexity after 2 epoch = 2700.2923\n",
            "Step 0 avg train loss = 7.8850\n",
            "Step 0 avg train perplexity = 2657.0826\n",
            "Step 1000 avg train loss = 7.9907\n",
            "Step 1000 avg train perplexity = 2953.2994\n",
            "Step 2000 avg train loss = 7.9886\n",
            "Step 2000 avg train perplexity = 2947.2880\n",
            "Validation loss after 3 epoch = 7.8661\n",
            "Validation perplexity after 3 epoch = 2607.4609\n",
            "Step 0 avg train loss = 7.9729\n",
            "Step 0 avg train perplexity = 2901.2434\n",
            "Step 1000 avg train loss = 7.9884\n",
            "Step 1000 avg train perplexity = 2946.6766\n",
            "Step 2000 avg train loss = 7.9932\n",
            "Step 2000 avg train perplexity = 2960.7020\n",
            "Validation loss after 4 epoch = 7.8966\n",
            "Validation perplexity after 4 epoch = 2688.1766\n",
            "Step 0 avg train loss = 8.0403\n",
            "Step 0 avg train perplexity = 3103.5630\n",
            "Step 1000 avg train loss = 7.9914\n",
            "Step 1000 avg train perplexity = 2955.5598\n",
            "Step 2000 avg train loss = 7.9877\n",
            "Step 2000 avg train perplexity = 2944.4286\n",
            "Validation loss after 5 epoch = 7.8861\n",
            "Validation perplexity after 5 epoch = 2660.0066\n",
            "Step 0 avg train loss = 7.9596\n",
            "Step 0 avg train perplexity = 2863.0367\n",
            "Step 1000 avg train loss = 7.9904\n",
            "Step 1000 avg train perplexity = 2952.4562\n",
            "Step 2000 avg train loss = 7.9890\n",
            "Step 2000 avg train perplexity = 2948.2283\n",
            "Validation loss after 6 epoch = 7.9297\n",
            "Validation perplexity after 6 epoch = 2778.6463\n",
            "Step 0 avg train loss = 7.9938\n",
            "Step 0 avg train perplexity = 2962.6750\n",
            "Step 1000 avg train loss = 7.9892\n",
            "Step 1000 avg train perplexity = 2948.8678\n",
            "Step 2000 avg train loss = 7.9890\n",
            "Step 2000 avg train perplexity = 2948.3181\n",
            "Validation loss after 7 epoch = 7.9022\n",
            "Validation perplexity after 7 epoch = 2703.1462\n",
            "Step 0 avg train loss = 7.9800\n",
            "Step 0 avg train perplexity = 2921.7960\n",
            "Step 1000 avg train loss = 7.9906\n",
            "Step 1000 avg train perplexity = 2952.9315\n",
            "Step 2000 avg train loss = 7.9925\n",
            "Step 2000 avg train perplexity = 2958.7432\n",
            "Validation loss after 8 epoch = 7.9006\n",
            "Validation perplexity after 8 epoch = 2698.9059\n",
            "Step 0 avg train loss = 7.9511\n",
            "Step 0 avg train perplexity = 2838.7848\n",
            "Step 1000 avg train loss = 7.9897\n",
            "Step 1000 avg train perplexity = 2950.5491\n",
            "Step 2000 avg train loss = 7.9895\n",
            "Step 2000 avg train perplexity = 2949.8309\n",
            "Validation loss after 9 epoch = 7.9064\n",
            "Validation perplexity after 9 epoch = 2714.6347\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_4:\n",
            "Step 0 avg train loss = 10.4194\n",
            "Step 0 avg train perplexity = 33503.9724\n",
            "Step 1000 avg train loss = 6.8830\n",
            "Step 1000 avg train perplexity = 975.5902\n",
            "Step 2000 avg train loss = 6.2030\n",
            "Step 2000 avg train perplexity = 494.2347\n",
            "Validation loss after 0 epoch = 5.7710\n",
            "Validation perplexity after 0 epoch = 320.8474\n",
            "Step 0 avg train loss = 5.9074\n",
            "Step 0 avg train perplexity = 367.7599\n",
            "Step 1000 avg train loss = 5.7645\n",
            "Step 1000 avg train perplexity = 318.7787\n",
            "Step 2000 avg train loss = 5.6415\n",
            "Step 2000 avg train perplexity = 281.8777\n",
            "Validation loss after 1 epoch = 5.4758\n",
            "Validation perplexity after 1 epoch = 238.8431\n",
            "Step 0 avg train loss = 5.5333\n",
            "Step 0 avg train perplexity = 252.9889\n",
            "Step 1000 avg train loss = 5.3971\n",
            "Step 1000 avg train perplexity = 220.7672\n",
            "Step 2000 avg train loss = 5.3374\n",
            "Step 2000 avg train perplexity = 207.9811\n",
            "Validation loss after 2 epoch = 5.3513\n",
            "Validation perplexity after 2 epoch = 210.8802\n",
            "Step 0 avg train loss = 5.0901\n",
            "Step 0 avg train perplexity = 162.4037\n",
            "Step 1000 avg train loss = 5.1367\n",
            "Step 1000 avg train perplexity = 170.1458\n",
            "Step 2000 avg train loss = 5.1227\n",
            "Step 2000 avg train perplexity = 167.7885\n",
            "Validation loss after 3 epoch = 5.2895\n",
            "Validation perplexity after 3 epoch = 198.2385\n",
            "Step 0 avg train loss = 4.9328\n",
            "Step 0 avg train perplexity = 138.7729\n",
            "Step 1000 avg train loss = 4.9456\n",
            "Step 1000 avg train perplexity = 140.5614\n",
            "Step 2000 avg train loss = 4.9517\n",
            "Step 2000 avg train perplexity = 141.4177\n",
            "Validation loss after 4 epoch = 5.2735\n",
            "Validation perplexity after 4 epoch = 195.0956\n",
            "Step 0 avg train loss = 4.6360\n",
            "Step 0 avg train perplexity = 103.1271\n",
            "Step 1000 avg train loss = 4.7966\n",
            "Step 1000 avg train perplexity = 121.0955\n",
            "Step 2000 avg train loss = 4.8136\n",
            "Step 2000 avg train perplexity = 123.1730\n",
            "Validation loss after 5 epoch = 5.2806\n",
            "Validation perplexity after 5 epoch = 196.4867\n",
            "Step 0 avg train loss = 4.7537\n",
            "Step 0 avg train perplexity = 116.0174\n",
            "Step 1000 avg train loss = 4.6698\n",
            "Step 1000 avg train perplexity = 106.6762\n",
            "Step 2000 avg train loss = 4.6960\n",
            "Step 2000 avg train perplexity = 109.5110\n",
            "Validation loss after 6 epoch = 5.2967\n",
            "Validation perplexity after 6 epoch = 199.6742\n",
            "Step 0 avg train loss = 4.6207\n",
            "Step 0 avg train perplexity = 101.5620\n",
            "Step 1000 avg train loss = 4.5668\n",
            "Step 1000 avg train perplexity = 96.2359\n",
            "Step 2000 avg train loss = 4.5917\n",
            "Step 2000 avg train perplexity = 98.6591\n",
            "Validation loss after 7 epoch = 5.3249\n",
            "Validation perplexity after 7 epoch = 205.3794\n",
            "Step 0 avg train loss = 4.2544\n",
            "Step 0 avg train perplexity = 70.4138\n",
            "Step 1000 avg train loss = 4.4752\n",
            "Step 1000 avg train perplexity = 87.8164\n",
            "Step 2000 avg train loss = 4.5063\n",
            "Step 2000 avg train perplexity = 90.5895\n",
            "Validation loss after 8 epoch = 5.3550\n",
            "Validation perplexity after 8 epoch = 211.6540\n",
            "Step 0 avg train loss = 4.2979\n",
            "Step 0 avg train perplexity = 73.5484\n",
            "Step 1000 avg train loss = 4.3942\n",
            "Step 1000 avg train perplexity = 80.9832\n",
            "Step 2000 avg train loss = 4.4348\n",
            "Step 2000 avg train perplexity = 84.3349\n",
            "Validation loss after 9 epoch = 5.3999\n",
            "Validation perplexity after 9 epoch = 221.3878\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_5:\n",
            "Step 0 avg train loss = 10.4065\n",
            "Step 0 avg train perplexity = 33074.0936\n",
            "Step 1000 avg train loss = 8.0544\n",
            "Step 1000 avg train perplexity = 3147.6084\n",
            "Step 2000 avg train loss = 7.9183\n",
            "Step 2000 avg train perplexity = 2746.9646\n",
            "Validation loss after 0 epoch = 7.8484\n",
            "Validation perplexity after 0 epoch = 2561.5412\n",
            "Step 0 avg train loss = 7.8774\n",
            "Step 0 avg train perplexity = 2636.9594\n",
            "Step 1000 avg train loss = 7.9402\n",
            "Step 1000 avg train perplexity = 2807.8523\n",
            "Step 2000 avg train loss = 7.9375\n",
            "Step 2000 avg train perplexity = 2800.2736\n",
            "Validation loss after 1 epoch = 7.9023\n",
            "Validation perplexity after 1 epoch = 2703.5606\n",
            "Step 0 avg train loss = 7.7949\n",
            "Step 0 avg train perplexity = 2428.2559\n",
            "Step 1000 avg train loss = 7.9361\n",
            "Step 1000 avg train perplexity = 2796.5331\n",
            "Step 2000 avg train loss = 7.9404\n",
            "Step 2000 avg train perplexity = 2808.4463\n",
            "Validation loss after 2 epoch = 7.8673\n",
            "Validation perplexity after 2 epoch = 2610.4136\n",
            "Step 0 avg train loss = 8.0397\n",
            "Step 0 avg train perplexity = 3101.8232\n",
            "Step 1000 avg train loss = 7.9422\n",
            "Step 1000 avg train perplexity = 2813.4549\n",
            "Step 2000 avg train loss = 7.9385\n",
            "Step 2000 avg train perplexity = 2803.0843\n",
            "Validation loss after 3 epoch = 7.8620\n",
            "Validation perplexity after 3 epoch = 2596.7462\n",
            "Step 0 avg train loss = 7.8486\n",
            "Step 0 avg train perplexity = 2562.0664\n",
            "Step 1000 avg train loss = 7.9391\n",
            "Step 1000 avg train perplexity = 2804.7549\n",
            "Step 2000 avg train loss = 7.9427\n",
            "Step 2000 avg train perplexity = 2815.0411\n",
            "Validation loss after 4 epoch = 7.8420\n",
            "Validation perplexity after 4 epoch = 2545.3374\n",
            "Step 0 avg train loss = 7.9574\n",
            "Step 0 avg train perplexity = 2856.5171\n",
            "Step 1000 avg train loss = 7.9396\n",
            "Step 1000 avg train perplexity = 2806.1872\n",
            "Step 2000 avg train loss = 7.9386\n",
            "Step 2000 avg train perplexity = 2803.3565\n",
            "Validation loss after 5 epoch = 7.8475\n",
            "Validation perplexity after 5 epoch = 2559.3158\n",
            "Step 0 avg train loss = 8.0692\n",
            "Step 0 avg train perplexity = 3194.4158\n",
            "Step 1000 avg train loss = 7.9411\n",
            "Step 1000 avg train perplexity = 2810.5372\n",
            "Step 2000 avg train loss = 7.9378\n",
            "Step 2000 avg train perplexity = 2801.2569\n",
            "Validation loss after 6 epoch = 7.8339\n",
            "Validation perplexity after 6 epoch = 2524.8568\n",
            "Step 0 avg train loss = 8.0327\n",
            "Step 0 avg train perplexity = 3079.9366\n",
            "Step 1000 avg train loss = 7.9393\n",
            "Step 1000 avg train perplexity = 2805.3388\n",
            "Step 2000 avg train loss = 7.9420\n",
            "Step 2000 avg train perplexity = 2812.9136\n",
            "Validation loss after 7 epoch = 7.8714\n",
            "Validation perplexity after 7 epoch = 2621.1562\n",
            "Step 0 avg train loss = 7.8534\n",
            "Step 0 avg train perplexity = 2574.5310\n",
            "Step 1000 avg train loss = 7.9373\n",
            "Step 1000 avg train perplexity = 2799.8043\n",
            "Step 2000 avg train loss = 7.9430\n",
            "Step 2000 avg train perplexity = 2815.6817\n",
            "Validation loss after 8 epoch = 7.8322\n",
            "Validation perplexity after 8 epoch = 2520.3983\n",
            "Step 0 avg train loss = 7.9228\n",
            "Step 0 avg train perplexity = 2759.3659\n",
            "Step 1000 avg train loss = 7.9417\n",
            "Step 1000 avg train perplexity = 2812.1836\n",
            "Step 2000 avg train loss = 7.9352\n",
            "Step 2000 avg train perplexity = 2794.0296\n",
            "Validation loss after 9 epoch = 7.8164\n",
            "Validation perplexity after 9 epoch = 2480.9592\n",
            "Finished training\n",
            "LstmLM(\n",
            "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
            "  (lstm): LSTM(64, 200, num_layers=5, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=200, out_features=33181, bias=True)\n",
            ")\n",
            "Training LSTM LM Finetuned_6:\n",
            "Step 0 avg train loss = 10.4098\n",
            "Step 0 avg train perplexity = 33183.1874\n",
            "Step 1000 avg train loss = 7.0681\n",
            "Step 1000 avg train perplexity = 1173.8871\n",
            "Step 2000 avg train loss = 6.9136\n",
            "Step 2000 avg train perplexity = 1005.8454\n",
            "Validation loss after 0 epoch = 6.7228\n",
            "Validation perplexity after 0 epoch = 831.1034\n",
            "Step 0 avg train loss = 6.9344\n",
            "Step 0 avg train perplexity = 1027.0113\n",
            "Step 1000 avg train loss = 6.8681\n",
            "Step 1000 avg train perplexity = 961.1579\n",
            "Step 2000 avg train loss = 6.8733\n",
            "Step 2000 avg train perplexity = 966.1072\n",
            "Validation loss after 1 epoch = 6.7073\n",
            "Validation perplexity after 1 epoch = 818.3844\n",
            "Step 0 avg train loss = 6.6329\n",
            "Step 0 avg train perplexity = 759.6674\n",
            "Step 1000 avg train loss = 6.8493\n",
            "Step 1000 avg train perplexity = 943.2448\n",
            "Step 2000 avg train loss = 6.8605\n",
            "Step 2000 avg train perplexity = 953.8781\n",
            "Validation loss after 2 epoch = 6.7079\n",
            "Validation perplexity after 2 epoch = 818.8722\n",
            "Step 0 avg train loss = 6.7744\n",
            "Step 0 avg train perplexity = 875.1597\n",
            "Step 1000 avg train loss = 6.8397\n",
            "Step 1000 avg train perplexity = 934.1945\n",
            "Step 2000 avg train loss = 6.8459\n",
            "Step 2000 avg train perplexity = 940.0068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jayNuTfjHYs-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "b22843bd-7227-428e-fe4a-5ae84f5843d0"
      },
      "source": [
        "# find best comb (lowest validation loss)\n",
        "sorted(finetune_res.items(), key=lambda x: x[1][1][-1][1])[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a2b0615fc324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetune_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'finetune_res' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bO3PC2KfcXs",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAundSFoka2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def compute_cosine_similarity(emb_matrix):\n",
        "  distance_matrix = np.zeros(emb_matrix.shape[0], emb_matrix.shape[0])\n",
        "  for i in range(emb_matrix.shape[0]):\n",
        "    for j in range(emb_matrix.shape[0]):\n",
        "      distance_matrix[i,j] = 1-spatial.distance.cosine(emb_matrix[i], emb_matrix[j])\n",
        "\n",
        "  return distance_matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvEFhwJntn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance_matrix=compute_cosine_similarity(model_lstm_tuned.state_dict()['embedded.weight'])\n",
        "words = ['the', 'run', 'dog', 'where', 'quick']\n",
        "for word in words:\n",
        "  row_distance = distance_matrix[wikitext_dict.get_id[word]]\n",
        "  closest_words = wikitext_dict.get_token[i for i in row_distance.argsort()[-10:][::-1]]\n",
        "  furtherest_words = wikitext_dict.get_token[i for i in row_distance.argsort()[10:]]\n",
        "  print(\"For {}:\".format(word))\n",
        "  print(\"the most similar words are: \", closest_words)\n",
        "  print(\"the least similar words are: \", furtherest_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jh1-FBYfcXt",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc2COexlfcXt",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7b85774-3646-4998-e6e0-d4228bd9ccd3"
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsPkFZOkfcXw",
        "colab_type": "code",
        "colab": {},
        "outputId": "a83c599e-a276-4d76-b00c-d7fc0a5d091b"
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LJWwkZcfcXy",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLIuLUvfcX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64VdHasfcX4",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22lYpwzsfcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDwB3jHfcYA",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCzDobrwfcYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q_6ZeVbfcYD",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtgckC9xfcYD",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpkn6pkMfcYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MDN0NX4fcYH",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5U6hOImfcYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvfYp0nHfcYS",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qrGetifcYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZcyAJYfcYV",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfmkPJ4fcYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBA1Z5kfcYZ",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2n-ec9fcYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7i24wZxfcYi",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyes6PHWfcYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}